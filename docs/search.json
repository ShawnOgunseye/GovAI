[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis book provides a comprehensive guide to AI governance for practitioners building and managing governance programs in organizations. It covers foundational concepts, legal and regulatory frameworks, governance across the AI lifecycle, multiple organizational perspectives, and emerging topics where the field continues to evolve.\nThe book takes a “governance first” approach, recognizing that governance foundations should precede large-scale AI adoption. It builds understanding systematically from foundational concepts through practical implementation, with worked examples, professional figures, and practical templates.\nTo open doors for all redeemed winners",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Preface",
    "section": "License",
    "text": "License\n\n\n\nCC BY-NC-ND 4.0\n\n\nThis work is licensed under a Creative Commons Attribution–NonCommercial–NoDerivatives 4.0 International License.\nYou are free to:\n\nShare — copy and redistribute the material in any medium or format\n\nUnder the following terms:\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nNonCommercial — You may not use the material for commercial purposes.\nNoDerivatives — If you remix, transform, or build upon the material, you may not distribute the modified material.\n\n© 2026 Shawn Ogunseye. All rights reserved under the terms of the CC BY-NC-ND 4.0 license.\nIf you use or reference this book, please cite it as: Ogunseye, S. (2026). Governing AI: Leading Responsible Innovation (Version 1.0). Zenodo. https://doi.org/10.5281/zenodo.18407349\n\n\n\nDOI",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Preface",
    "section": "About This Book",
    "text": "About This Book\nEach chapter follows a consistent structure: an opening that situates the material, substantive prose explaining core concepts and their relationships, worked examples that demonstrate application, and end-of-chapter review questions that reinforce learning. Templates and detailed checklists appear in the appendices.\nA note on legal and regulatory content: AI governance operates within a rapidly evolving regulatory landscape. This book describes requirements and frameworks as they exist at the time of writing, but readers should verify current requirements in their jurisdictions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Preface",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nIf you have limited time, focus on Chapters 1 and 2 to understand foundational concepts and the regulatory landscape, then proceed to Chapter 5 on governance by design.\nFor comprehensive learning, read all chapters in sequence and work through the practice scenarios in Appendix C.\nFor practitioners, the appendix templates and worked examples provide starting points for organizational adaptation.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html",
    "href": "chapters/01-foundations.html",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "",
    "text": "1.1 Introduction\nArtificial intelligence has moved from research laboratories into the daily operations of businesses, governments, and consumer products. This rapid adoption brings substantial benefits: organizations can analyze data at scales impossible for humans, automate routine decisions, personalize services, and discover patterns that would otherwise remain hidden. These capabilities create real competitive advantages and genuine improvements in efficiency, accuracy, and service quality.\nBut AI adoption also introduces significant risks that traditional governance frameworks were not designed to address. AI systems can discriminate against protected groups without any human intending that outcome. They can make consequential errors at speeds that preclude human review. They can invade privacy through surveillance capabilities that were previously impractical. They can generate convincing misinformation at scale. And when things go wrong, the complexity and opacity of AI systems can make it difficult to understand what happened or hold anyone accountable.\nAI governance exists to help organizations capture the benefits of AI while managing these risks responsibly. It provides the structures, processes, and policies that enable organizations to develop and deploy AI in ways that are effective, ethical, and compliant with legal requirements. Done well, AI governance does not slow innovation but enables it by providing the certainty organizations need to invest confidently in AI capabilities.\nThis chapter establishes the foundational knowledge that AI governance professionals need. It begins by examining what AI actually is, because effective governance requires understanding what you are governing. It then explores the characteristics that make AI governance distinct from governing other technologies. The chapter proceeds to catalog the risks and harms AI can cause, because governance priorities should reflect actual risks. It examines the principles that guide responsible AI development and deployment. And it concludes by addressing how organizations establish governance structures, define roles, and create policies to manage AI effectively.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#what-is-artificial-intelligence",
    "href": "chapters/01-foundations.html#what-is-artificial-intelligence",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.2 What Is Artificial Intelligence?",
    "text": "1.2 What Is Artificial Intelligence?\nDefining artificial intelligence precisely has proven surprisingly difficult. The term encompasses a wide range of technologies with different capabilities, mechanisms, and implications. For governance purposes, however, certain definitions have achieved broad acceptance and provide useful starting points.\nThe National Institute of Standards and Technology defines AI as “an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments.” NIST emphasizes that AI systems “are designed to operate with varying levels of autonomy,” meaning they can function with different degrees of human involvement. This definition appears in the NIST AI Risk Management Framework and has influenced regulatory approaches in the United States.\nThe EU AI Act takes a similar approach, defining AI systems as “machine-based system[s] that [are] designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.”\nThe Organisation for Economic Co-operation and Development, whose AI Principles have been adopted by over forty countries and have shaped regulatory frameworks globally, defines an AI system as “a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.”\nThese definitions share common elements that matter for governance. First, AI involves engineered or machine-based systems, distinguishing it from human intelligence or natural processes. Second, AI systems generate outputs including predictions, recommendations, content, or decisions. Third, those outputs influence real or virtual environments, meaning AI has effects in the world. Fourth, AI systems operate with varying levels of autonomy, meaning they can function with different degrees of human oversight. Fifth, AI systems infer how to generate outputs from input, rather than simply following explicit programmed rules for every situation.\nUnderstanding these definitional elements helps governance professionals identify what systems fall within AI governance scope. A traditional rule-based system that follows explicit programmed logic may not qualify as AI under these definitions. A system that learns patterns from data and applies them to generate predictions almost certainly does. The boundaries can be fuzzy, and organizations should establish clear criteria for what falls within their AI governance programs.\n\nMachine Learning and Its Relationship to AI\nMachine learning is the dominant approach to building AI systems today. Rather than programming explicit rules for every situation, machine learning uses data and algorithms to enable systems to learn patterns and improve their performance over time. A machine learning system learns from examples rather than following predetermined instructions for every case.\nThis learning-based approach is both powerful and problematic for governance. It is powerful because machine learning systems can discover patterns humans might miss and handle complexity that would be impossible to program explicitly. A fraud detection system can learn subtle indicators of fraudulent transactions from millions of historical examples. A medical imaging system can learn to identify tumors from thousands of labeled scans. These capabilities create real value.\nThe same learning-based approach creates governance challenges. The patterns a machine learning system learns may reflect biases present in training data. If a hiring algorithm learns from historical hiring decisions made in a discriminatory context, it may learn to perpetuate that discrimination. The reasoning behind machine learning decisions may be opaque; a deep learning model with millions of parameters does not provide a simple explanation for why it classified an input in a particular way. And the system’s behavior may change as it encounters new data, potentially drifting from its original performance characteristics.\nSupervised learning, the most common machine learning approach, trains models on labeled examples. A spam detection system trains on emails labeled as spam or not spam. A credit risk model trains on historical loans labeled as defaulted or repaid. The system learns to predict labels for new, unlabeled examples. Governance concerns include ensuring training labels are accurate and unbiased, ensuring training data is representative of the population the system will serve, and monitoring for performance degradation as data distributions change over time.\nUnsupervised learning trains models on unlabeled data to discover structure or patterns. Clustering algorithms group similar items together. Dimensionality reduction techniques identify important features. These systems raise governance questions about whether the discovered patterns are meaningful and whether the system’s operation remains appropriate as data evolves.\nReinforcement learning trains models through interaction with an environment, receiving rewards or penalties for different actions. Game-playing AI and robotic control systems often use reinforcement learning. Governance concerns include ensuring reward functions align with intended objectives and preventing systems from learning to achieve rewards through unintended means.\nKnowledge-based systems represent an older approach to AI that captures human expertise in explicit rules and decision trees. Expert systems encode the knowledge of specialists in a particular domain. These systems are generally more transparent and predictable than machine learning systems, but they cannot learn from experience or handle situations their designers did not anticipate. Some organizations continue to use knowledge-based systems for applications where transparency and predictability are paramount.\n\n\nGenerative AI and Predictive AI\nA crucial distinction for governance professionals is between generative and predictive AI. These categories have different capabilities, different use cases, and different risk profiles that require different governance approaches.\nPredictive AI analyzes existing data to forecast outcomes, classify information, or identify patterns. A credit scoring model predicts the probability that a loan applicant will default. A medical imaging system classifies whether a scan shows signs of disease. A recommendation system predicts which products a customer is likely to purchase. These systems answer questions like “what will happen?” or “what category does this belong to?” or “what should we recommend?”\nThe risks of predictive AI center on accuracy, fairness, and transparency. An inaccurate prediction can lead to wrong decisions with real consequences for individuals. A biased prediction can systematically disadvantage protected groups. An opaque prediction can leave affected individuals unable to understand or contest decisions that affect them. Governance approaches for predictive AI focus on validating accuracy, testing for bias across demographic groups, and ensuring appropriate transparency and explainability.\nGenerative AI creates new content that did not exist before. Large language models generate text. Image generation systems create pictures from text descriptions. Music composition systems generate audio. Video generation systems produce moving images. These systems do not just analyze existing information; they produce novel outputs that resemble their training data.\nGenerative AI introduces additional risk categories beyond those of predictive AI. Hallucination occurs when generative systems produce confident but false information, presenting fabricated facts, fake citations, or incorrect claims as if they were accurate. Intellectual property concerns arise when generative systems produce outputs that substantially replicate copyrighted training material or when questions arise about who owns the rights to generated content. Misinformation becomes possible at unprecedented scale when systems can generate persuasive false content cheaply and quickly. Fraud and manipulation become easier when systems can generate convincing fake communications or impersonate real people.\nThe rapid proliferation of generative AI since 2022 has challenged existing governance frameworks and prompted new regulatory attention. Organizations deploying generative AI need governance approaches that address both the shared risks with predictive AI and the novel risks specific to content generation.\n\n\nNarrow AI and General AI\nCurrent AI systems are narrow or weak AI, meaning they excel at specific tasks but cannot generalize to new domains. A chess-playing AI cannot diagnose diseases. A language model trained on text cannot process images unless specifically designed to do so. A medical imaging classifier cannot suddenly analyze financial statements. Each narrow AI system operates within the boundaries of its training and design.\nThis narrow capability has important implications for governance. Narrow AI systems have specific, bounded capabilities that can be tested, validated, and monitored. Their performance can be measured on relevant benchmarks. Their failure modes can be identified and mitigated. They are tools that humans can understand and control, even if their internal mechanisms are complex.\nArtificial general intelligence refers to hypothetical systems with human-like ability to learn any intellectual task. AGI does not currently exist and may never exist. It features prominently in discussions about long-term AI risks and has attracted significant attention from researchers concerned about existential risk. However, for practical governance purposes, AGI remains speculative.\nAI governance professionals should focus on the AI systems that organizations actually deploy today, which are narrow AI systems with specific capabilities and limitations. The governance frameworks in this book address real, deployed narrow AI rather than hypothetical future systems. This is not to dismiss concerns about more capable future AI, but to ground governance in practical reality.\n\n\nAI as a Socio-Technical System\nPerhaps the most important concept for governance professionals is that AI systems are not purely technical artifacts. They are socio-technical systems where technical components interact with social contexts, human behaviors, and institutional structures. Understanding this interaction is essential for effective governance.\nAn AI hiring tool does not operate in isolation. It operates within recruiting processes designed by humans, organizational cultures that shape how the tool is used, legal requirements that constrain permissible decisions, and societal expectations about fairness in employment. The technical characteristics of the AI model matter, but so do the processes surrounding it, the humans who use it, the institutions that deploy it, and the individuals affected by it.\nThis socio-technical nature means that technical excellence alone does not ensure responsible AI. A technically sound model can cause harm if deployed in inappropriate contexts, used in unintended ways, or embedded in flawed processes. Conversely, governance mechanisms can sometimes compensate for technical limitations through appropriate human oversight or process controls.\nEffective AI governance therefore requires more than technical expertise. It requires understanding the social contexts where AI operates, the people who will use and be affected by AI systems, and the institutional structures that shape how AI is developed and deployed. This is why AI governance programs benefit from cross-disciplinary collaboration involving not just engineers and data scientists but also ethicists, social scientists, legal experts, user experience researchers, and representatives of affected communities.\n\n\n\n\n\n\nFigure 1.1: AI System Types and Characteristics\n\n\n\nFigure 1.1: AI System Types and Characteristics — A visual taxonomy showing the relationships between AI categories including machine learning approaches, generative vs. predictive AI, and governance implications for each type.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#why-ai-requires-specialized-governance",
    "href": "chapters/01-foundations.html#why-ai-requires-specialized-governance",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.3 Why AI Requires Specialized Governance",
    "text": "1.3 Why AI Requires Specialized Governance\nAI systems possess characteristics that distinguish them from traditional software and require specialized governance approaches. Understanding these characteristics helps governance professionals design appropriate controls and communicate effectively with technical teams and organizational leadership about why AI governance matters.\n\nComplexity and Opacity\nModern AI systems, particularly deep learning models, can have millions or billions of parameters that interact in ways that are difficult to understand. A traditional software program follows explicit logic that developers can trace and explain. If a conventional program denies a loan application, a developer can examine the code and identify exactly which conditions triggered the denial.\nDeep neural networks work differently. They learn internal representations that encode patterns in training data, but these representations do not correspond to human-interpretable concepts in any straightforward way. When a deep learning model denies a loan application, there is no simple code path to examine. The decision emerged from the interaction of millions of learned parameters in ways that even the system’s creators may not fully comprehend.\nThis opacity creates real governance challenges. Regulations increasingly require organizations to provide meaningful explanations of automated decisions affecting individuals. The EU General Data Protection Regulation requires data controllers to provide “meaningful information about the logic involved” in automated decision-making. The EU AI Act requires that high-risk AI systems be designed to allow human oversight including understanding of system outputs. United States laws prohibiting discrimination require organizations to explain their decision-making processes.\nMeeting these requirements with opaque AI systems is genuinely difficult. Research in explainable AI has produced techniques for generating explanations of model behavior, such as identifying which input features most influenced a particular prediction. But these explanations are approximations that may not fully capture the model’s actual reasoning. Organizations must navigate the tension between the performance benefits of complex models and the governance requirements for transparency and explainability.\n\n\nAutonomy and Speed\nAI systems can make decisions and take actions without human intervention, often at speeds that preclude human review of individual decisions. A content moderation system may review millions of posts daily. A fraud detection system may evaluate thousands of transactions per second. A recommendation algorithm may personalize experiences for millions of users simultaneously.\nThis autonomy and speed mean that errors or biases can affect many people before anyone notices a problem. If a content moderation system has a systematic blind spot, millions of problematic posts may slip through before the issue is identified. If a fraud detection system has a biased decision boundary, thousands of legitimate transactions may be wrongly blocked before the pattern becomes apparent.\nGovernance approaches must account for this scale. Monitoring systems must be capable of detecting issues across large numbers of automated decisions, using statistical methods rather than individual review. Organizations must establish clear criteria for when human oversight is required and ensure that humans can meaningfully exercise that oversight. They must have mechanisms to pause or roll back AI systems quickly when problems are identified.\nThe challenge is designing oversight that preserves AI’s efficiency benefits while ensuring accountability. Requiring human review of every AI decision would negate the reasons for using AI in the first place. But removing humans entirely from consequential decisions raises serious concerns about accountability and control. Finding the right balance requires careful analysis of the specific use case, the potential consequences of errors, and the feasibility of different oversight approaches.\n\n\nData Dependency\nAI systems learn from data, and the quality and characteristics of that data fundamentally shape system behavior. This data dependency distinguishes AI from traditional software where behavior is determined by explicit code.\nTraining data that underrepresents certain groups can produce systems that perform poorly for those groups. If a facial recognition system trains primarily on images of light-skinned faces, it may have higher error rates for darker-skinned faces. If a speech recognition system trains primarily on standard American English, it may struggle with other accents and dialects. These performance disparities can translate into discriminatory impacts when systems are deployed.\nHistorical data reflecting past discrimination can produce systems that perpetuate that discrimination. If a hiring algorithm trains on historical hiring decisions, and those decisions reflected discriminatory preferences, the algorithm may learn to replicate those preferences. The system encodes historical patterns without distinguishing between legitimate predictive relationships and discriminatory biases.\nData that becomes outdated can cause model performance to degrade over time. If a fraud detection model trains on patterns of fraud from several years ago, and fraud tactics have evolved, the model may miss new fraud patterns while continuing to flag behaviors that are no longer associated with fraud. This phenomenon, called data drift or concept drift, requires ongoing monitoring and model maintenance.\nGovernance must extend beyond the AI model itself to encompass the entire data pipeline. Where does training data come from? What biases might it contain? How representative is it of the population the system will serve? Are there legal constraints on using the data for AI training? How will the organization detect and respond when data distributions change? These questions require governance attention throughout the AI lifecycle.\n\n\nProbabilistic Outputs\nTraditional software produces deterministic outputs: given the same inputs, it produces the same outputs every time. AI systems often produce probabilistic results with inherent uncertainty. A classification model might indicate 73% confidence that an image shows a cat. A language model might generate different text each time it responds to the same prompt. A risk prediction might output a probability distribution rather than a single answer.\nThis probabilistic nature requires governance approaches that account for uncertainty. What confidence threshold is required before acting on a prediction? A medical diagnosis system that is 60% confident of a disease might warrant further testing, while one that is 99% confident might justify immediate treatment. These thresholds have real consequences and should not be set arbitrarily.\nHow should systems communicate uncertainty to users? Research shows that people often misunderstand probabilistic information, treating high-confidence predictions as certainties or ignoring uncertainty entirely. User interface design and training can help, but perfect communication of uncertainty may not be achievable.\nHow should organizations handle the inevitable cases where probabilistic systems make errors? Even an AI system with 99% accuracy will make errors on 1% of cases. At scale, that 1% can affect many people. Organizations need processes for identifying errors, providing recourse to affected individuals, and learning from mistakes to improve system performance.\n\n\nEmergent Behavior\nComplex AI systems can exhibit emergent behavior that was not explicitly programmed and may not have been anticipated by developers. Large language models, for example, have demonstrated capabilities that emerged from scale rather than being specifically designed, including the ability to perform arithmetic, translate between languages not heavily represented in training data, and solve certain reasoning problems.\nEmergent behavior creates governance challenges because it means AI systems may have capabilities beyond what testing revealed. A system tested for one purpose may turn out to be capable of other purposes, some of which may be problematic. A system that behaved safely in testing may behave differently when deployed at scale or when users interact with it in unanticipated ways.\nThis unpredictability argues for ongoing monitoring even after deployment, conservative deployment practices that limit initial exposure, and mechanisms to detect and respond to unexpected behavior. It also suggests humility about our ability to fully anticipate AI system behavior through pre-deployment testing alone.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#the-risks-and-harms-ai-can-cause",
    "href": "chapters/01-foundations.html#the-risks-and-harms-ai-can-cause",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.4 The Risks and Harms AI Can Cause",
    "text": "1.4 The Risks and Harms AI Can Cause\nAI governance exists because AI systems can cause real harm. Understanding the types of harm AI can cause helps governance professionals identify risks, design appropriate controls, prioritize governance efforts, and communicate the importance of governance to organizational leadership.\n\nHarms to Individuals\nAI systems increasingly make or influence decisions that significantly affect individual lives. Hiring algorithms determine who gets job interviews. Credit models decide who receives loans and at what interest rates. Healthcare AI influences diagnosis and treatment recommendations. Criminal justice algorithms affect bail decisions, sentencing recommendations, and parole outcomes. Content recommendation systems shape what information people see and consequently what they believe about the world.\nWhen these systems err or discriminate, individuals suffer concrete harms. Someone may be wrongly denied employment, credit, housing, or benefits. Someone may receive inappropriate medical treatment. Someone may face unjustified criminal justice consequences. These are not abstract concerns but documented harms affecting real people.\nConsider facial recognition technology as an example. Peer-reviewed research has consistently demonstrated that facial recognition systems have higher error rates for women and for people with darker skin. The National Institute of Standards and Technology’s Face Recognition Vendor Test has confirmed these disparities across commercial systems. When law enforcement uses these systems, individuals may be wrongly identified as suspects. Multiple documented cases exist of individuals being arrested based on faulty facial recognition matches.\nBeyond accuracy issues, facial recognition enables surveillance capabilities that can chill civil liberties. People may avoid protests, political activities, or religious gatherings knowing they could be identified. The accumulation of biometric data creates security risks; unlike a password, you cannot change your face if the data is compromised.\nPrivacy harms from AI extend beyond facial recognition. AI systems can infer sensitive information from seemingly innocuous data. Research has shown that AI can predict sexual orientation from facial images, political orientation from social media activity, and health conditions from consumer purchasing patterns. These inference capabilities raise profound questions about informational privacy in an age of ubiquitous data collection.\n\n\nHarms to Groups\nAI systems can systematically disadvantage particular groups, even when they appear neutral on their face. Machine learning systems learn patterns from historical data, and that historical data often reflects past discrimination. A hiring algorithm trained on past hiring decisions may learn to prefer candidates who resemble previously successful employees, perpetuating historical exclusion of women or minorities. A predictive policing system trained on arrest records may direct police to communities that have historically been over-policed, creating a self-reinforcing cycle.\nThese group harms are particularly insidious because they can occur without any discriminatory intent and may be invisible without careful analysis. An organization may deploy an AI system believing it to be fair, only to discover later through statistical analysis that it systematically disadvantages protected groups. The discrimination is built into the learned patterns rather than being explicitly programmed, making it harder to detect and attribute.\nResearch has documented algorithmic discrimination across domains. ProPublica’s investigation of the COMPAS recidivism prediction system found that Black defendants were more likely to be incorrectly flagged as high risk than white defendants, while white defendants were more likely to be incorrectly flagged as low risk than Black defendants. Studies of AI hiring tools have found disparate impacts based on gender and race. Research on healthcare algorithms has found that systems trained to predict healthcare costs disadvantaged Black patients because historical spending patterns reflected unequal access to care rather than equal health needs.\nGovernance programs must proactively test for these disparate impacts rather than assuming neutrality. Testing should examine performance across demographic groups, using appropriate fairness metrics for the application context. Organizations should also consider intersectional impacts, recognizing that harms may be greatest for individuals who belong to multiple disadvantaged groups.\n\n\nHarms to Organizations\nAI failures can significantly harm the organizations that deploy them. Reputational damage from AI discrimination or errors can affect brand value and customer trust for years. When Amazon’s experimental hiring algorithm was revealed to disadvantage women, the company abandoned the tool and faced lasting reputational consequences. When Microsoft’s Tay chatbot was manipulated into generating offensive content within hours of launch, it became a cautionary tale cited for years afterward.\nRegulatory penalties for AI violations are substantial and growing. The EU AI Act imposes fines of up to 35 million euros or 7% of global annual turnover for the most serious violations. Civil rights enforcement agencies in the United States have brought actions against companies whose AI systems discriminated. Class action lawsuits challenging algorithmic discrimination present significant liability exposure.\nFailed AI projects waste resources. Research suggests that a significant percentage of AI projects fail to deliver expected value. Poor governance can contribute to these failures by allowing inappropriate use cases to proceed, failing to ensure adequate data quality, or deploying systems without proper validation. These failures consume budgets, distract from more promising opportunities, and can create organizational skepticism about AI that impedes future beneficial adoption.\nAI can also cause cultural harm within organizations. When AI makes consequential decisions without clear accountability, employees may feel disempowered. When AI systems embody values inconsistent with organizational culture, they can erode trust and morale. When organizations rush to deploy AI without appropriate governance, they may create technical debt and institutional patterns that prove difficult to reverse.\n\n\nHarms to Society\nAt the broadest level, AI can harm society itself. These societal harms are more diffuse than individual harms but potentially more consequential.\nAI-generated misinformation can undermine shared understanding of facts and erode trust in institutions. Generative AI makes it increasingly easy to create convincing fake text, images, audio, and video. When people cannot reliably distinguish authentic content from fabrication, the epistemic foundations of democracy are threatened.\nAI-enabled surveillance can threaten privacy and civil liberties at scale. Systems that track individuals across public spaces, analyze their communications, and predict their behavior create capabilities that authoritarian regimes have exploited and that raise concerns even in democratic societies.\nAI automation can disrupt labor markets faster than workers and institutions can adapt. While economists debate the long-term employment effects of AI, the transition period clearly creates hardship for workers whose skills become less valuable. The benefits of AI-driven productivity gains may accrue primarily to capital owners while the costs fall on displaced workers, exacerbating inequality.\nAI systems can concentrate power in the hands of those who control the technology. The resources required to develop frontier AI systems mean that only a small number of organizations can do so, creating the potential for monopolistic or oligopolistic control over critical infrastructure.\n\n\n\n\n\n\nFigure 1.2: AI Harms Taxonomy\n\n\n\nFigure 1.2: AI Harms Taxonomy — The four levels of AI harms (Individual, Group, Organizational, Societal) with examples and cascade effects.\n\n\nEnvironmental Harms\nTraining large AI models requires substantial computing resources that consume significant energy. Estimates suggest that training a single large language model can produce carbon emissions equivalent to hundreds of transatlantic flights. The hardware required for AI has its own environmental footprint from manufacturing through disposal, including the mining of rare earth elements, water consumption for chip fabrication, and electronic waste at end of life. As AI adoption grows, these environmental impacts grow as well.\nOrganizations increasingly face pressure to account for the environmental impacts of their AI use. Environmental, social, and governance reporting requirements may encompass AI’s carbon footprint. Stakeholders may question whether the benefits of particular AI applications justify their environmental costs. Governance programs should include consideration of computational efficiency, hardware lifecycle impacts, and the proportionality of AI use to its benefits.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#principles-of-responsible-ai",
    "href": "chapters/01-foundations.html#principles-of-responsible-ai",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.5 Principles of Responsible AI",
    "text": "1.5 Principles of Responsible AI\nIn response to AI risks, numerous organizations have developed principles for responsible AI development and deployment. These include governmental bodies like the OECD and the European Commission’s High-Level Expert Group on AI, standards organizations like IEEE, professional associations like the IAPP, and individual companies publishing their own AI principles. While specific formulations vary, common themes emerge across these frameworks.\n\nFairness\nAI systems should treat people fairly and should not discriminate based on protected characteristics like race, gender, age, disability, religion, or national origin. This principle appears straightforward but proves complex in practice.\nFairness can be defined in multiple ways that sometimes conflict mathematically. Demographic parity requires that outcomes be distributed proportionally across groups. Equalized odds requires that error rates be equal across groups. Individual fairness requires that similar individuals receive similar treatment. Researchers have proven that some of these definitions cannot be satisfied simultaneously except in trivial cases.\nConsider a hiring algorithm as an example. Demographic parity would require that the proportion of candidates recommended for interviews be equal across demographic groups. But if qualified candidates are not equally distributed across groups because of historical inequities in education or opportunity, demographic parity might require recommending less qualified candidates from some groups over more qualified candidates from others. Alternatively, an algorithm optimized purely for predicting job performance might have disparate impacts that perpetuate historical exclusion.\nOrganizations must make deliberate choices about which conception of fairness to prioritize for each application, recognizing that different stakeholders may prefer different definitions and that no choice is neutral. These choices should be documented, justified, and subject to appropriate review.\nAchieving fairness requires proactive effort because machine learning systems learn from data, and if that data reflects historical unfairness, the system will tend to perpetuate it. Governance programs must include processes for defining appropriate fairness criteria, testing systems against those criteria, identifying disparities, and addressing them through technical means, process controls, or decisions not to deploy systems that cannot be made sufficiently fair.\n\n\nSafety and Reliability\nAI systems should function appropriately under expected conditions and should not cause unintended harm. They should perform reliably within their designed parameters and fail gracefully when encountering unexpected conditions. They should be robust against adversarial attacks designed to manipulate their behavior.\nSafety becomes particularly critical for AI systems that interact with the physical world. Autonomous vehicles must not endanger passengers, pedestrians, or other drivers. Medical devices must not harm patients. Industrial robots must not injure workers. But safety concerns extend to purely digital AI as well. A content recommendation system that promotes self-harm content to vulnerable users can contribute to real-world harm. A financial trading algorithm that malfunctions can cause market disruption.\nSafety requires anticipating potential failure modes and designing systems to minimize harm when failures occur. It requires rigorous testing before deployment, including adversarial testing that actively tries to break systems or cause them to behave inappropriately. It requires monitoring deployed systems to detect problems and having mechanisms to respond quickly when issues arise, including the ability to pause or roll back systems.\nThe EU AI Act specifically addresses safety, prohibiting AI systems that pose unacceptable safety risks and imposing extensive requirements on high-risk AI systems in critical infrastructure, essential services, and other safety-relevant domains.\n\n\nPrivacy and Security\nAI systems often require large amounts of data, and that data frequently includes personal information. The privacy principle requires that AI systems protect personal information, collect only what is necessary, use data only for appropriate purposes, and respect individuals’ rights over their information.\nPrivacy requirements for AI include ensuring lawful basis for data collection and use, providing appropriate notice to individuals about AI processing, implementing data minimization principles, honoring data subject rights including access, correction, and deletion, and protecting data through appropriate security measures. These requirements flow from general data protection laws like the GDPR and CCPA but take on specific implications in AI contexts.\nAI introduces novel privacy challenges beyond traditional data protection. AI systems can infer sensitive information from non-sensitive data, can enable surveillance at scales previously impractical, and can make predictions about individuals that those individuals might not want made. Privacy governance for AI must address these novel challenges.\nSecurity complements privacy by protecting against unauthorized access, data breaches, and adversarial attacks. AI systems face unique security threats including data poisoning attacks that manipulate training data to introduce biases or backdoors, adversarial examples crafted to fool trained models into making errors, and model extraction attacks that steal valuable trained models. AI security governance must address these AI-specific threats alongside traditional cybersecurity concerns.\n\n\nTransparency and Explainability\nThe transparency principle requires openness about AI use. People should know when they are interacting with AI systems and when AI influences decisions that affect them. Organizations should be clear about what AI systems do, what data they use, and how they reach conclusions.\nTransparency requirements are increasingly codified in law. The EU AI Act requires that users be informed when they are interacting with AI systems in certain contexts. The GDPR requires that individuals be informed about automated decision-making and its significance. Various US laws require disclosure of AI use in specific contexts like hiring.\nExplainability goes further, requiring that AI decisions can be understood and explained in terms meaningful to affected individuals. When AI makes a consequential decision, affected individuals should be able to understand the factors that influenced that decision and what they might do to achieve a different outcome.\nThe transparency and explainability principles are in tension with the complexity of modern AI systems. Deep learning models may be highly accurate but difficult to explain. Governance programs must navigate this tension, potentially accepting some reduction in performance for greater transparency, developing techniques to generate post-hoc explanations of complex model decisions, or limiting use of opaque models in contexts where explainability is essential.\n\n\nAccountability\nWhen AI systems cause harm, someone must be responsible. The accountability principle requires clear allocation of responsibility for AI systems and their outcomes. It requires mechanisms for identifying and addressing problems. It requires consequences for failures and incentives for responsible behavior.\nAccountability becomes complex when AI systems involve multiple parties. An organization may deploy an AI system developed by a vendor, trained on data from multiple sources, operating on cloud infrastructure from a third party, with components licensed from various providers. When something goes wrong, who bears responsibility? Regulatory frameworks increasingly allocate specific responsibilities to different actors in the AI value chain, distinguishing between developers, deployers, importers, and distributors.\nOrganizations must establish clear internal accountability as well. Who is responsible for ensuring an AI system is properly tested before deployment? Who is responsible for monitoring deployed systems? Who has authority to pause or withdraw a system that is causing harm? Who is accountable for the overall AI governance program? These questions should have clear answers, and those answers should be documented and communicated.\n\n\nHuman Oversight\nAI should serve human needs and remain under meaningful human control. This principle, sometimes called human-in-the-loop or human-centricity, requires that humans can understand, monitor, and when appropriate override AI systems. It rejects the notion that AI should operate autonomously beyond human comprehension or control.\nHuman oversight does not mean humans must review every AI decision. That would negate the efficiency benefits of AI. Rather, it means that appropriate human oversight exists for the type of system and decision involved. The appropriate level depends on the consequences of errors, the reversibility of decisions, and the feasibility of human review.\nA spam filter may operate without human review of individual decisions because the consequences of errors are relatively minor and easily corrected. A system that contributes to criminal sentencing recommendations requires meaningful human involvement because the consequences are severe and liberty is at stake. A medical diagnostic system might require physician review because medical judgment cannot be delegated entirely to machines.\nThe EU AI Act codifies human oversight requirements for high-risk AI systems, requiring that such systems be designed to allow effective oversight by natural persons during the period of use.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#establishing-ai-governance-in-organizations",
    "href": "chapters/01-foundations.html#establishing-ai-governance-in-organizations",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.6 Establishing AI Governance in Organizations",
    "text": "1.6 Establishing AI Governance in Organizations\nPrinciples provide direction, but organizations need practical structures to implement AI governance. Survey data from the IAPP AI Governance in Practice Report 2025 confirms that organizations are actively building these structures: 77% of surveyed organizations reported currently working on AI governance, with the rate rising to approximately 90% among organizations already using AI for process automation, automated decision-making, or data analysis.\n\nGovernance Structures and Roles\nEffective AI governance requires clear assignment of responsibilities across the organization. Research consistently shows that responsibility distributed across multiple teams correlates with positive governance outcomes, though the specific structures vary based on organizational size, industry, and AI maturity.\nExecutive leadership sets the tone for AI governance by establishing risk tolerance, allocating resources, and holding the organization accountable for responsible AI. Without executive support, governance programs lack the authority and resources to be effective. The IAPP survey found that lack of board support was reported as a challenge by only 10% of respondents, suggesting that organizational leadership generally recognizes the importance of AI governance even if other challenges remain.\nMany organizations locate primary AI governance responsibility within an existing function. Survey data shows that approximately 50% of AI governance professionals are assigned to ethics, compliance, privacy, or legal teams. The Mastercard case study from the IAPP report illustrates how AI governance can emerge from the intersection of privacy and data strategy functions, building on existing expertise and processes. IBM built its AI governance program out of the Chief Privacy Office, eventually renaming it the Office of Privacy and Responsible Technology to reflect expanded responsibilities.\nSome organizations create dedicated AI governance roles or teams. Titles like Head of Responsible AI, AI Governance Director, or Chief AI Ethics Officer are becoming more common, particularly at larger organizations. These dedicated roles can provide focused attention but must still collaborate with other functions to be effective.\nCross-functional governance committees bring together stakeholders from different parts of the organization. Survey data found that 39% of organizations have an AI governance committee, with significantly higher rates among organizations using AI extensively. These committees might include representatives from legal, compliance, privacy, security, data science, engineering, business units, risk management, and human resources. They provide diverse perspectives and help ensure governance addresses the full range of AI risks and applications.\nAt the working level, AI project teams bear responsibility for implementing governance requirements in their specific projects. This includes documenting AI systems, conducting required assessments, testing for bias and accuracy, and monitoring deployed systems. Governance programs must make clear what is expected of project teams and provide them with tools and guidance to meet those expectations.\n\n\nThe Importance of Cross-Functional Collaboration\nAI governance cannot succeed as a siloed function. Survey data confirms this: organizations with larger AI governance teams are significantly more likely to involve multiple functions in governance activities. AI risks span technical, legal, ethical, and social domains that no single discipline fully understands.\nTechnical teams understand how AI systems work, what they can and cannot do, and how to test and monitor them. They may not fully appreciate legal requirements, ethical considerations, or how systems affect real people in social contexts. Legal and compliance teams understand regulatory requirements but may not understand technical capabilities and limitations. Business teams understand operational needs and customer impacts but may not appreciate technical or regulatory constraints. Risk management teams understand frameworks for identifying and mitigating risks but may not understand AI-specific risks.\nThe TELUS case study illustrates mature cross-functional collaboration through several mechanisms. Data stewards are in-business data leaders throughout the organization who receive specialized training to act as data and AI champions within their teams. A “Purple Team” combining blue team (defense) and red team (adversarial testing) approaches allows any employee to participate in testing AI systems before release. A Responsible AI Squad brings together AI engineers, policy professionals, and risk professionals for regular collaboration on responsible AI issues.\nBuilding effective collaboration requires organizational effort. It requires establishing forums where cross-functional discussion occurs regularly, not just during crises. It requires creating shared vocabulary so different disciplines can communicate effectively about AI risks and governance. It requires building relationships and trust across organizational boundaries. And it requires leadership that values diverse perspectives rather than treating governance as an obstacle to overcome.\n\n\n\n\n\n\nFigure 1.3: AI Governance Organizational Structure\n\n\n\nFigure 1.3: AI Governance Organizational Structure — Typical governance roles and reporting relationships from executive leadership through working teams.\n\n\nTraining and Awareness\nAI governance depends on people throughout the organization understanding their responsibilities and having the skills to fulfill them. Survey data identified shortage of qualified AI professionals (31%) and lack of understanding of AI and underlying technologies (49%) as significant challenges organizations face in delivering AI governance.\nAll employees benefit from basic AI literacy that helps them understand what AI is, how it affects their work, and their role in responsible AI use. This general awareness helps create a culture where people raise concerns, ask questions, and support governance efforts. The TELUS case study emphasizes company-wide data and AI literacy programming available to all team members regardless of their roles.\nThose who develop or deploy AI systems need more detailed training on governance policies and procedures. They need to understand what assessments are required, what documentation they must create, what testing they must perform, and how to escalate concerns. This training should be practical, explaining not just what is required but how to accomplish it.\nExecutives and board members need training appropriate to their oversight role. They need to understand AI risks and governance approaches well enough to ask good questions, evaluate reports, and make informed decisions about risk tolerance and resource allocation. The survey found that lack of board-level understanding was reported as a challenge by many respondents, suggesting room for improvement in executive AI education.\nProfessional certification provides a way to validate AI governance competence. The IAPP’s AIGP certification, which this book supports, demonstrates knowledge of AI governance principles and practices. Several case study organizations mentioned that the AIGP certification “stands out when recruiting new staff” regardless of the candidate’s background.\n\n\nTailoring Governance to Context\nAI governance is not one-size-fits-all. The appropriate governance approach depends on organizational characteristics including size, industry, AI maturity, and risk tolerance.\nA large financial services company faces different AI governance challenges than a small technology startup. The financial services company operates in a heavily regulated industry with established compliance functions and may be deploying AI in high-stakes contexts like lending decisions. It likely has resources for dedicated governance staff, formal processes, and specialized tools. The startup may move faster, have less regulatory burden for the moment, and deploy AI in initially lower-risk contexts. It may lack resources for extensive governance infrastructure but can build governance into its culture from the beginning.\nSurvey data confirms these patterns: larger organizations by revenue and employee count are more likely to have mature AI governance programs, AI governance committees, larger budgets, and higher confidence in their ability to comply with regulations like the EU AI Act. Organizations with annual revenue below $100 million were significantly underrepresented among those actively working on AI governance, while larger organizations were overrepresented.\nGovernance should evolve as an organization’s AI maturity grows. An organization new to AI may start with basic policies, lightweight review processes, and a single person with part-time governance responsibility. As AI use expands and matures, governance becomes more sophisticated with detailed procedures, specialized tools, dedicated resources, and formal committee structures. The case studies illustrate this evolution: several organizations described building out more professionalized AI governance programs after using AI at smaller scales.\nRisk tolerance also varies across organizations and use cases. Some organizations accept more risk in exchange for innovation speed. Others prioritize caution even at the cost of slower adoption. Within organizations, different use cases warrant different levels of governance intensity based on potential impacts. A low-risk internal productivity tool might receive lighter governance than a customer-facing system making consequential decisions.\n\n\nDeveloper, Deployer, and User Distinctions\nAI governance professionals must understand the different roles organizations play in the AI ecosystem because responsibilities differ based on role.\nAI developers create AI systems, training models and building the technology that others will use. A company that trains its own machine learning models is a developer. A company that builds AI products or platforms for others to use is a developer. Developers bear primary responsibility for the technical characteristics of AI systems including their accuracy, fairness, security, and reliability. Regulatory frameworks increasingly impose specific obligations on developers, including documentation requirements, technical requirements, testing obligations, and transparency duties.\nAI deployers take AI systems created by developers and make them available for use in specific contexts. A company that purchases an AI hiring tool from a vendor and uses it in their recruiting process is a deployer. A company that integrates a third-party language model into their customer service application is a deployer. Deployers bear responsibility for using AI systems appropriately in their specific context. Even if a developer created a well-designed system, a deployer can misuse it or deploy it in inappropriate contexts. The EU AI Act allocates specific obligations to deployers of high-risk AI systems including conducting impact assessments, ensuring human oversight, and informing affected individuals.\nAI users are the people who interact with deployed AI systems in the course of their work or daily life. Employees using an AI tool in their work are users. Consumers interacting with AI-powered products are users. Users generally bear less governance responsibility but should understand AI capabilities and limitations and should be able to report concerns.\nMany organizations play multiple roles simultaneously or sequentially. A company might develop some AI systems internally while deploying AI systems purchased from vendors. A company might be a deployer for its internal use of AI while being a developer of AI products it sells to customers. Governance programs must address responsibilities in all roles the organization plays and must manage relationships with external parties playing other roles in the AI value chain.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#policies-across-the-ai-lifecycle",
    "href": "chapters/01-foundations.html#policies-across-the-ai-lifecycle",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.7 Policies Across the AI Lifecycle",
    "text": "1.7 Policies Across the AI Lifecycle\nAI governance requires policies that address the entire AI lifecycle from initial conception through retirement. These policies create the framework within which AI development and deployment occurs.\n\nUse Case Assessment and Approval\nBefore developing or deploying an AI system, organizations should assess whether the proposed use case is appropriate. This assessment considers whether AI is necessary and suitable for the intended purpose, what risks the use case presents, whether those risks can be adequately managed, and whether the use case aligns with organizational values and strategy.\nUse case assessment policies should establish criteria for evaluating proposed AI applications and specify approval processes. High-risk uses involving consequential decisions, vulnerable populations, sensitive contexts, or novel capabilities warrant more scrutiny than lower-risk applications. Policies should specify who has authority to approve different categories of use cases, what documentation is required for approval, and what ongoing oversight approved uses require.\nSome organizations establish explicit lists of prohibited use cases that will not be approved regardless of circumstances, as well as presumptively approved low-risk uses that can proceed with minimal review. The Boston Consulting Group case study describes a screening process that identifies potentially high-risk use cases for review by a senior-level Responsible AI Council, while lower-risk cases proceed through streamlined processes.\n\n\nRisk Management\nAI risk management policies establish how the organization identifies, assesses, and mitigates AI risks. This includes processes for risk assessment during development and deployment, criteria for categorizing risk levels, requirements for risk mitigation measures, procedures for ongoing risk monitoring, and escalation paths for identified risks.\nEffective AI risk management integrates with existing enterprise risk management frameworks rather than operating in isolation. AI risks relate to security risk, privacy risk, operational risk, compliance risk, and reputational risk. Organizations should leverage existing risk management capabilities and expertise while adding AI-specific considerations.\nThe NIST AI Risk Management Framework provides a widely referenced structure for AI risk management, organized around four core functions: Govern (establish accountability and culture), Map (understand context and identify risks), Measure (assess and analyze risks), and Manage (prioritize and address risks). Organizations can use this framework as a starting point while tailoring it to their specific circumstances.\n\n\nData Governance\nAI systems depend on data, and data governance policies establish requirements for data used in AI. This includes requirements for data quality ensuring training data is accurate, complete, and fit for purpose. It includes requirements for data sourcing ensuring the organization has appropriate legal rights to use data for AI purposes. It includes requirements for data privacy ensuring personal information is protected appropriately. It includes requirements for data documentation establishing lineage and provenance.\nData governance for AI extends existing data governance programs but adds AI-specific considerations. Data that is adequate for business analytics may be inadequate for training AI models that make consequential individual decisions. Data biases that might be acceptable in aggregate statistics become problematic when they affect individual outcomes. Organizations should evaluate existing data policies and extend them as needed for AI contexts.\n\n\nDocumentation and Record-Keeping\nDocumentation policies establish what information must be recorded about AI systems and how that information must be maintained. Good documentation supports accountability, enables oversight, facilitates troubleshooting, and demonstrates compliance.\nDocumentation requirements typically include system purpose and intended use, training data characteristics and sources, model architecture and key parameters, testing results and performance metrics, known limitations and risks, deployment procedures, and operational monitoring approaches. The level of detail should be appropriate to the risk level and complexity of the system.\nModel cards and datasheets have emerged as standard formats for AI documentation. A model card documents a trained model’s purpose, performance characteristics, limitations, and ethical considerations. A datasheet documents a dataset’s composition, collection process, intended uses, and potential biases. Organizations should consider adopting these formats while adapting them to their specific needs.\n\n\nThird-Party Management\nMany organizations use AI systems developed by external parties rather than building everything internally. Third-party management policies establish how the organization evaluates, selects, contracts with, and monitors AI vendors and partners.\nThese policies should address due diligence requirements for evaluating potential AI vendors, contractual provisions that allocate responsibilities and ensure access to necessary information, ongoing monitoring requirements for third-party AI systems, and incident response procedures when third-party systems cause problems.\nThe increasing use of cloud-based AI services and pre-trained models from external providers makes third-party management increasingly important. Organizations must manage risks that originate outside their boundaries while maintaining accountability for AI systems they deploy regardless of where those systems were developed.\n\n\nIncident Management\nDespite best efforts, AI systems sometimes cause harm or malfunction. Incident management policies establish how the organization detects, responds to, and learns from AI incidents.\nThese policies should define what constitutes an AI incident, establish processes for reporting and escalating incidents, specify response procedures including the ability to pause or withdraw problematic systems, require root cause analysis and documentation, and establish processes for communicating with affected parties and regulators as appropriate.\nThe EU AI Act requires providers of high-risk AI systems to report serious incidents to relevant authorities. Organizations subject to this requirement must have processes to detect incidents, assess their severity, and make required reports within specified timeframes.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#the-oecd-framework-for-ai-classification",
    "href": "chapters/01-foundations.html#the-oecd-framework-for-ai-classification",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.8 The OECD Framework for AI Classification",
    "text": "1.8 The OECD Framework for AI Classification\nBeyond definitions and principles, governance professionals benefit from frameworks that help classify and analyze AI systems. The OECD Framework for the Classification of AI Systems provides a comprehensive approach adopted by many countries and organizations.\nThe framework classifies AI systems along five dimensions, each with multiple characteristics that help characterize specific systems and their governance implications.\nThe first dimension addresses People and Planet, considering the potential of AI systems to affect human rights, well-being, society, and the environment. This dimension examines who uses the system, who is affected by it, whether use is optional or compelled, and what types of impacts the system may have on individuals, groups, and the environment.\nThe second dimension addresses Economic Context, describing the sectoral and business environment in which an AI system operates. This includes the industry sector, business function, critical or non-critical nature of the application, scale of deployment, and the organization’s experience with the technology.\nThe third dimension addresses Data and Input, characterizing the data and information that feeds into the AI system. This includes data provenance and quality, collection methods, data structure and format, whether data includes personal information, and how data changes over time.\nThe fourth dimension addresses the AI Model itself, examining how the system processes inputs to generate outputs. This includes the type of machine learning or other AI approach, how the model was trained, what objectives it optimizes, and how performance is measured.\nThe fifth dimension addresses Task and Output, characterizing what the system does and produces. This includes the types of tasks performed, the nature of outputs, the level of autonomy in decision-making, and how outputs influence subsequent actions.\nUsing this framework helps governance professionals systematically analyze AI systems, compare systems across dimensions, identify governance-relevant characteristics, and communicate about AI systems in consistent terms. The framework does not prescribe specific governance requirements but provides a structured approach to understanding what is being governed.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#chapter-1-summary",
    "href": "chapters/01-foundations.html#chapter-1-summary",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.9 Chapter 1 Summary",
    "text": "1.9 Chapter 1 Summary\nThis chapter established the foundational knowledge that AI governance professionals need. AI governance exists because artificial intelligence has moved from research laboratories into daily organizational operations, bringing both substantial benefits and significant risks that require specialized management.\nAI encompasses engineered systems that generate outputs like predictions, recommendations, content, or decisions based on learned patterns rather than explicit programming alone. Machine learning, the dominant approach to building AI systems today, enables powerful capabilities but also creates governance challenges related to opacity, data dependency, and unpredictable emergent behavior. The distinction between generative and predictive AI matters for governance because these categories have different risk profiles requiring different approaches.\nAI requires specialized governance because of characteristics that distinguish it from traditional software: complexity and opacity that challenge transparency requirements, autonomy and speed that require new approaches to oversight, data dependency that extends governance concerns to data pipelines, probabilistic outputs that require managing uncertainty, and emergent behavior that challenges pre-deployment testing. Understanding AI as a socio-technical system recognizes that technical components interact with social contexts, human behaviors, and institutional structures in ways that purely technical governance cannot address.\nAI can cause harms to individuals through inaccurate or discriminatory decisions in consequential contexts, to groups through systematic patterns that perpetuate historical disadvantage, to organizations through reputational damage and regulatory penalties and cultural erosion, to society through misinformation and surveillance and power concentration, and to the environment through energy consumption and hardware lifecycle impacts.\nResponsible AI principles provide direction for governance programs: fairness requires proactive attention to equitable treatment, safety and reliability require anticipating and preventing harms, privacy and security require protecting information and systems, transparency and explainability require openness about AI use and decisions, accountability requires clear assignment of responsibility, and human oversight requires meaningful human control over AI systems.\nOrganizations establish AI governance through structures including executive sponsorship, designated governance functions, cross-functional committees, and project-level responsibilities. Success requires cross-functional collaboration because AI risks span technical, legal, ethical, and social domains. Training and awareness programs build the organizational capability to implement governance effectively. Governance approaches should be tailored to organizational context including size, industry, maturity, and risk tolerance. Understanding distinctions between developers, deployers, and users helps allocate responsibilities appropriately.\nPolicies across the AI lifecycle address use case assessment, risk management, data governance, documentation, third-party management, and incident response. The OECD Framework for AI Classification provides a structured approach to analyzing AI systems across dimensions of people and planet, economic context, data and input, AI model characteristics, and task and output.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#chapter-1-review-questions",
    "href": "chapters/01-foundations.html#chapter-1-review-questions",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.10 Chapter 1 Review Questions",
    "text": "1.10 Chapter 1 Review Questions\n\nA large healthcare organization is evaluating a deep learning system that analyzes medical images to assist radiologists in detecting tumors. The system achieves higher accuracy than human radiologists on benchmark tests but the organization cannot explain in simple terms why the system flags particular images. Which characteristic of AI systems creates the most significant governance challenge in this situation?\nAn AI hiring tool trained on a company’s historical hiring decisions shows strong predictive accuracy for job performance. However, analysis reveals that the tool recommends male candidates at significantly higher rates than female candidates with similar qualifications. This pattern reflects the company’s historical hiring practices in a male-dominated industry. Which category of AI harm does this situation primarily illustrate?\nA financial services company is implementing an AI governance program. The Chief Privacy Officer has been assigned responsibility for AI governance, but she finds that she cannot effectively govern AI systems without regular input from the data science team, legal counsel, business unit leaders, and the security function. This situation illustrates which fundamental principle of AI governance program design?\nA content moderation AI system reviews millions of social media posts daily, removing those that violate platform policies. The system occasionally removes legitimate content (false positives) and occasionally fails to remove violating content (false negatives). Which characteristic of AI systems makes it impractical to have humans review every decision the system makes?\nA retail company uses an AI recommendation system that personalizes product suggestions based on customer browsing and purchase history. The company is preparing for compliance with the EU AI Act. Based on the risk classification framework, how would this system most likely be classified?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/01-foundations.html#references",
    "href": "chapters/01-foundations.html#references",
    "title": "1  Understanding the Foundations of AI Governance",
    "section": "1.11 References",
    "text": "1.11 References\nIAPP. AI Governance in Practice Report 2025. International Association of Privacy Professionals, 2025.\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.\nNational Institute of Standards and Technology. AI Risk Management Framework 1.0. NIST AI 100-1, 2023.\nOrganisation for Economic Co-operation and Development. OECD Framework for the Classification of AI Systems. OECD Digital Economy Papers No. 323, 2022.\nOrganisation for Economic Co-operation and Development. Recommendation of the Council on Artificial Intelligence. OECD/LEGAL/0449, 2019.\nEuropean Parliament and Council. Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union, 2024.\nMitchell, Margaret, et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\nGebru, Timnit, et al. “Datasheets for Datasets.” Communications of the ACM 64, no. 12 (2021).\nBuolamwini, Joy and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Proceedings of Machine Learning Research 81 (2018).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Foundations of AI Governance</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html",
    "href": "chapters/02-legal-frameworks.html",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "",
    "text": "2.1 Introduction\nAI systems do not operate in a legal vacuum. They interact with existing laws never designed with AI in mind, face new AI-specific regulations emerging worldwide, and are shaped by voluntary frameworks and standards that establish expectations for responsible practice. AI governance professionals must understand this complex landscape to help their organizations deploy AI lawfully and responsibly.\nThis chapter surveys the legal and regulatory frameworks that apply to AI. It begins with existing laws that predate AI but apply to it, including privacy laws, anti-discrimination laws, consumer protection laws, and product liability laws. It then examines AI-specific regulations, with particular attention to the EU AI Act as the most comprehensive AI law to date. It surveys emerging AI regulations in other jurisdictions. Finally, it examines voluntary standards and frameworks that, while not legally binding, shape expectations and may become legally relevant as courts and regulators reference them.\nA word of caution: this chapter describes the legal landscape as it exists at the time of writing, but that landscape is evolving rapidly. New regulations are being enacted, existing regulations are being implemented and interpreted, and enforcement priorities are being established. AI governance professionals should monitor developments in jurisdictions relevant to their organizations and seek qualified legal counsel for specific situations. The goal here is to build understanding of the legal framework, not to provide legal advice.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#how-privacy-laws-apply-to-ai",
    "href": "chapters/02-legal-frameworks.html#how-privacy-laws-apply-to-ai",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.2 How Privacy Laws Apply to AI",
    "text": "2.2 How Privacy Laws Apply to AI\nPrivacy laws were enacted before the current AI era, but they apply powerfully to AI systems that process personal data. Understanding this intersection is essential because AI systems frequently rely on personal data for training, operation, and outputs.\n\nThe GDPR Framework\nThe European Union’s General Data Protection Regulation provides a comprehensive framework for data protection that applies when AI systems process personal data of individuals in the EU. Several GDPR provisions have particular relevance to AI.\nThe lawfulness requirements of Article 6 require a legal basis for processing personal data. For AI systems, this means organizations must identify a valid basis for collecting data used to train models, for processing data during model operation, and for any personal data in model outputs. Consent, contractual necessity, legal obligation, vital interests, public task, and legitimate interests each have specific conditions that may or may not fit AI use cases. The legitimate interests basis, commonly relied upon for AI, requires balancing organizational interests against the rights and interests of data subjects.\nThe data minimization principle requires that personal data be adequate, relevant, and limited to what is necessary for the processing purposes. This principle creates tension with AI systems that often perform better with more data. Organizations must be able to justify the data they collect and use, demonstrating that it is genuinely necessary for the AI application’s purposes.\nPurpose limitation requires that personal data be collected for specified, explicit, and legitimate purposes and not further processed in a manner incompatible with those purposes. When organizations use data originally collected for one purpose to train AI models for different purposes, they must analyze whether this constitutes compatible further processing or requires a new legal basis.\nData subject rights under the GDPR have specific implications for AI. The right to access allows individuals to obtain information about processing of their data, including in AI systems. The right to rectification allows correction of inaccurate data, which may require model retraining if the data was used for training. The right to erasure raises complex questions about whether and how personal data can be removed from trained models. The right not to be subject to solely automated decision-making with legal or similarly significant effects, found in Article 22, directly addresses AI-driven decisions.\nArticle 22 deserves particular attention. It establishes that data subjects have the right not to be subject to decisions based solely on automated processing, including profiling, that produce legal effects or similarly significantly affect them. Exceptions exist for contractual necessity, explicit consent, or legal authorization, but even when these exceptions apply, organizations must implement suitable safeguards including the right to obtain human intervention, express views, and contest the decision.\nCourts and regulators have not fully resolved what constitutes solely automated decision-making versus human-AI collaboration, what effects are similarly significant to legal effects, or what constitutes meaningful human intervention. Organizations should take a conservative approach, providing human review of consequential AI-driven decisions affecting individuals and ensuring that human review is genuinely meaningful rather than perfunctory.\nData Protection Impact Assessments are required under Article 35 when processing is likely to result in high risk to individuals’ rights and freedoms. AI systems that make automated decisions about individuals or that systematically evaluate personal aspects typically trigger this requirement. DPIAs require systematic description of processing operations, assessment of necessity and proportionality, assessment of risks, and measures to address risks.\n\n\nPrivacy Laws in the United States\nThe United States lacks a comprehensive federal privacy law comparable to the GDPR, but a patchwork of federal and state laws applies to AI systems.\nThe California Consumer Privacy Act, as amended by the California Privacy Rights Act, provides California residents with rights regarding their personal information including the right to know what information is collected, the right to delete personal information, the right to opt out of sale or sharing of personal information, and the right to limit use of sensitive personal information. CPRA added provisions specifically addressing automated decision-making, requiring businesses to provide information about the logic involved in automated decision-making and meaningful information about the consequences of such decisions. California regulations implementing these provisions are still developing.\nOther state comprehensive privacy laws including those in Virginia, Colorado, Connecticut, Utah, and additional states enact similar rights with variations in scope and requirements. Several include provisions addressing automated decision-making, profiling, or requiring opt-out mechanisms. Organizations operating across multiple states face a complex compliance landscape.\nFederal sector-specific laws apply to AI in their respective domains. The Health Insurance Portability and Accountability Act governs protected health information, which is frequently used in healthcare AI. The Gramm-Leach-Bliley Act governs financial data. The Children’s Online Privacy Protection Act governs data about children under 13. The Fair Credit Reporting Act governs consumer reports and the information used to create them, with significant implications for AI used in credit, employment, and insurance decisions.\nThe Federal Trade Commission Act’s prohibition on unfair and deceptive practices has been applied to AI-related conduct. The FTC has issued guidance on AI and has brought enforcement actions against companies whose AI practices it deemed unfair or deceptive. The agency has indicated it will scrutinize AI systems for accuracy, bias, and transparency and will use its enforcement authority against problematic AI practices.\n\n\nData Protection Impact Assessments and AI Conformity Assessments\nAI governance professionals should understand the relationship between privacy-focused Data Protection Impact Assessments and AI-focused conformity assessments increasingly required by AI regulations.\nA DPIA under the GDPR focuses on data protection risks, assessing processing operations from the perspective of privacy and data subject rights. It examines legal bases, data minimization, purpose limitation, data subject rights, security measures, and data transfers.\nAn AI conformity assessment under the EU AI Act focuses on AI-specific requirements, examining whether a high-risk AI system meets requirements for risk management, data governance, transparency, human oversight, accuracy, robustness, and cybersecurity.\nWhen a high-risk AI system processes personal data, organizations will likely need to conduct both assessments. These assessments overlap significantly: both examine data, both assess risks, both consider safeguards. The DPIA can form a foundation for the conformity assessment, and organizations should coordinate these processes to avoid duplicative effort and ensure consistent conclusions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#how-other-existing-laws-apply-to-ai",
    "href": "chapters/02-legal-frameworks.html#how-other-existing-laws-apply-to-ai",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.3 How Other Existing Laws Apply to AI",
    "text": "2.3 How Other Existing Laws Apply to AI\nBeyond privacy laws, numerous existing legal frameworks apply to AI systems. These laws were not designed with AI in mind but reach AI applications within their scope.\n\nAnti-Discrimination Laws\nLaws prohibiting discrimination based on protected characteristics apply when AI systems make or influence decisions in covered contexts. In the United States, Title VII of the Civil Rights Act prohibits employment discrimination on the basis of race, color, religion, sex, or national origin. The Americans with Disabilities Act prohibits discrimination against qualified individuals with disabilities. The Age Discrimination in Employment Act prohibits discrimination against individuals 40 and older. The Fair Housing Act prohibits discrimination in housing. The Equal Credit Opportunity Act prohibits discrimination in credit decisions.\nThese laws apply regardless of whether discrimination is intentional or results from facially neutral practices with discriminatory effects. An AI hiring tool that has a disparate impact on protected groups may violate Title VII even if it was not designed to discriminate. An AI credit model that disadvantages protected groups may violate the Equal Credit Opportunity Act even if the variables it uses are not explicitly related to protected characteristics.\nRegulatory agencies have issued guidance on applying these laws to AI. The Equal Employment Opportunity Commission has issued guidance on the use of AI and automated systems in employment decisions, emphasizing that employers remain responsible for discriminatory outcomes even when they rely on AI tools developed by vendors. The Consumer Financial Protection Bureau has addressed AI in the context of fair lending requirements.\nBeyond the United States, discrimination laws in other jurisdictions similarly apply to AI systems. The EU’s Charter of Fundamental Rights prohibits discrimination, and national laws throughout Europe implement anti-discrimination requirements that reach AI applications.\n\n\nConsumer Protection Laws\nConsumer protection laws prohibit unfair and deceptive practices, and these prohibitions apply to AI-powered products and services.\nThe FTC Act prohibits unfair and deceptive acts or practices in or affecting commerce. The FTC has applied this authority to AI contexts, taking action against companies for deceptive claims about AI capabilities, for unfair use of AI to harm consumers, and for failing to adequately protect AI systems and the data they use. The agency has signaled that it considers AI bias and discrimination to be potential unfairness violations.\nState consumer protection laws similarly prohibit unfair and deceptive practices. State attorneys general have used these authorities in AI contexts and can be expected to continue doing so.\nThe EU’s Unfair Commercial Practices Directive prohibits unfair business-to-consumer commercial practices, including misleading actions, misleading omissions, and aggressive practices. This directive applies to AI-powered interactions with consumers.\nThe EU’s Digital Services Act imposes transparency requirements on online platforms, including requirements to disclose when content is AI-generated and to explain how recommendation systems work. These requirements reflect growing regulatory attention to the role of AI in shaping online content and behavior.\n\n\nProduct Liability Laws\nProduct liability laws impose responsibility for defective products that cause harm. The application of these laws to AI is evolving and raises novel questions.\nTraditional product liability distinguishes between manufacturing defects, design defects, and failure to warn. Applying these categories to AI is not straightforward: AI defects may emerge from training data, learned parameters, or deployment context rather than fitting neatly into traditional categories.\nThe EU is updating its product liability framework to address AI. The revised Product Liability Directive explicitly includes software within the definition of products and addresses AI-specific issues. The proposed AI Liability Directive would ease the burden of proof for claimants harmed by AI systems by establishing rebuttable presumptions linking fault to damage and by providing disclosure mechanisms allowing claimants to access relevant evidence.\nIn the United States, product liability law varies by state, and courts are beginning to address AI-specific issues without comprehensive legislative guidance.\n\n\nIntellectual Property Laws\nIntellectual property laws intersect with AI in multiple ways relevant to governance.\nCopyright law addresses both the use of copyrighted material to train AI systems and the copyright status of AI-generated outputs. Training AI systems on copyrighted works raises questions about fair use in the United States or equivalent exceptions in other jurisdictions. Multiple lawsuits challenging AI training on copyrighted material are pending. The copyright status of AI-generated outputs remains uncertain; the US Copyright Office has indicated that copyright requires human authorship, limiting protection for AI-generated content.\nPatent law faces similar questions about AI inventorship. Patent offices in most jurisdictions have determined that AI systems cannot be listed as inventors on patent applications, requiring human inventorship. This does not prevent patenting inventions developed with AI assistance if a human qualifies as inventor.\nTrade secret law may protect trained AI models, training data, and other AI-related assets if they derive economic value from being secret and are subject to reasonable secrecy measures.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#the-eu-ai-act",
    "href": "chapters/02-legal-frameworks.html#the-eu-ai-act",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.4 The EU AI Act",
    "text": "2.4 The EU AI Act\nThe European Union’s Artificial Intelligence Act, adopted in 2024 with phased implementation through 2027, is the world’s most comprehensive AI-specific regulation. Understanding the AI Act is essential for organizations that operate in or serve the EU market and provides insight into regulatory approaches other jurisdictions may follow.\n\nRisk Classification Framework\nThe AI Act takes a risk-based approach, classifying AI systems into categories with different regulatory requirements based on their potential for harm.\nProhibited AI practices are banned entirely. These include AI systems that deploy subliminal, manipulative, or deceptive techniques to distort behavior in ways causing significant harm; systems that exploit vulnerabilities related to age, disability, or social or economic situation; social scoring systems by public authorities; systems that infer emotions in workplace and educational settings except for medical or safety reasons; certain forms of real-time remote biometric identification in public spaces for law enforcement; systems creating or expanding facial recognition databases through untargeted scraping; and AI systems that create risk assessments predicting criminal behavior based solely on profiling or personality traits.\nHigh-risk AI systems are permitted but subject to extensive requirements. The AI Act identifies high-risk systems in two ways. First, certain AI systems intended as safety components of products already subject to EU harmonization legislation are high-risk. Second, the Act lists specific use cases that qualify as high-risk, including biometric identification and categorization; management and operation of critical infrastructure; education and vocational training; employment, worker management, and access to self-employment; access to and enjoyment of essential private and public services including credit, insurance, and emergency services; law enforcement; migration, asylum, and border control; and administration of justice and democratic processes.\nHigh-risk AI systems must comply with requirements spanning risk management, data governance, technical documentation, record-keeping, transparency and provision of information to users, human oversight, accuracy, robustness, and cybersecurity.\nLimited risk AI systems face transparency obligations but not the full high-risk requirements. These include AI systems that interact with natural persons, emotion recognition systems, and AI systems generating synthetic content.\nMinimal risk AI systems face no specific requirements under the Act beyond general requirements and encouragement to adopt voluntary codes of conduct.\n\n\n\n\n\n\nFigure 2.1: EU AI Act Risk Classification Pyramid\n\n\n\nFigure 2.1: EU AI Act Risk Classification Pyramid — The four-tier risk system from prohibited practices at the apex to minimal-risk systems at the base.\n\n\nRequirements for High-Risk AI Systems\nOrganizations developing or deploying high-risk AI systems must understand the detailed requirements the AI Act imposes.\nRisk management must be established as a continuous iterative process throughout the system lifecycle. This includes identifying and analyzing known and foreseeable risks, estimating and evaluating risks that may emerge when the system is used as intended and under reasonably foreseeable misuse, adopting appropriate risk management measures, and ensuring that residual risks are acceptable.\nData governance requirements address the quality of training, validation, and testing data. Data must be relevant, representative, accurate, and complete. Appropriate data governance and management practices must be implemented addressing data collection, data preparation, data labeling, and gap analysis.\nTechnical documentation must be prepared before the system is placed on the market or put into service and must be kept up to date. Documentation must demonstrate compliance with requirements and provide authorities with necessary information for assessment.\nRecord-keeping requirements mandate that high-risk AI systems include logging capabilities enabling automatic recording of events relevant to identifying risks, facilitating post-market monitoring, and enabling investigation of incidents.\nTransparency and information provision require that systems be designed to enable users to interpret outputs and use the system appropriately. Instructions must accompany systems including information about provider identity, system characteristics and capabilities, intended purpose, accuracy and robustness metrics, known circumstances of foreseeable misuse, human oversight measures, and expected lifetime.\nHuman oversight measures must be designed into systems to enable human oversight during use. Depending on circumstances, this may include ability to fully understand system capabilities, remain aware of automation bias, correctly interpret outputs, decide not to use outputs, intervene on operation, or stop the system.\nAccuracy, robustness, and cybersecurity requirements ensure systems achieve appropriate levels of accuracy for their intended purpose, are resilient to errors or inconsistencies, and resist unauthorized attempts to alter their use or performance.\n\n\nAllocation of Obligations\nThe AI Act allocates obligations among different actors in the AI value chain.\nProviders, essentially developers who place systems on the market or put them into service under their own name, bear the primary compliance burden. They must ensure systems meet requirements, conduct conformity assessments, prepare documentation, implement quality management systems, and carry out post-market monitoring.\nDeployers, organizations using AI systems under their authority, must use systems according to instructions, ensure human oversight, monitor operation for risks, inform providers of incidents, keep logs, inform affected individuals that they are subject to high-risk AI, and conduct fundamental rights impact assessments for certain uses.\nImporters must verify provider compliance before placing systems on the EU market. Distributors must verify systems bear required markings and documentation. Product manufacturers integrating high-risk AI into their products bear provider obligations.\n\n\nGeneral Purpose AI Models\nThe AI Act includes provisions addressing general purpose AI models, sometimes called foundation models, that can be used for many different applications rather than a single specific purpose.\nAll providers of general purpose AI models must prepare and maintain technical documentation, provide information and documentation to downstream providers, establish policies to comply with copyright law, and publish a summary of training content.\nProviders of general purpose AI models with systemic risk face additional obligations. A model has systemic risk if it has high impact capabilities indicated by computation used for training above a threshold, or by Commission decision based on criteria including number of users, degree of autonomy, and access to data. Providers of such models must conduct model evaluations including adversarial testing, assess and mitigate systemic risks, track and report serious incidents, and ensure adequate cybersecurity.\n\n\nEnforcement and Penalties\nThe AI Act establishes an enforcement framework with significant penalties for non-compliance.\nThe AI Office within the European Commission coordinates enforcement across member states and has direct enforcement power for general purpose AI model provisions. National competent authorities designated by member states enforce provisions applicable to organizations in their jurisdictions.\nPenalties follow a tiered structure. Violations involving prohibited AI practices can result in fines up to 35 million euros or 7% of worldwide annual turnover, whichever is higher. Violations of requirements for high-risk systems can result in fines up to 15 million euros or 3% of worldwide turnover. Supply of incorrect information to authorities can result in fines up to 7.5 million euros or 1% of worldwide turnover.\n\n\nImplementation Timeline\nThe AI Act has a phased implementation timeline. Prohibitions on banned AI practices apply from February 2025. Provisions on general purpose AI models and establishment of governance structures apply from August 2025. Most provisions including requirements for high-risk systems apply from August 2026. Requirements for high-risk AI systems that are safety components of products subject to other EU harmonization legislation apply from August 2027.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#other-ai-specific-regulatory-developments",
    "href": "chapters/02-legal-frameworks.html#other-ai-specific-regulatory-developments",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.5 Other AI-Specific Regulatory Developments",
    "text": "2.5 Other AI-Specific Regulatory Developments\nWhile the EU AI Act is the most comprehensive AI law to date, other jurisdictions are developing their own approaches to AI regulation.\n\nUnited States Federal Developments\nThe United States has not enacted comprehensive federal AI legislation as of early 2025, but significant federal activity shapes AI governance.\nExecutive Order 14110 on Safe, Secure, and Trustworthy Artificial Intelligence, issued in October 2023, directed federal agencies to take numerous actions addressing AI safety and security, privacy, civil rights, consumer protection, workforce impacts, innovation, and international leadership. The order requires safety testing for powerful AI models, addresses AI in critical infrastructure, directs agencies to address AI-related discrimination, and establishes governance structures within the federal government.\nFederal agencies are using existing authorities to address AI. The FTC has issued guidance and brought enforcement actions addressing AI under its unfair and deceptive practices authority. The EEOC has issued guidance on AI in employment decisions. Banking regulators have issued model risk management guidance applicable to AI systems used by financial institutions. The FDA regulates AI medical devices.\nThe NIST AI Risk Management Framework, while voluntary, represents authoritative federal guidance on AI risk management. The White House Blueprint for an AI Bill of Rights articulates principles including safety, protection from algorithmic discrimination, data privacy, notice and explanation, and human alternatives.\n\n\nUnited States State Laws\nIn the absence of comprehensive federal AI legislation, states have enacted AI-specific laws creating a complex compliance landscape.\nColorado enacted the Colorado AI Act in 2024, becoming the first state to enact comprehensive AI legislation addressing high-risk AI systems. The law requires deployers of high-risk AI systems to use reasonable care to protect consumers from algorithmic discrimination. Deployers must conduct impact assessments, provide public statements describing AI systems, and notify consumers when AI makes or substantially influences consequential decisions about them. Developers must provide deployers with documentation necessary to conduct impact assessments and evaluate AI systems. The law takes effect in 2026.\nNew York City Local Law 144 requires employers and employment agencies using automated employment decision tools to conduct annual independent bias audits, publish audit results, and provide candidates notice of AI use.\nThe Illinois Artificial Intelligence Video Interview Act requires employers using AI to analyze video interviews to notify applicants that AI will be used, explain how the AI works, and obtain consent. Maryland prohibits using facial recognition in hiring without applicant consent.\nAdditional states have enacted or are considering AI-specific legislation addressing employment, insurance, housing, healthcare, and other domains.\n\n\nInternational Developments\nAI regulation is a global phenomenon with significant developments across jurisdictions.\nCanada’s Artificial Intelligence and Data Act, proposed as part of broader digital legislation, would establish requirements for organizations responsible for high-impact AI systems. Requirements would include conducting impact assessments, establishing risk mitigation measures, maintaining records, and publishing plain-language descriptions of high-impact AI systems.\nChina has enacted multiple AI-related regulations. The Provisions on the Management of Algorithmic Recommendations requires providers of algorithmic recommendation services to ensure transparency, protect user choice, and avoid harmful discrimination. The Provisions on the Management of Deep Synthesis regulates deepfakes and synthetic content. The Interim Measures for the Management of Generative Artificial Intelligence Services addresses generative AI services offered to the public, requiring that such services not generate illegal content, obtain proper consent for personal data, ensure training data quality, and cooperate with authorities.\nSingapore has developed governance frameworks emphasizing practical guidance and voluntary adoption. The Model AI Governance Framework provides guidance on responsible AI development and deployment. AI Verify provides a testing framework and toolkit for organizations to demonstrate responsible AI practices. The Veritas framework addresses AI in financial services specifically. Singapore’s approach emphasizes enabling innovation while encouraging responsible practices.\nJapan has pursued a pro-innovation approach emphasizing guidance rather than hard regulation, though specific rules apply in certain sectors. The Social Principles of Human-Centric AI and accompanying governance guidelines establish principles for trustworthy AI.\nThe United Kingdom initially signaled a light-touch regulatory approach relying on sector regulators to address AI within their existing mandates. However, policy continues to develop.\nBrazil, India, South Korea, and numerous other jurisdictions are developing AI policies and regulations, creating an evolving global landscape.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#industry-standards-and-frameworks",
    "href": "chapters/02-legal-frameworks.html#industry-standards-and-frameworks",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.6 Industry Standards and Frameworks",
    "text": "2.6 Industry Standards and Frameworks\nBeyond legal requirements, industry standards and voluntary frameworks shape AI governance expectations. While not legally binding in themselves, these standards influence what is considered reasonable practice, may be referenced in regulations or contracts, and provide practical guidance for implementation.\n\nOECD AI Principles\nThe OECD AI Principles, adopted in 2019 and updated subsequently, articulate values for trustworthy AI that have been adopted by over forty countries and have influenced legal frameworks including the EU AI Act.\nThe principles articulate five complementary values. First, AI should contribute to inclusive growth, sustainable development, and well-being, augmenting human capabilities, advancing inclusion, reducing inequalities, and protecting the environment. Second, AI actors should respect human-centered values and fairness including human rights, democratic values, diversity, fairness, and social justice. Third, there should be transparency and explainability so stakeholders understand AI systems and can challenge decisions affecting them. Fourth, AI systems should be robust, secure, and safe throughout their lifecycle with appropriate risk management. Fifth, organizations and individuals developing or operating AI should be accountable for proper functioning of AI systems.\nThe OECD has also developed the Framework for the Classification of AI Systems, which provides structured dimensions for analyzing AI systems. The OECD maintains the AI Policy Observatory tracking AI policy developments globally and providing comparative analysis.\n\n\nNIST AI Risk Management Framework\nThe National Institute of Standards and Technology AI Risk Management Framework, published in 2023, provides voluntary guidance for managing AI risks. While developed in the United States, it has influenced approaches internationally.\nThe framework organizes AI risk management around four core functions. The Govern function addresses the organizational culture, structures, policies, and accountability for AI risk management. The Map function addresses understanding context and identifying risks. The Measure function addresses assessing and analyzing identified risks through testing, metrics, and ongoing evaluation. The Manage function addresses prioritizing and responding to risks through controls, treatment options, monitoring, and communication.\n\n\n\n\n\n\nFigure 2.2: NIST AI Risk Management Framework\n\n\n\nFigure 2.2: NIST AI Risk Management Framework — The four core functions: Govern (central), Map, Measure, and Manage.\nThe accompanying NIST AI RMF Playbook provides detailed guidance for implementing each function, including suggested actions, transparency and documentation practices, and references to standards and frameworks.\nNIST has also established the Assessing Risks and Impacts of AI (ARIA) program, which develops methodologies, tools, metrics, and measurements for evaluating AI safety and trustworthiness. ARIA provides practical resources for organizations seeking to operationalize the AI RMF, including testing approaches for evaluating AI system performance, fairness, and robustness. The program bridges the gap between high-level risk management principles and concrete assessment practices.\n\n\nISO AI Standards\nThe International Organization for Standardization has developed AI-specific standards that provide frameworks for AI governance.\nISO/IEC 42001:2023 specifies requirements for an AI management system. It provides a framework for organizational governance of AI, addressing leadership commitment, planning, support, operation, performance evaluation, and improvement. Organizations can certify their AI management systems against this standard, demonstrating commitment to systematic AI governance.\nISO/IEC 22989:2022 establishes AI concepts and terminology, providing common vocabulary for discussing AI systems consistently.\nAdditional ISO standards address bias in AI systems, robustness of neural networks, transparency, explainability, and other AI governance topics.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#connecting-legal-requirements-to-governance-practice",
    "href": "chapters/02-legal-frameworks.html#connecting-legal-requirements-to-governance-practice",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.7 Connecting Legal Requirements to Governance Practice",
    "text": "2.7 Connecting Legal Requirements to Governance Practice\nAI governance professionals must translate legal requirements into practical organizational processes.\n\nImpact Assessments\nImpact assessments are a common mechanism across multiple legal frameworks for identifying and addressing AI risks before deployment.\nData Protection Impact Assessments under the GDPR focus on privacy and data protection risks. AI impact assessments, required under laws like the Colorado AI Act and emerging in other jurisdictions, focus on AI-specific risks including algorithmic discrimination, accuracy, transparency, and fairness. Fundamental rights impact assessments are required under the EU AI Act for certain deployers of high-risk AI systems.\nThese various assessment requirements overlap and can often be coordinated. Organizations should develop impact assessment processes that address all applicable requirements efficiently while ensuring each specific legal requirement is satisfied.\n\n\nDocumentation and Record-Keeping\nMultiple legal frameworks impose documentation requirements that governance programs must address.\nThe EU AI Act requires technical documentation for high-risk systems addressing system design, capabilities, testing, and risk management. The GDPR requires documentation of processing activities and of DPIAs. Various sector-specific laws impose additional documentation requirements.\nModel cards, datasheets for datasets, and similar documentation frameworks provide structured approaches to AI documentation that can help satisfy multiple legal requirements.\n\n\nTransparency and Notice\nMany legal frameworks require disclosure to individuals about AI use.\nThe EU AI Act requires disclosure when individuals interact with certain AI systems and when AI generates synthetic content. The GDPR requires information about automated decision-making. Consumer protection laws may require disclosure of AI involvement in products or services. Employment laws in various jurisdictions require disclosure of AI use in hiring.\nOrganizations should identify all applicable disclosure requirements and implement processes ensuring required disclosures are made appropriately.\n\n\nHuman Oversight\nLegal frameworks increasingly require human oversight of AI systems, but the specific requirements vary.\nThe GDPR’s Article 22 requires the right to obtain human intervention in automated decisions with significant effects. The EU AI Act requires human oversight measures in high-risk AI systems. Various sector-specific requirements mandate human review in particular contexts.\nOrganizations must design AI systems and processes to enable the required human oversight while maintaining operational efficiency.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#chapter-2-summary",
    "href": "chapters/02-legal-frameworks.html#chapter-2-summary",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.8 Chapter 2 Summary",
    "text": "2.8 Chapter 2 Summary\nThis chapter surveyed the legal and regulatory frameworks that apply to AI systems. These frameworks include existing laws not designed with AI in mind that nonetheless apply to it, new AI-specific regulations, and voluntary standards and frameworks that shape expectations for responsible practice.\nPrivacy laws including the GDPR and various US laws apply when AI systems process personal data, imposing requirements for lawful processing bases, data minimization, purpose limitation, and data subject rights. The GDPR’s Article 22 specifically addresses automated decision-making, providing rights related to decisions based solely on automated processing with significant effects.\nAnti-discrimination laws prohibit discrimination based on protected characteristics regardless of whether discrimination results from AI or human decision-making. Consumer protection laws prohibit unfair and deceptive practices including in AI-powered products and services. Product liability laws are evolving to address AI defects and harms. Intellectual property laws address both the use of copyrighted material for AI training and the status of AI-generated outputs.\nThe EU AI Act provides the most comprehensive AI-specific regulation to date, classifying AI systems by risk level and imposing extensive requirements on high-risk systems. Prohibited AI practices are banned. High-risk AI systems must comply with requirements for risk management, data governance, documentation, transparency, human oversight, accuracy, and robustness. General purpose AI models face separate requirements. The Act allocates obligations among providers, deployers, and other actors and establishes significant penalties for non-compliance.\nOther jurisdictions are developing AI regulations including US states like Colorado with comprehensive AI acts and cities like New York with targeted requirements. Canada, China, Singapore, Japan, and other countries are developing their own approaches ranging from comprehensive legislation to sectoral regulation to voluntary frameworks.\nIndustry standards and frameworks including the OECD AI Principles, NIST AI Risk Management Framework, and ISO standards provide guidance for responsible AI that, while voluntary, shapes expectations and may become legally relevant.\nTranslating legal requirements into practice requires impact assessments addressing multiple frameworks’ requirements, documentation meeting various legal standards, transparency and notice satisfying disclosure obligations, human oversight as legally required, and compliance programs addressing multiple jurisdictions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#chapter-2-review-questions",
    "href": "chapters/02-legal-frameworks.html#chapter-2-review-questions",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.9 Chapter 2 Review Questions",
    "text": "2.9 Chapter 2 Review Questions\n\nAn organization is deploying an AI system that will analyze job applicants’ resumes and video interviews to recommend candidates for human review. The system will be used in the European Union. Which of the following assessment requirements will most likely apply to this deployment?\nA US-based company is developing a large language model that will be offered commercially to customers worldwide, including in the European Union. Under the EU AI Act, which category of requirements is most likely to apply to this company’s activities?\nAn AI system makes recommendations about loan applications that are then reviewed by human loan officers who make final decisions. In approximately 95% of cases, the human loan officers follow the AI’s recommendation. Under the GDPR’s Article 22, how should the organization analyze this arrangement?\nAn organization is preparing for compliance with multiple AI-related regulations including the EU AI Act, the Colorado AI Act, and GDPR requirements. The organization is considering how to conduct impact assessments efficiently. Which approach best addresses this situation?\nA company operating in the United States uses an AI hiring tool purchased from a vendor. The tool has been shown in testing to have disparate impact on protected groups. Who bears legal responsibility for this disparate impact under US anti-discrimination law?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/02-legal-frameworks.html#references",
    "href": "chapters/02-legal-frameworks.html#references",
    "title": "2  Understanding How Laws, Standards, and Frameworks Apply to AI",
    "section": "2.10 References",
    "text": "2.10 References\nEuropean Parliament and Council. Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union, 2024.\nEuropean Parliament and Council. Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation). Official Journal of the European Union, 2016.\nNational Institute of Standards and Technology. AI Risk Management Framework 1.0. NIST AI 100-1, 2023.\nOrganisation for Economic Co-operation and Development. Recommendation of the Council on Artificial Intelligence. OECD/LEGAL/0449, 2019.\nIAPP. Global AI Governance Law and Policy Series. International Association of Privacy Professionals, 2025.\nEqual Employment Opportunity Commission. The Americans with Disabilities Act and the Use of Software, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees. Technical Assistance Document, 2022.\nFederal Trade Commission. Aiming for truth, fairness, and equity in your company’s use of AI. FTC Business Blog, 2021.\nInternational Organization for Standardization. ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management system, 2023.\nState of Colorado. Senate Bill 24-205, Concerning Consumer Protections in Interactions with Artificial Intelligence Systems, 2024.\nExecutive Order 14110, Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Federal Register, 2023.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding How Laws, Standards, and Frameworks Apply to AI</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html",
    "href": "chapters/03-development.html",
    "title": "3  Governing AI Development",
    "section": "",
    "text": "3.1 Introduction\nThe development of AI systems presents governance challenges that differ fundamentally from traditional software development. Where conventional software follows predetermined logic that developers explicitly specify, AI systems learn patterns from data in ways that may be difficult to predict or explain. Where traditional software testing can verify correct behavior against defined specifications, AI testing must grapple with probabilistic outputs, edge cases that cannot be fully enumerated, and performance that may vary across different populations. Where conventional software documentation describes what the code does, AI documentation must also address what the training data contains, what the model learned, and how performance was validated.\nThis chapter examines governance throughout the AI development lifecycle, from initial conception of a use case through design, data preparation, model training, testing, documentation, and release. Each stage presents distinct governance considerations and requires specific controls. The chapter emphasizes that governance is not a gate at the end of development but a continuous practice woven throughout the process. Effective governance enables development by providing clarity about requirements and expectations, not merely constraining it through review and approval.\nThe material in this chapter maps primarily to Domain III of the AIGP Body of Knowledge, which addresses the knowledge and skills required to govern the design, development, and validation of AI systems. Organizations may be developers creating AI systems for their own use, developers creating AI systems for others to deploy, or both. The governance practices described here apply regardless of whether the ultimate deployer is internal or external, though the specific documentation and communication requirements may differ.\nFigure 3.1: AI Development Lifecycle with Governance Touchpoints — Governance activities mapped to each development phase with feedback loops.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#introduction",
    "href": "chapters/03-development.html#introduction",
    "title": "3  Governing AI Development",
    "section": "",
    "text": "Figure 3.1: AI Development Lifecycle with Governance Touchpoints",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#defining-the-business-context-and-use-case",
    "href": "chapters/03-development.html#defining-the-business-context-and-use-case",
    "title": "3  Governing AI Development",
    "section": "3.2 Defining the Business Context and Use Case",
    "text": "3.2 Defining the Business Context and Use Case\nEvery AI system begins with a purpose: a problem to solve, a decision to support, or a capability to provide. Governance starts here, before any data is collected or model is trained, by ensuring the proposed use case is appropriate, well-defined, and aligned with organizational values and risk tolerance.\n\nUse Case Intake and Initial Assessment\nOrganizations should establish a formal process for proposing and evaluating potential AI use cases. This intake process serves multiple purposes: it creates visibility into AI activities across the organization, it ensures proposed uses receive appropriate scrutiny before resources are committed, and it builds an inventory of AI systems that supports ongoing governance and regulatory compliance.\nAn effective intake process captures essential information about the proposed use case. What problem is the AI system intended to solve? What decisions will it make or support? Who will use the system and who will be affected by its outputs? What data will be required? What is the expected benefit, and how will success be measured? What are the potential risks if the system performs poorly or is misused?\nThis information enables initial risk classification. Not every AI system requires the same level of governance intensity. A system that recommends internal meeting times based on calendar availability presents different risks than a system that influences hiring decisions or medical diagnoses. Organizations should establish risk tiers with corresponding governance requirements, directing intensive oversight toward high-risk applications while allowing lower-risk applications to proceed with lighter-touch governance.\nThe EU AI Act’s risk classification provides one model for this tiering, distinguishing prohibited uses, high-risk uses requiring extensive compliance, limited-risk uses requiring transparency, and minimal-risk uses with no specific requirements. Organizations can adapt this framework to their context, potentially adding categories or adjusting criteria based on their specific risk landscape and values.\n\n\nEvaluating Appropriateness\nBeyond risk classification, organizations should evaluate whether AI is appropriate for the proposed use case at all. AI is not the right solution for every problem, and governance should help organizations avoid pursuing AI applications that are unlikely to succeed or that present risks disproportionate to their benefits. The question is not merely “can we include AI?” but “should we include AI?”—a distinction that separates sensible solutions from AI hype.\nSeveral questions help evaluate appropriateness. Is there sufficient quality data available to train an effective model? AI systems learn from data, and if relevant data does not exist, is not accessible, or is not representative of the population the system will serve, the AI project may be doomed from the start. Can the problem be adequately specified in terms an AI system can optimize? Some problems involve nuanced judgment that resists reduction to measurable objectives. Is the expected benefit proportionate to the risks and costs? Some AI applications provide marginal improvements over simpler alternatives while introducing significant new risks.\nOrganizations should also consider whether the use case aligns with organizational values and public commitments. A company that has publicly committed to certain principles around AI use should evaluate proposed applications against those commitments. An organization operating in a sensitive domain should consider how stakeholders would perceive the proposed AI use if it became public.\n\n\nDefining Success Criteria\nBefore development begins, organizations should define how success will be measured. What accuracy is required? What fairness criteria must be satisfied? What latency or throughput is needed for operational viability? What explainability is required for the intended users and affected individuals?\nThese success criteria become requirements that guide development and testing. Without clear criteria defined upfront, development teams may optimize for metrics that do not reflect actual organizational needs, and governance reviews may lack clear standards against which to evaluate systems.\nSuccess criteria should address both technical performance and broader considerations. Technical criteria might include accuracy metrics, false positive and false negative rates, performance across different subgroups, response time, and scalability. Broader criteria might include user acceptance, integration with existing workflows, compliance with applicable regulations, and alignment with organizational values.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#conducting-impact-assessments",
    "href": "chapters/03-development.html#conducting-impact-assessments",
    "title": "3  Governing AI Development",
    "section": "3.3 Conducting Impact Assessments",
    "text": "3.3 Conducting Impact Assessments\nImpact assessments provide a structured process for identifying, analyzing, and planning to mitigate the risks an AI system may present. Various regulatory frameworks require impact assessments, and even where not legally mandated, they represent governance best practice for consequential AI systems.\n\nTypes of Impact Assessments\nDifferent types of impact assessments address different risk categories, and a given AI system may require multiple assessments.\nData Protection Impact Assessments, required under the GDPR when processing is likely to result in high risk to individuals’ rights and freedoms, focus on privacy and data protection risks. They examine the lawfulness of processing, data minimization, data subject rights, and security measures. AI systems that process personal data for profiling, automated decision-making, or systematic monitoring typically trigger DPIA requirements.\nAlgorithmic Impact Assessments focus specifically on risks arising from algorithmic decision-making, including discrimination, inaccuracy, lack of transparency, and inappropriate automation of human judgment. Various frameworks and emerging regulations specify algorithmic impact assessment requirements. Canada’s Algorithmic Impact Assessment Tool, while designed for government use, provides a model that private organizations can adapt.\nFundamental Rights Impact Assessments, required under the EU AI Act for certain deployers of high-risk AI systems, examine impacts on fundamental rights including dignity, freedoms, equality, solidarity, citizens’ rights, and justice. These assessments consider how the AI system might affect the rights of individuals and groups.\nHuman Rights Impact Assessments take a broader view, examining potential impacts on the full range of internationally recognized human rights. The UN Guiding Principles on Business and Human Rights provide a framework for human rights due diligence that organizations can apply to AI systems.\nOrganizations may consolidate these assessments into a unified AI impact assessment process that addresses privacy, algorithmic, fundamental rights, and human rights considerations together, or they may maintain separate processes depending on organizational structure and regulatory requirements.\n\n\nConducting an Effective Assessment\nEffective impact assessments require input from diverse perspectives. Technical teams understand what the system does and how it works. Legal and compliance teams understand regulatory requirements and potential liabilities. Business teams understand the operational context and intended use. User experience researchers understand how people will interact with the system. Subject matter experts understand the domain in which the system operates. And affected communities can provide perspective on potential impacts that internal stakeholders might miss.\nThe assessment should describe the AI system thoroughly: its purpose, how it works, what data it uses, what outputs it produces, and how those outputs will be used in decision-making. It should identify potential risks across categories including accuracy, fairness, transparency, privacy, security, and misuse. For each identified risk, it should assess likelihood and severity, considering both individual and aggregate impacts.\nThe assessment should then identify measures to mitigate identified risks. These might include technical measures like bias testing and monitoring, process measures like human review requirements, or policy measures like use restrictions. For each mitigation measure, the assessment should evaluate expected effectiveness and any residual risk that remains after mitigation.\nFinally, the assessment should document the conclusion: whether to proceed with the AI system, proceed with modifications, or not proceed. This conclusion should be justified based on the analysis, and the decision-maker should be identified and accountable.\n\n\nWhen to Conduct Assessments\nImpact assessments should occur early in the development process, when findings can still influence design decisions, and should be updated as the system evolves. An assessment conducted only at the end of development, when significant resources have already been invested, creates pressure to approve systems regardless of identified risks.\nOrganizations should also reassess when circumstances change materially. If the AI system is modified significantly, if it is deployed in new contexts, or if monitoring reveals unexpected impacts, the original assessment may no longer be valid. Governance processes should trigger reassessment when appropriate conditions are met.\n\n\n\n\n\n\nFigure 3.2: Impact Assessment Process Flow\n\n\n\nFigure 3.2: Impact Assessment Process Flow — From intake through risk classification, assessment, mitigation planning, and decision.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#identifying-applicable-legal-requirements",
    "href": "chapters/03-development.html#identifying-applicable-legal-requirements",
    "title": "3  Governing AI Development",
    "section": "3.4 Identifying Applicable Legal Requirements",
    "text": "3.4 Identifying Applicable Legal Requirements\nBefore development proceeds, organizations must identify the legal requirements that apply to the proposed AI system. This analysis depends on the system’s purpose, the data it uses, the decisions it influences, the jurisdictions where it operates, and the roles the organization plays in the AI value chain.\n\nMapping Legal Obligations\nThe legal analysis should systematically consider applicable frameworks. Privacy laws apply when the system processes personal data. The GDPR applies if data subjects are in the EU; CCPA/CPRA applies if data subjects are California residents; other privacy laws apply based on their respective scope. Requirements may include lawful processing bases, data subject rights, data protection impact assessments, and cross-border transfer restrictions.\nAnti-discrimination laws apply when the system influences decisions in covered contexts. Employment decisions trigger Title VII and similar laws; credit decisions trigger ECOA and fair lending requirements; housing decisions trigger Fair Housing Act requirements. Compliance requires avoiding both intentional discrimination and facially neutral practices with unjustified disparate impact.\nSector-specific regulations apply based on the domain. Healthcare AI may require FDA approval and HIPAA compliance. Financial services AI may require compliance with model risk management guidance. AI in education, transportation, telecommunications, or other regulated sectors must comply with applicable sectoral requirements.\nAI-specific regulations apply based on risk classification and jurisdictional reach. The EU AI Act applies to AI systems placed on the EU market or whose outputs are used in the EU. State laws like the Colorado AI Act apply to systems used in those jurisdictions. Requirements depend on whether the system is high-risk, general purpose, or another category.\n\n\nDocumenting Legal Requirements\nThe legal analysis should produce clear documentation of applicable requirements that can guide development. This documentation should specify what laws apply, what obligations they impose, and how those obligations translate to concrete requirements for the AI system.\nFor example, if GDPR Article 22 applies, requirements might include: provide meaningful information about the logic involved; implement safeguards including right to human intervention; conduct DPIA. If the EU AI Act high-risk requirements apply, requirements might include: implement risk management system; ensure data governance for training data; prepare technical documentation per Annex IV; implement logging capabilities; design for human oversight; achieve appropriate accuracy and robustness.\nThis documentation becomes a checklist against which the developed system can be evaluated and provides evidence of compliance efforts if questions arise later.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#designing-and-building-the-ai-model",
    "href": "chapters/03-development.html#designing-and-building-the-ai-model",
    "title": "3  Governing AI Development",
    "section": "3.5 Designing and Building the AI Model",
    "text": "3.5 Designing and Building the AI Model\nWith use case approved, impact assessment completed, and legal requirements identified, development proceeds to system design and model building. Governance during this phase ensures that design choices reflect governance requirements and that the model is built in ways that support transparency, testing, and ongoing oversight.\n\nDesign Considerations for Governance\nDesign decisions made early in development have lasting governance implications. Certain design choices make systems easier to govern; others make governance more difficult.\nExplainability is significantly influenced by model architecture. Simpler models like linear regression or decision trees produce outputs that can be traced to specific input features, enabling meaningful explanations of individual decisions. Complex models like deep neural networks can achieve higher accuracy on some tasks but produce outputs that are difficult to explain. The appropriate tradeoff depends on the use case: a content recommendation system may tolerate opacity that would be unacceptable in a credit decisioning system.\nModularity affects the ability to test, monitor, and update systems. A modular design that separates data preprocessing, model inference, and output processing allows each component to be tested and updated independently. A monolithic design that combines these functions makes targeted testing and improvement more difficult.\nLogging and auditability must be designed into systems from the start. What events should be logged? What information should be captured about each prediction or decision? How long should logs be retained? These questions are easier to answer and implement during initial design than to retrofit later.\nHuman oversight mechanisms must be designed to support the required level of human involvement. If human review is required for certain decisions, the system should surface relevant information to support that review and provide mechanisms for humans to override or adjust system outputs.\n\n\nModel Selection and Development\nThe choice of modeling approach should reflect both technical requirements and governance considerations. More complex models may achieve higher accuracy but at the cost of explainability and potentially increased risk of unexpected behavior. Simpler models may be easier to understand and govern but may not achieve required performance levels.\nDevelopment practices should support governance objectives. Version control for code, data, and model artifacts enables traceability and rollback. Documentation during development, not just at the end, captures design decisions and their rationales. Code review and pair programming can catch issues early. Separation of training and validation data prevents overfitting and supports honest performance evaluation.\nOrganizations developing multiple AI systems benefit from standardized development practices, shared infrastructure, and reusable components. These not only improve efficiency but also support governance by creating consistency that enables meaningful oversight and comparison across systems.\n\n\nAddressing Common Failure Modes\nAI governance professionals should understand common failure modes so they can ensure development processes address them.\nBrittleness refers to AI systems that perform well on training data but fail on inputs that differ even slightly from training distribution. Brittle systems may produce confident but incorrect outputs when deployed in real-world conditions that differ from training conditions. Addressing brittleness requires diverse training data, robust testing including adversarial examples, and monitoring for distribution shift after deployment.\nHallucination, particularly relevant for generative AI, refers to systems producing confident but false outputs. A language model may generate plausible-sounding but fabricated facts, citations, or claims. Addressing hallucination requires techniques like retrieval augmentation, output verification, and user education about system limitations.\nConcept drift occurs when the relationships the model learned from historical data no longer hold because the world has changed. A fraud detection model trained on past fraud patterns may miss new fraud tactics. A demand forecasting model trained before a market disruption may produce inaccurate forecasts after. Addressing drift requires monitoring for performance degradation and processes for model retraining when necessary.\nFeedback loops occur when model outputs influence future training data in ways that amplify biases or errors. A predictive policing model that directs officers to certain neighborhoods generates more arrests in those neighborhoods, which then reinforces the model’s predictions. Addressing feedback loops requires understanding how model outputs affect the data-generating process and implementing safeguards against self-reinforcing patterns.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#governing-data-for-ai",
    "href": "chapters/03-development.html#governing-data-for-ai",
    "title": "3  Governing AI Development",
    "section": "3.6 Governing Data for AI",
    "text": "3.6 Governing Data for AI\nData is foundational to AI systems, and data governance for AI extends traditional data governance with AI-specific considerations. The data used to train AI systems shapes what those systems learn and how they perform. Garbage in, garbage out applies with particular force to machine learning.\n\nData Quality\nTraining data must be fit for purpose. This means more than simply being accurate; it means being relevant, representative, complete, and timely for the specific AI application.\nRelevance requires that training data actually contain the patterns the model needs to learn. If the goal is to predict customer churn, training data must include features that genuinely relate to churn behavior, not just data that happens to be available.\nRepresentativeness requires that training data reflect the population on which the model will be deployed. A medical imaging model trained only on images from one demographic group may perform poorly on other groups. A language model trained only on formal written text may struggle with casual speech or regional dialects.\nCompleteness requires sufficient examples across the range of situations the model will encounter. A classification model needs adequate examples of each class. A regression model needs examples spanning the range of target values. Rare but important cases need sufficient representation for the model to learn them.\nTimeliness requires that training data reflect current conditions. Patterns in data from five years ago may not hold today. Using stale data may produce models optimized for historical conditions that no longer exist.\n\n\nData Provenance and Lineage\nOrganizations must track where data comes from and how it has been processed. Data provenance documents the origins of data: who collected it, when, how, and for what purpose. Data lineage documents the transformations data undergoes: what processing, filtering, aggregation, or derivation has been applied.\nThis documentation serves multiple purposes. It enables assessment of data quality and fitness for purpose. It supports compliance with legal requirements about data sourcing and use. It facilitates debugging when model issues arise. It enables reproduction of results for validation or audit.\nFor training data specifically, provenance documentation should address: the original sources of the data, any licenses or restrictions on its use, whether personal data is included and if so the legal basis for its use in AI training, what preprocessing or cleaning has been applied, and any known issues or limitations.\n\n\nBias in Training Data\nTraining data can contain biases that models learn and perpetuate. These biases may arise from multiple sources.\nHistorical bias exists when data reflects past decisions or conditions that were themselves biased. Hiring data from a company with historically discriminatory practices encodes that discrimination. Loan performance data from a system that denied loans to creditworthy applicants from certain groups reflects those denials, not the true creditworthiness of those groups.\nRepresentation bias exists when certain groups are underrepresented in training data. If training data contains fewer examples from certain populations, models may learn those populations less well and perform worse for them.\nMeasurement bias exists when the features or labels in training data are measured or recorded differently for different groups. If one group is more heavily monitored than another, that group will have more recorded incidents regardless of actual behavior.\nAggregation bias exists when data that should be disaggregated is combined, obscuring important differences between subgroups.\nAddressing data bias requires first understanding what biases may be present, then deciding how to respond. Options include collecting additional data to improve representation, applying techniques to rebalance or adjust data, using algorithmic approaches to mitigate bias during training, or accepting certain data and addressing bias through post-processing or process controls.\n\n\nData Documentation\nThe Datasheets for Datasets framework, developed by researchers at Google and other institutions, provides a structured approach to documenting datasets used for AI. A datasheet documents:\nMotivation: why the dataset was created, who created it, and who funded it.\nComposition: what instances the dataset contains, how many, and what data is associated with each instance; whether there is a label or target; whether the dataset contains sensitive information.\nCollection process: how data was acquired, who collected it, what mechanisms were used, whether informed consent was obtained.\nPreprocessing: what preprocessing, cleaning, or labeling was done; whether raw data is available.\nUses: what tasks the dataset has been used for, what uses are recommended, and what uses should be avoided.\nDistribution: how the dataset is distributed and under what license.\nMaintenance: who maintains the dataset and how updates are communicated.\nOrganizations should adopt standardized dataset documentation practices, adapting frameworks like Datasheets to their needs. This documentation supports governance review, enables appropriate reuse, and provides evidence of diligence.\n\n\n\n\n\n\nFigure 3.3: Data Governance for AI\n\n\n\nFigure 3.3: Data Governance for AI — The data pipeline with governance checkpoints from collection through training use.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#training-and-testing-ai-models",
    "href": "chapters/03-development.html#training-and-testing-ai-models",
    "title": "3  Governing AI Development",
    "section": "3.7 Training and Testing AI Models",
    "text": "3.7 Training and Testing AI Models\nOnce data is prepared, model training proceeds, followed by testing to validate that the trained model meets requirements. Governance during this phase ensures that training is conducted appropriately and that testing is rigorous enough to provide confidence in model performance.\n\nTraining Practices\nModel training should follow established practices that support reproducibility, traceability, and quality.\nEnvironment management ensures that training occurs in controlled, documented environments. What hardware is used? What software versions? What random seeds? Documenting these details enables reproduction of results and investigation of issues.\nExperiment tracking captures the parameters and results of training runs. What hyperparameters were used? What was the training loss trajectory? What validation performance was achieved? This tracking enables comparison of approaches and provides evidence of the development process.\nSeparation of training, validation, and test data ensures honest performance evaluation. If the same data is used for training and evaluation, performance metrics will be inflated and will not reflect true generalization ability. Standard practice is to split data into training data used to fit model parameters, validation data used to tune hyperparameters and make development decisions, and test data used only for final evaluation.\nRegularization and other techniques to prevent overfitting help ensure models generalize to new data rather than merely memorizing training examples. Overfitting produces models that perform well on training data but poorly on new data.\n\n\nTesting and Validation\nTesting AI systems differs from testing traditional software. Traditional software testing verifies that code behaves correctly according to specifications. AI testing must assess probabilistic performance on data the system has not seen before, a fundamentally different challenge.\nFunctional testing verifies that the system produces outputs in the expected format and responds appropriately to various inputs including edge cases. This is similar to traditional software testing and should cover the range of inputs the system may receive.\nPerformance testing evaluates model accuracy and related metrics on held-out test data. Metrics should align with the success criteria defined during use case specification. Common metrics include accuracy, precision, recall, F1 score, AUC-ROC for classification; mean squared error, mean absolute error, R-squared for regression; and task-specific metrics for other applications.\nFairness testing evaluates whether performance is equitable across demographic groups and whether the system produces discriminatory outcomes. This requires defining appropriate fairness metrics for the application context, identifying relevant demographic groups, and computing metrics disaggregated by group. Common fairness metrics include demographic parity, equalized odds, and calibration across groups. Because different fairness metrics can be mathematically incompatible, organizations must make deliberate choices about which criteria to prioritize.\nRobustness testing evaluates model behavior under challenging conditions including adversarial inputs, distribution shift, and edge cases. Adversarial testing deliberately crafts inputs designed to cause model errors. Stress testing evaluates behavior under heavy load or resource constraints. Boundary testing evaluates behavior at the edges of input ranges.\nExplainability testing evaluates whether model decisions can be explained adequately for the intended context. Can the system provide meaningful explanations of individual predictions? Are those explanations accurate and useful? Do they meet regulatory requirements for explanation?\nSecurity testing evaluates vulnerability to attacks including data poisoning, model extraction, and adversarial examples. AI systems face unique security threats that traditional security testing may not address.\n\n\nDocumentation of Testing\nTesting should produce documentation that enables review and provides evidence of diligence. This documentation should include:\nTest plan describing what testing was performed, what data was used, what metrics were evaluated, and what pass/fail criteria were applied.\nTest results reporting actual performance on each metric, disaggregated as appropriate.\nAnalysis of results explaining what the results mean, whether they meet requirements, and what limitations or concerns remain.\nSign-off from appropriate reviewers indicating that testing was adequate and results are acceptable.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#documentation-throughout-development",
    "href": "chapters/03-development.html#documentation-throughout-development",
    "title": "3  Governing AI Development",
    "section": "3.8 Documentation Throughout Development",
    "text": "3.8 Documentation Throughout Development\nDocumentation is not a final step before release but a continuous activity throughout development. Good documentation supports governance review, enables ongoing oversight, facilitates incident investigation, and demonstrates compliance.\n\nModel Cards\nThe Model Cards framework, developed by researchers at Google, provides a structured approach to documenting trained models. A model card documents:\nModel details: developer, version, type, training procedures, parameters, license.\nIntended use: primary intended uses, primary intended users, out-of-scope uses that should be avoided.\nFactors: groups, instrumentation, and environments for which model behavior may differ.\nMetrics: performance metrics used, why they were chosen, and any known limitations of those metrics.\nEvaluation data: description of evaluation datasets, preprocessing applied, and how data was obtained.\nTraining data: description of training datasets, though this may be less detailed than evaluation data for proprietary reasons.\nQuantitative analyses: disaggregated performance metrics across relevant factors.\nEthical considerations: known limitations, potential risks, and appropriate use guidance.\nCaveats and recommendations: guidance for users about appropriate and inappropriate applications.\nOrganizations should adapt the Model Card framework to their needs, potentially adding sections required by regulations or internal policies. Model cards should be living documents updated as models are modified or as new information emerges about performance or limitations.\n\n\nTechnical Documentation for Compliance\nVarious regulations impose specific documentation requirements. The EU AI Act requires technical documentation for high-risk AI systems covering:\nGeneral description of the AI system, its intended purpose, and how it interacts with hardware and software.\nDetailed description of system elements including algorithms, data, and training processes.\nInformation about data used for training, validation, and testing, including data governance measures and examination of biases.\nMetrics used to measure accuracy, robustness, and compliance with requirements, along with potentially discriminatory impacts.\nDescription of human oversight measures.\nDescription of the risk management system and risk assessments performed.\nDescription of changes made during system lifecycle.\nOrganizations should design their documentation practices to satisfy applicable regulatory requirements, generating required documentation as part of the development process rather than attempting to reconstruct it retroactively.\n\n\nMaintaining Documentation\nDocumentation must be maintained as systems evolve. Version control should track changes to documentation alongside changes to code and models. Review processes should ensure documentation remains accurate and complete. Retention policies should ensure documentation is available for the required period, which for regulatory compliance may extend years after system deployment.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#release-monitoring-and-maintenance",
    "href": "chapters/03-development.html#release-monitoring-and-maintenance",
    "title": "3  Governing AI Development",
    "section": "3.9 Release, Monitoring, and Maintenance",
    "text": "3.9 Release, Monitoring, and Maintenance\nThe completion of development does not end governance responsibility. Release decisions, deployment monitoring, and ongoing maintenance all require governance attention.\n\nRelease Readiness Assessment\nBefore an AI system is released for deployment, a structured assessment should verify that all governance requirements have been satisfied. This assessment might address:\nHave all required impact assessments been completed and approved?\nHave all identified legal requirements been addressed?\nDoes testing demonstrate that the system meets performance and fairness requirements?\nIs documentation complete and accurate?\nAre monitoring capabilities in place to detect issues after deployment?\nHave operational procedures been established for incident response?\nHave affected stakeholders been appropriately notified?\nThis assessment should involve reviewers beyond the development team, providing independent verification that requirements have been met. The result should be a documented release decision with clear accountability.\n\n\nDeployment Monitoring\nOnce deployed, AI systems require ongoing monitoring to detect issues that may not have appeared during testing. Monitoring should address:\nTechnical performance: Is the system functioning correctly? Are there errors, outages, or performance degradation?\nModel performance: Is the model achieving expected accuracy? Are there signs of performance degradation over time?\nFairness: Are outcomes equitable across demographic groups? Are patterns emerging that suggest bias?\nDrift: Is the distribution of inputs changing in ways that might affect model validity? Are the relationships the model learned still accurate?\nUser feedback: Are users raising concerns about system behavior? Are affected individuals reporting problems?\nMonitoring should be automated where possible, with alerts when metrics exceed thresholds. But automated monitoring should be complemented by periodic human review that can identify issues automated systems might miss.\n\n\nModel Maintenance and Retraining\nAI models may need to be updated over time as data distributions change, as performance degrades, as new requirements emerge, or as better approaches become available. Governance should address when and how retraining occurs.\nOrganizations should define criteria that trigger model updates: performance degradation beyond a threshold, detection of significant drift, regulatory changes requiring updates, or scheduled periodic reviews.\nUpdates should go through appropriate governance review. A minor adjustment may require less scrutiny than the initial release, but significant changes should be evaluated for their potential impacts, and documentation should be updated accordingly.\nThe process of updating models while they are deployed in production raises additional considerations. How is the transition managed? How is consistency maintained if some users receive outputs from the old model while others receive outputs from the new model? How is the update rolled back if problems are detected?\n\n\nIncident Management\nDespite best efforts, AI systems sometimes cause harm or malfunction. Incident management processes should enable rapid detection, response, and learning.\nDetection requires monitoring systems and reporting channels that surface issues quickly. Users, affected individuals, and internal stakeholders should have clear paths to report concerns.\nResponse should follow established procedures that address immediate harms, investigate root causes, and implement fixes. For serious incidents, this may require suspending or withdrawing the AI system until issues are resolved. Response plans should be developed before incidents occur, not improvised during crises.\nThe EU AI Act requires providers of high-risk AI systems to report serious incidents to relevant authorities within defined timeframes. Organizations must have processes to identify incidents meeting reporting thresholds and to make required notifications.\nLearning from incidents improves future systems. Root cause analysis should identify not just immediate causes but systemic factors that allowed the incident to occur. Findings should inform updates to governance processes, development practices, and training materials.\n\n\n\n\n\n\nFigure 3.4: Release and Monitoring Process\n\n\n\nFigure 3.4: Release and Monitoring Process — Release decisions, continuous monitoring, and incident response pathways.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#privacy-enhancing-technologies",
    "href": "chapters/03-development.html#privacy-enhancing-technologies",
    "title": "3  Governing AI Development",
    "section": "3.10 Privacy-Enhancing Technologies",
    "text": "3.10 Privacy-Enhancing Technologies\nPrivacy-enhancing technologies offer technical approaches to enable AI development and deployment while reducing privacy risks. AI governance professionals should understand these technologies and when they may be appropriate.\n\nAnonymization and Pseudonymization\nAnonymization transforms data so that individuals cannot be identified, even by combining the data with other information. If data is truly anonymized, it falls outside the scope of data protection laws like the GDPR, which apply only to personal data.\nAchieving true anonymization is difficult, particularly for rich datasets. Research has demonstrated that individuals can often be re-identified from supposedly anonymized data by combining with other sources. Traditional techniques like removing direct identifiers may not be sufficient. More sophisticated approaches like k-anonymity, l-diversity, and t-closeness provide stronger guarantees but may reduce data utility.\nPseudonymization replaces direct identifiers with pseudonyms, allowing data to be linked to individuals if the pseudonymization key is available but not otherwise. Pseudonymized data remains personal data under the GDPR, but pseudonymization is recognized as a security measure that may support certain processing activities.\nFor AI training data, organizations should carefully evaluate anonymization claims and consider whether data can truly be said to be anonymized given the risk of re-identification.\n\n\nDifferential Privacy\nDifferential privacy is a mathematical framework for quantifying and limiting the privacy loss from statistical queries or model training. A differentially private analysis provides guarantees that the output would be essentially the same whether any individual’s data was included or not, limiting what can be learned about any individual from the results.\nDifferential privacy can be applied to AI training, producing models with formal guarantees about privacy loss. The tradeoff is between privacy (measured by the privacy budget or epsilon parameter) and utility (model accuracy). Stronger privacy guarantees typically require accepting some reduction in accuracy.\nMajor technology companies have deployed differentially private AI training for certain applications, and the technique is an active area of research. Governance programs should consider differential privacy for applications involving sensitive data, though the technical complexity may require specialized expertise.\n\n\nFederated Learning\nFederated learning is an approach to training AI models on decentralized data without centralizing that data. Instead of collecting data into a central repository for training, federated learning trains models locally on data where it resides, sharing only model updates rather than raw data.\nThis approach can reduce privacy risks by keeping sensitive data local. For example, a hospital collaboration could train a medical AI model without any hospital sharing patient data with others; each hospital trains on its local data and shares only learned model parameters.\nFederated learning has limitations. It requires appropriate infrastructure and coordination among participants. It may be vulnerable to certain attacks that infer information about training data from model updates. It does not eliminate privacy concerns but rather changes their character.\n\n\nSynthetic Data\nSynthetic data is artificially generated data that mimics the statistical properties of real data without containing actual individual records. Training AI models on synthetic data rather than real data can reduce privacy risks if the synthetic data generation process provides adequate privacy protections.\nThe challenge is generating synthetic data that captures enough of the real data’s structure to train effective models while not capturing so much that it enables individual identification. Sophisticated approaches use generative models to produce synthetic data, potentially combining with differential privacy to provide formal guarantees.\nSynthetic data is an active area of development with growing commercial offerings. Governance programs should evaluate synthetic data approaches for applications where privacy constraints limit use of real data.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#chapter-3-summary",
    "href": "chapters/03-development.html#chapter-3-summary",
    "title": "3  Governing AI Development",
    "section": "3.11 Chapter 3 Summary",
    "text": "3.11 Chapter 3 Summary\nThis chapter examined governance throughout AI development from initial use case definition through release and ongoing maintenance.\nGovernance begins before development with use case intake and assessment that evaluates appropriateness, classifies risk, and defines success criteria. Impact assessments provide structured processes for identifying and planning to mitigate risks across categories including privacy, algorithmic fairness, and fundamental rights. Legal analysis identifies applicable requirements that become design constraints.\nDesign decisions have lasting governance implications, affecting explainability, testability, and auditability. Understanding common failure modes including brittleness, hallucination, concept drift, and feedback loops enables governance processes that address them.\nData governance for AI addresses quality including relevance, representativeness, completeness, and timeliness; provenance and lineage documentation; bias identification and mitigation; and standardized dataset documentation. Training practices should support reproducibility and honest evaluation. Testing must address performance, fairness, robustness, explainability, and security using metrics aligned with defined success criteria.\nDocumentation including model cards and regulatory technical documentation should be produced continuously throughout development rather than retrospectively. Release readiness assessment verifies that governance requirements have been satisfied before deployment. Ongoing monitoring detects issues that testing did not reveal. Maintenance processes address model updates and retraining. Incident management enables rapid response when problems occur.\nPrivacy-enhancing technologies including anonymization, differential privacy, federated learning, and synthetic data offer approaches to enable AI while managing privacy risks.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#chapter-3-review-questions",
    "href": "chapters/03-development.html#chapter-3-review-questions",
    "title": "3  Governing AI Development",
    "section": "3.12 Chapter 3 Review Questions",
    "text": "3.12 Chapter 3 Review Questions\n\nAn organization is evaluating a proposed AI system that would analyze employee communications to identify workers at risk of leaving the company. The system would process email content, chat messages, and calendar data. During use case assessment, which consideration should receive the highest priority?\nAn AI development team has completed model training and achieved accuracy metrics that exceed the targets defined during use case specification. However, when accuracy is disaggregated by demographic group, the team discovers significant disparities, with accuracy for one protected group substantially lower than for others. How should governance processes address this situation?\nA healthcare AI system trained to identify skin cancer from images performs well in clinical testing at the hospital where it was developed. After deployment to other hospitals, performance drops significantly, with increased false negative rates. Which common AI failure mode does this most likely represent?\nAn organization is using data originally collected for customer service quality monitoring to train an AI model that will predict which customers are likely to cancel their subscriptions. Under the GDPR, how should the organization analyze the use of this data?\nA financial services company is preparing to release an AI credit scoring model. The release readiness assessment reveals that while required documentation is complete and testing shows adequate performance, the monitoring infrastructure is not yet operational. What should the governance process recommend?",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/03-development.html#references",
    "href": "chapters/03-development.html#references",
    "title": "3  Governing AI Development",
    "section": "3.13 References",
    "text": "3.13 References\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.\nMitchell, Margaret, et al. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\nGebru, Timnit, et al. “Datasheets for Datasets.” Communications of the ACM 64, no. 12 (2021).\nNational Institute of Standards and Technology. AI Risk Management Framework 1.0. NIST AI 100-1, 2023.\nGovernment of Canada. Algorithmic Impact Assessment Tool. Treasury Board of Canada Secretariat, 2019.\nEuropean Parliament and Council. Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union, 2024.\nDwork, Cynthia, and Aaron Roth. “The Algorithmic Foundations of Differential Privacy.” Foundations and Trends in Theoretical Computer Science 9, no. 3-4 (2014).\nMcMahan, Brendan, et al. “Communication-Efficient Learning of Deep Networks from Decentralized Data.” Proceedings of AISTATS, 2017.\nChouldechova, Alexandra. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5, no. 2 (2017).\nOgunseye, S. “They Can Include AI, But Should They? Teaching students about sensible solutions in the age of AI hype.” Communications of the ACM Blog, 2025. https://cacm.acm.org/blogcacm/they-can-include-ai-but-should-they/",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Governing AI Development</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html",
    "href": "chapters/04-deployment.html",
    "title": "4  Governing AI Deployment",
    "section": "",
    "text": "4.1 Introduction\nThe deployment of an AI system marks a transition from controlled development to real-world operation, where the system interacts with actual users, affects actual individuals, and operates in conditions that may differ from those anticipated during development. This transition introduces new governance challenges. The risks that were theoretical during development become actual. The edge cases that were difficult to test may now be encountered. The individuals affected by AI decisions now have a stake in how the system operates.\nThis chapter examines governance during AI deployment and operation, addressing how organizations evaluate AI systems before deployment, manage ongoing operations, handle third-party AI relationships, protect user rights, address workforce implications, and integrate AI governance with enterprise risk management. The material maps primarily to Domain IV of the AIGP Body of Knowledge, which addresses the knowledge and skills required to govern the deployment and use of AI systems.\nOrganizations may be deployers using AI systems developed by others, deployers using AI systems they developed themselves, or both. The governance practices described here apply regardless of where the AI system originated, though the available information and control mechanisms differ when systems come from external providers versus internal development teams.\nFigure 4.1: AI Deployment Governance Framework — Five-phase deployment lifecycle with activities and stakeholder responsibilities.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#introduction",
    "href": "chapters/04-deployment.html#introduction",
    "title": "4  Governing AI Deployment",
    "section": "",
    "text": "Figure 4.1: AI Deployment Governance Framework",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#evaluating-ai-deployment-decisions",
    "href": "chapters/04-deployment.html#evaluating-ai-deployment-decisions",
    "title": "4  Governing AI Deployment",
    "section": "4.2 Evaluating AI Deployment Decisions",
    "text": "4.2 Evaluating AI Deployment Decisions\nBefore deploying an AI system, organizations must evaluate whether deployment is appropriate. This evaluation considers whether the system is ready, whether the organization is ready, and whether the deployment context is appropriate.\n\nSystem Readiness\nSystem readiness assessment verifies that the AI system meets requirements and is fit for deployment. For internally developed systems, this assessment builds on development testing. For externally sourced systems, this assessment may require independent evaluation.\nTechnical readiness requires that the system functions correctly, achieves required performance levels, and integrates appropriately with organizational infrastructure. Does the system produce outputs in the expected format? Does it meet latency and throughput requirements? Does it handle errors gracefully? Does it integrate with authentication, logging, and monitoring systems?\nPerformance readiness requires that the system achieves required accuracy, fairness, and robustness. Performance should be validated on data representative of actual deployment conditions, not just development test data. If the deployment context differs significantly from development conditions, additional testing may be needed.\nCompliance readiness requires that legal and regulatory requirements have been addressed. Has required documentation been prepared? Are required disclosures ready to be made? Are human oversight mechanisms in place? Have required impact assessments been completed?\n\n\nOrganizational Readiness\nDeploying an AI system requires organizational capabilities beyond the system itself. Organizational readiness assessment verifies these capabilities are in place.\nOperational readiness requires that the organization can operate the AI system effectively. Are staff trained to use the system appropriately? Are procedures documented? Is support available when users encounter issues?\nOversight readiness requires that the organization can provide required human oversight. If human review is required for certain decisions, are reviewers identified, trained, and available? Do they have access to the information needed for meaningful review?\nMonitoring readiness requires that the organization can monitor the deployed system. Are monitoring systems in place? Are metrics defined? Are alert thresholds set? Is someone responsible for reviewing monitoring outputs?\nIncident readiness requires that the organization can respond when problems occur. Are incident response procedures established? Are roles and responsibilities clear? Are escalation paths defined? Are regulatory notification requirements understood?\n\n\nDeployment Context Assessment\nThe same AI system may be appropriate for deployment in some contexts but not others. Deployment context assessment evaluates whether the specific deployment scenario is appropriate.\nUse case alignment verifies that the deployment use case matches the system’s intended purpose and validated capabilities. An AI system validated for one application may not be appropriate for different applications, even if they seem similar. A credit risk model developed for consumer lending may not be appropriate for small business lending without additional validation.\nPopulation alignment verifies that the deployment population matches the population on which the system was developed and tested. If the system will serve a different demographic, geographic, or other population than it was developed for, performance may differ.\nEnvironmental alignment verifies that deployment conditions match development assumptions. If the system depends on particular data inputs, infrastructure, or integration points, those dependencies must be satisfied in the deployment environment.\nRisk proportionality verifies that the deployment context does not present risks disproportionate to the validated capabilities. A system with modest accuracy might be appropriate for low-stakes applications but inappropriate for consequential decisions.\n\n\nDeployment Decision\nThe deployment decision integrates system readiness, organizational readiness, and deployment context assessment into a determination of whether to proceed. This decision should be documented, with clear accountability for the decision-maker.\nThe decision may be to proceed with deployment as planned, proceed with modifications or conditions, delay deployment until readiness gaps are addressed, or not proceed because the deployment is inappropriate.\nConditions might include limiting initial deployment scope, implementing additional oversight measures, requiring enhanced monitoring, or setting triggers for deployment review. These conditions should be documented and tracked to ensure they are actually implemented.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#assessing-third-party-ai-systems",
    "href": "chapters/04-deployment.html#assessing-third-party-ai-systems",
    "title": "4  Governing AI Deployment",
    "section": "4.3 Assessing Third-Party AI Systems",
    "text": "4.3 Assessing Third-Party AI Systems\nMany organizations deploy AI systems obtained from external providers rather than developing systems internally. Third-party AI introduces governance challenges because the organization has less visibility into system development and less control over system characteristics.\n\nDue Diligence\nBefore acquiring a third-party AI system, organizations should conduct due diligence to evaluate the system and the provider.\nProvider assessment evaluates the provider’s capabilities, practices, and track record. Does the provider have appropriate expertise? What governance practices does the provider follow? Has the provider had incidents with other AI systems? What is the provider’s financial stability and likely continuity?\nSystem assessment evaluates the AI system itself. What is the system designed to do? How was it developed? What data was used for training? What testing was performed? What are the known limitations? This assessment may be constrained by limited access to system details; providers may restrict information for competitive or security reasons.\nDocumentation assessment evaluates whether the provider supplies documentation sufficient to support the organization’s governance needs. Is there adequate description of system capabilities and limitations? Are performance metrics provided? Is the information sufficient for required impact assessments?\nCompliance assessment evaluates whether the provider has addressed applicable regulatory requirements. For systems subject to the EU AI Act, has the provider completed conformity assessment? Does the provider supply required documentation? Will the provider support the organization’s compliance obligations?\n\n\nContractual Provisions\nContracts with AI providers should address governance needs that cannot be fully satisfied through pre-acquisition due diligence.\nInformation rights should ensure the organization receives information needed for ongoing governance. This might include performance metrics, information about system updates, notification of incidents or identified issues, and documentation needed for regulatory compliance.\nAudit rights should enable the organization to verify provider claims and assess ongoing compliance. This might include rights to audit provider practices, review testing results, or conduct independent testing of the system.\nUpdate provisions should address how system updates are handled. Will updates be automatic or subject to organization approval? Will the organization be notified in advance? What testing will the organization be able to conduct before updates take effect?\nIncident provisions should address how incidents are handled. What notification will the provider give? What cooperation will the provider provide for incident investigation? What remediation will the provider undertake?\nLiability provisions should appropriately allocate responsibility for AI-related harms. Who bears liability if the system causes harm to third parties? What indemnification does the provider offer? How are regulatory penalties allocated?\nTermination provisions should address what happens if the relationship ends. Will the organization retain access to trained models? Will data be returned or deleted? What transition support will the provider offer?\n\n\nOngoing Vendor Management\nThird-party AI relationships require ongoing management beyond initial due diligence and contracting.\nPerformance monitoring tracks whether the system continues to meet requirements. If performance degrades or unexpected issues arise, the organization should investigate and work with the provider to address problems.\nCompliance monitoring tracks whether the provider continues to meet contractual and regulatory obligations. Regular review should verify that required documentation is current, required notifications are being made, and representations remain accurate.\nRelationship management maintains communication with the provider about system issues, upcoming changes, and evolving requirements. Organizations should ensure they have appropriate contacts and escalation paths at the provider.\nRisk assessment should be updated periodically and when circumstances change. A provider that was appropriate when selected may become inappropriate if their practices change, their financial condition weakens, or organizational requirements evolve.\n\n\nOpen Source AI Models\nOpen source AI models present distinct governance considerations compared to commercial vendor relationships. Models released under open source licenses offer advantages including transparency into model architecture and training approaches, freedom to modify and adapt models for organizational needs, and avoidance of vendor lock-in. However, they also introduce specific risks that governance programs must address.\nLicense compliance requires understanding the terms under which the model is released. Open source AI licenses vary significantly. Some permit commercial use freely; others impose restrictions on commercial deployment or require attribution. Some licenses require that modifications be shared under the same license terms. Organizations should evaluate whether license terms are compatible with intended use cases and whether compliance obligations can be met.\nSupport and maintenance differ from commercial relationships. Open source models typically lack guaranteed support, service level agreements, or committed maintenance schedules. Organizations deploying open source models assume responsibility for monitoring security vulnerabilities, applying updates, and addressing issues that arise. This requires technical capability that may not be needed with fully managed commercial services.\nProvenance and training data transparency vary across open source releases. Some projects provide detailed documentation of training data sources, preprocessing, and known limitations. Others offer minimal information. Organizations should assess whether available documentation is sufficient for governance needs including impact assessments and compliance obligations.\nLiability allocation differs fundamentally from commercial contracts. Open source licenses typically disclaim warranties and limit liability. If an open source model causes harm, the organization deploying it generally bears full responsibility. There is no vendor to pursue for indemnification or remediation support.\nCommunity dynamics affect long-term viability. Open source projects depend on community contributions and maintainer commitment. A project that is actively maintained today may become abandoned. Organizations should assess project health indicators including contributor activity, release frequency, and community engagement when making deployment decisions.\n\n\n\n\n\n\nFigure 4.2: Third-Party AI Governance Lifecycle\n\n\n\nFigure 4.2: Third-Party AI Governance Lifecycle — Cyclical governance from due diligence through monitoring and renewal or exit.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#deployment-options-and-their-governance-implications",
    "href": "chapters/04-deployment.html#deployment-options-and-their-governance-implications",
    "title": "4  Governing AI Deployment",
    "section": "4.4 Deployment Options and Their Governance Implications",
    "text": "4.4 Deployment Options and Their Governance Implications\nAI systems can be deployed through various technical architectures, each with different governance implications. Organizations should understand these options and their tradeoffs.\n\nCloud Deployment\nCloud deployment runs AI systems on infrastructure operated by cloud service providers. The organization accesses AI capabilities through APIs or managed services without operating the underlying infrastructure.\nCloud deployment offers advantages including reduced infrastructure burden, scalability, and access to advanced capabilities the organization might not be able to develop independently. Many AI services are available only or primarily through cloud deployment.\nCloud deployment introduces governance considerations. Data sent to cloud services leaves organizational control; privacy and security depend on provider practices and contractual protections. The organization may have limited visibility into how systems operate. Latency and availability depend on network connectivity and provider uptime. Regulatory requirements may restrict use of cloud services for certain data or applications.\nA less obvious but increasingly important consideration is the competitive intelligence embedded in organizational data. When employees use cloud AI services for their work—analyzing documents, writing code, refining strategies—their interactions may train or improve the provider’s models. The organizational knowledge encoded in these interactions can become part of a platform that serves competitors. When AI becomes part of the team rather than a private assistant, it becomes a platform for organizational intelligence that may not remain organizational. Governance should address what data and interactions flow to external AI services and whether competitive sensitivity warrants constraints.\n\n\nOn-Premise Deployment\nOn-premise deployment runs AI systems on infrastructure the organization owns and operates. The organization maintains control over the computing environment and data.\nOn-premise deployment offers advantages for sensitive applications. Data remains within organizational boundaries. The organization has full control over the computing environment. Operation does not depend on external connectivity or provider availability.\nOn-premise deployment requires organizational capabilities to operate and maintain AI infrastructure. This includes appropriate hardware, software expertise, and ongoing maintenance. On-premise deployment may limit access to capabilities available only through cloud services.\n\n\nEdge Deployment\nEdge deployment runs AI systems on devices at the edge of networks, close to data sources or end users. This might include AI on mobile devices, IoT devices, industrial equipment, or local computing appliances.\nEdge deployment offers advantages for applications requiring low latency, offline operation, or local data processing. AI on a mobile device can operate without network connectivity. AI on manufacturing equipment can respond faster than a round-trip to a cloud server would allow.\nEdge deployment introduces governance considerations. Models deployed to edge devices may be harder to update and monitor. Security of edge devices may be challenging. Distributed deployment may complicate version management and consistency.\n\n\nHybrid Deployment\nMany organizations use hybrid approaches combining cloud, on-premise, and edge deployment for different systems or different aspects of the same system. For example, model training might occur in the cloud while inference runs on-premise, or a mobile app might use on-device AI for routine processing while calling cloud services for complex requests.\nHybrid deployment requires governance approaches that address each deployment context and the interactions between them. Data flows between environments must be understood and protected. Consistency must be maintained when the same model runs in different environments.\n\n\nModel Customization Approaches\nOrganizations deploying AI models—whether proprietary or third-party—often customize them to improve performance for specific use cases. These customization approaches have distinct governance implications.\nFine-tuning adjusts a pre-trained model’s parameters using organization-specific data. This creates a derivative model that reflects both the original training and the fine-tuning data. Governance considerations include ensuring the fine-tuning data is appropriate and properly governed, understanding how fine-tuning affects model behavior and safety properties, and maintaining documentation of what data was used and how the model changed.\nRetrieval Augmented Generation (RAG) supplements a language model with retrieved information from organizational knowledge bases at inference time. Rather than changing the model itself, RAG provides context that shapes outputs. Governance considerations include ensuring the knowledge base contains accurate and appropriate information, managing access controls for sensitive retrieved content, and understanding how retrieval failures affect system behavior.\nPrompt engineering shapes model behavior through carefully crafted instructions without modifying the model or providing additional data. System prompts, few-shot examples, and structured output formats can significantly affect how models perform. Governance considerations include documenting and version-controlling prompts, testing prompt effectiveness across scenarios, and understanding prompt injection risks.\nEach customization approach affects the division of responsibility between model providers and deploying organizations. Fine-tuning creates greater organizational responsibility for model behavior. RAG makes the organization responsible for knowledge base quality. Prompt engineering makes the organization responsible for instruction design. Governance programs should address how customization affects accountability and what controls are appropriate for each approach.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#governing-ai-during-operation",
    "href": "chapters/04-deployment.html#governing-ai-during-operation",
    "title": "4  Governing AI Deployment",
    "section": "4.5 Governing AI During Operation",
    "text": "4.5 Governing AI During Operation\nOnce deployed, AI systems require ongoing governance throughout their operational life. This governance ensures systems continue to perform appropriately, risks are managed, and issues are identified and addressed.\n\nOperational Monitoring\nContinuous monitoring provides visibility into AI system operation and enables early detection of issues.\nTechnical monitoring tracks system health including availability, latency, throughput, and errors. Standard application monitoring practices apply, supplemented by AI-specific considerations.\nPerformance monitoring tracks model accuracy and related metrics over time. This requires defining appropriate metrics, collecting the data needed to compute them, and establishing processes to review results. For supervised learning systems, this typically requires obtaining ground truth labels for a sample of predictions, which may be available immediately, delayed, or require active collection.\nFairness monitoring tracks whether outcomes remain equitable across demographic groups. Disparities that were not present or were acceptable at deployment may emerge or worsen over time.\nDrift monitoring detects changes in input data distributions or in the relationship between inputs and appropriate outputs. Data drift occurs when the characteristics of incoming data change from the training distribution. Concept drift occurs when the underlying relationships the model learned change. Both can cause performance degradation.\nUsage monitoring tracks how the system is being used, which can identify misuse, unexpected use patterns, or opportunities for improvement.\n\n\nHuman Oversight in Operation\nHuman oversight requirements established during design must be implemented during operation. The nature and intensity of oversight depends on the application context and regulatory requirements.\nFor some applications, human oversight means humans review AI recommendations before taking action. The human reviewer should have appropriate expertise, access to relevant information, and genuine authority to override AI recommendations. Organizations should guard against automation bias, where humans routinely accept AI outputs without meaningful review.\nFor other applications, human oversight means humans monitor aggregate AI behavior without reviewing individual decisions. This might involve periodic review of performance metrics, audits of decision samples, or investigation of anomalies or complaints.\nFor applications requiring human intervention capabilities, humans must be able to understand system operation, intervene when necessary, and stop the system if required. These capabilities must be tested and maintained.\n\n\nDocumentation During Operation\nOperational documentation captures information needed for ongoing governance, incident investigation, and compliance demonstration.\nLogging should capture appropriate information about AI system inputs, outputs, and operation. What constitutes appropriate logging depends on the application; logs should be sufficient to investigate issues and demonstrate compliance without unnecessarily retaining sensitive information.\nAudit trails should enable reconstruction of decisions for investigation or challenge. When an individual questions an AI decision affecting them, the organization should be able to retrieve relevant information and explain what happened.\nPerformance records should document ongoing performance metrics, enabling trend analysis and compliance demonstration.\nIncident records should document any incidents, investigations, and responses, supporting organizational learning and demonstrating diligent incident management.\n\n\nChange Management\nAI systems change over time through updates, retraining, configuration changes, and environmental changes. Change management ensures changes are controlled and do not introduce unintended consequences.\nUpdate assessment should evaluate proposed changes before implementation. What is the purpose of the change? What are the expected effects? What testing has been performed? What risks might the change introduce?\nApproval processes should ensure appropriate review of changes. Minor changes might be approved by operational staff; significant changes might require governance review. The level of review should be proportionate to the potential impact.\nRollback capabilities should enable reverting to previous versions if changes cause problems. This requires maintaining previous versions and having procedures to restore them.\nDocumentation should capture what changes were made, when, why, and by whom. This supports troubleshooting and provides an audit trail.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#managing-downstream-risks",
    "href": "chapters/04-deployment.html#managing-downstream-risks",
    "title": "4  Governing AI Deployment",
    "section": "4.6 Managing Downstream Risks",
    "text": "4.6 Managing Downstream Risks\nAI systems can affect parties beyond the immediate users, and deployers must consider and manage these downstream risks.\n\nIdentifying Affected Parties\nAI systems affect multiple categories of stakeholders whose interests governance should consider.\nUsers are individuals who interact directly with the AI system. They may be employees using AI tools in their work, customers using AI-powered products or services, or others who engage with the system.\nSubjects are individuals about whom the AI system makes decisions or predictions, who may or may not be the same as users. A hiring AI affects job applicants who may never directly interact with the system. A credit AI affects loan applicants whose information the system processes.\nThird parties are individuals or entities affected by AI system outputs or by actions taken based on those outputs. If an AI system recommends a price increase that affects customers, those customers are affected parties even if the AI’s role is invisible to them.\nSociety broadly may be affected by aggregate impacts of AI deployment, including effects on labor markets, information environments, or social dynamics.\n\n\nRisk Communication\nOrganizations should communicate appropriately about AI risks to affected parties.\nUsers should understand what the AI system does, how to use it appropriately, and what limitations to be aware of. Documentation, training, and interface design should support appropriate use.\nSubjects should be informed about AI involvement in decisions affecting them, as required by law and as appropriate for building trust. The EU AI Act and various national laws require disclosure of AI use in specified contexts.\nDownstream deployers, when an organization provides AI systems for others to deploy, should receive information needed to deploy responsibly. This includes accurate descriptions of capabilities and limitations, documentation supporting impact assessments, and guidance on appropriate use.\n\n\nMisuse Prevention\nOrganizations should take reasonable steps to prevent misuse of AI systems they deploy or provide.\nUse restrictions should prohibit inappropriate applications. Terms of service, acceptable use policies, and technical controls can restrict use to appropriate contexts.\nAccess controls should limit who can use the system to authorized parties with legitimate needs.\nMonitoring should detect potential misuse patterns that warrant investigation.\nResponse procedures should address identified misuse through warnings, access revocation, or other appropriate measures.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#external-communication",
    "href": "chapters/04-deployment.html#external-communication",
    "title": "4  Governing AI Deployment",
    "section": "4.7 External Communication",
    "text": "4.7 External Communication\nOrganizations must communicate externally about their AI systems to various audiences including regulators, affected individuals, and the public.\n\nRegulatory Communication\nRegulatory communication requirements vary by jurisdiction and system type.\nThe EU AI Act requires providers of high-risk AI systems to register in an EU database before placing systems on the market. Deployers of high-risk AI systems must also register certain uses. Serious incidents must be reported to authorities.\nSector-specific regulations may impose additional reporting requirements. Financial institutions may need to report to banking regulators about AI use in covered activities. Healthcare AI may require regulatory filings.\nOrganizations should identify applicable reporting requirements and establish processes to meet them.\n\n\nIndividual Communication\nCommunication to individuals affected by AI systems serves both compliance and trust-building purposes.\nDisclosure requirements may mandate informing individuals about AI involvement in decisions. GDPR Article 22 requires information about automated decision-making. The EU AI Act requires disclosure when individuals interact with certain AI systems. Various national laws require AI disclosure in specific contexts.\nExplanation requirements may mandate explaining AI decisions to affected individuals. When AI contributes to a decision that affects someone, they may have the right to understand the factors involved and how they might achieve a different outcome.\nRecourse mechanisms should provide paths for individuals to question, contest, or seek review of AI decisions affecting them. This might include human review processes, complaint mechanisms, or formal appeal rights.\n\n\nPublic Communication\nMany organizations make public statements about their AI use through AI principles, transparency reports, or other communications.\nPublic commitments create expectations that organizations should be prepared to meet. Stating a commitment to fairness or transparency creates accountability for actually achieving those goals.\nTransparency reporting provides information about AI systems, their performance, and their impacts. Some organizations voluntarily publish detailed information about their AI systems; others may be required to do so by regulations or contractual obligations.\nCrisis communication may be necessary if AI systems cause publicized harms. Organizations should be prepared to communicate about incidents, taking accountability while protecting legal interests.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#deactivation-capabilities",
    "href": "chapters/04-deployment.html#deactivation-capabilities",
    "title": "4  Governing AI Deployment",
    "section": "4.8 Deactivation Capabilities",
    "text": "4.8 Deactivation Capabilities\nOrganizations must maintain the ability to deactivate AI systems when necessary. This capability supports incident response, provides a safeguard against runaway systems, and satisfies regulatory requirements.\n\nTechnical Deactivation\nTechnical mechanisms should enable stopping AI systems quickly when needed.\nKill switches provide immediate shutdown capability. For critical systems, this might be a physical or software control that immediately halts operation.\nGraceful shutdown procedures enable stopping systems in an orderly way that preserves data integrity and enables investigation.\nRollback capabilities enable reverting to previous versions or configurations if problems arise with updates.\nFallback modes enable continuing operations without AI, either by reverting to manual processes or by using simpler backup systems.\n\n\nOrganizational Authority\nClear authority for deactivation decisions ensures that necessary shutdowns can happen quickly.\nWho has authority to order a shutdown? This should include operational personnel who can respond to immediate technical issues, governance personnel who can respond to compliance concerns, and executive leadership who can respond to strategic or reputational concerns.\nWhat triggers a mandatory shutdown? Some conditions should require shutdown regardless of other considerations, such as serious safety incidents, regulatory orders, or evidence of severe discrimination.\nWhat processes apply after shutdown? Investigation, remediation, and restart authorization processes should be established in advance.\n\n\nDocumentation and Testing\nDeactivation capabilities should be documented and tested.\nDocumentation should describe shutdown procedures, authority, and decision criteria so that personnel can act quickly when needed.\nTesting should verify that shutdown mechanisms work as intended. Organizations should periodically test their ability to deactivate AI systems, just as they test disaster recovery for other systems.\nIncident exercises should include scenarios requiring AI deactivation, ensuring that personnel are prepared to execute shutdown procedures under pressure.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#protecting-user-rights",
    "href": "chapters/04-deployment.html#protecting-user-rights",
    "title": "4  Governing AI Deployment",
    "section": "4.9 Protecting User Rights",
    "text": "4.9 Protecting User Rights\nIndividuals affected by AI systems have various rights that organizations must respect. Some rights are established by law; others reflect ethical commitments or best practices.\n\nOpt-Out and Consent\nSome laws provide rights to opt out of AI processing or require consent for certain AI uses.\nGDPR Article 22 provides the right not to be subject to decisions based solely on automated processing with significant effects, subject to exceptions. Where exceptions apply, suitable safeguards must be implemented.\nVarious consumer protection laws provide opt-out rights for certain AI uses. California’s CPRA provides rights related to automated decision-making.\nSector-specific requirements may impose consent or opt-out requirements for AI in particular contexts.\nOrganizations should identify applicable requirements and implement mechanisms to honor them. This may require offering alternatives to AI-based processing for individuals who opt out.\n\n\nExplanation and Transparency\nRights to explanation require organizations to provide meaningful information about AI decisions.\nWhat was decided? The individual should understand the outcome of AI processing affecting them.\nWhat factors were considered? The individual should understand what information influenced the decision.\nWhy was this outcome reached? The individual should understand, at least at a general level, why the AI reached this conclusion rather than another.\nWhat can the individual do? The individual should understand options for contesting the decision or achieving a different outcome.\nExplanations should be meaningful to the individuals receiving them, not technical descriptions comprehensible only to AI specialists. Plain language explanations, even if they sacrifice some precision, may be more valuable than technically accurate but incomprehensible descriptions.\n\n\nContestation and Recourse\nRights to contest AI decisions require mechanisms for individuals to challenge outcomes.\nHuman review enables individuals to have AI decisions reviewed by a human who can consider factors the AI may have missed and exercise judgment the AI cannot replicate.\nAppeal processes enable individuals to escalate concerns through defined channels with increasing levels of review.\nCorrection mechanisms enable individuals to provide information that was missing or incorrect and have decisions reconsidered in light of complete or corrected information.\nRemediation provides appropriate relief when AI decisions are found to be wrong, including reversing decisions, providing compensation, or taking other corrective action.\n\n\nRights Implementation\nImplementing user rights requires organizational processes, not just policies.\nAwareness ensures that staff understand rights requirements and their role in honoring them.\nIntake mechanisms provide clear channels for individuals to exercise their rights.\nResponse processes ensure that rights requests are handled consistently, completely, and within required timeframes.\nDocumentation captures how rights were exercised and honored, supporting compliance demonstration.\n\n\n\n\n\n\nFigure 4.3: User Rights in AI Systems\n\n\n\nFigure 4.3: User Rights in AI Systems — Rights request lifecycle and implementation requirements for opt-out, explanation, and contestation.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#workforce-considerations",
    "href": "chapters/04-deployment.html#workforce-considerations",
    "title": "4  Governing AI Deployment",
    "section": "4.10 Workforce Considerations",
    "text": "4.10 Workforce Considerations\nAI deployment affects workforces, and governance should address these impacts. This includes impacts on workers who use AI systems, workers whose jobs are affected by AI automation, and workers who are subjects of AI decision-making.\n\nAI Augmentation of Work\nMany AI deployments augment worker capabilities rather than replacing workers entirely. Governance should address how these AI-augmented work arrangements operate.\nTraining should prepare workers to use AI tools effectively and appropriately. Workers should understand what AI systems can and cannot do, how to interpret AI outputs, when to rely on AI and when to exercise independent judgment, and how to identify potential AI errors.\nWorkflow design should integrate AI appropriately into work processes. AI should support workers rather than creating burdens. Human oversight should be meaningful rather than perfunctory.\nPerformance management should account for AI assistance. Expectations should reflect AI’s contributions, and workers should not be penalized for exercising appropriate judgment to override AI recommendations.\nWell-being considerations should address potential negative effects of AI-augmented work, including deskilling, reduced autonomy, or surveillance effects.\n\n\nWorkforce Displacement\nAI may reduce demand for certain types of work, potentially displacing workers. Organizations should consider these impacts and respond appropriately.\nImpact assessment should analyze potential workforce effects of AI deployment. Which roles might be affected? How many workers? Over what timeframe?\nTransition support might include retraining, reassignment to other roles, severance packages, or outplacement assistance.\nStakeholder communication should inform workers and their representatives about AI plans and their potential impacts.\nResponsible automation practices might include gradual implementation, worker input into automation decisions, or commitments to redeploy rather than lay off affected workers.\n\n\nAI in Employment Decisions\nAI systems that make or influence decisions about workers raise particular governance concerns.\nHiring AI affects applicants who may have no relationship with the organization beyond their application. Discrimination in hiring AI can exclude qualified individuals from opportunities.\nPerformance management AI affects existing workers’ evaluations, compensation, and advancement. Errors or bias can harm careers.\nSurveillance AI monitors worker behavior, raising privacy concerns and potentially affecting workplace culture and worker well-being.\nWorkforce AI should receive heightened governance attention given the power dynamics involved and the potential for significant individual impacts.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#integrating-ai-governance-with-enterprise-risk-management",
    "href": "chapters/04-deployment.html#integrating-ai-governance-with-enterprise-risk-management",
    "title": "4  Governing AI Deployment",
    "section": "4.11 Integrating AI Governance with Enterprise Risk Management",
    "text": "4.11 Integrating AI Governance with Enterprise Risk Management\nAI governance does not exist in isolation but operates within organizations’ broader risk management frameworks. Effective AI governance integrates with enterprise risk management rather than creating parallel structures.\n\nAI Risk Categories\nAI risks relate to traditional enterprise risk categories while also presenting novel characteristics.\nOperational risk includes risks of AI system failures, errors, or performance degradation that disrupt operations.\nCompliance risk includes risks of violating laws, regulations, or contractual obligations related to AI.\nReputational risk includes risks of public criticism, loss of trust, or brand damage from AI incidents or practices.\nStrategic risk includes risks that AI investments fail to achieve expected benefits or that competitors gain advantage through superior AI capabilities.\nFinancial risk includes potential costs from AI incidents, including regulatory penalties, litigation, remediation, and lost business.\n\n\nIntegration Approaches\nOrganizations can integrate AI governance with enterprise risk management in various ways.\nRisk taxonomy integration incorporates AI risks into existing risk taxonomies, ensuring AI risks are identified and assessed through established processes.\nControl framework integration maps AI governance controls to existing control frameworks, enabling consistent assessment and reporting.\nReporting integration incorporates AI risk information into enterprise risk reporting, ensuring appropriate visibility to risk committees and leadership.\nAssurance integration includes AI governance in internal audit scope, providing independent verification of governance effectiveness.\n\n\nThree Lines Model\nThe three lines model common in enterprise risk management can be applied to AI governance.\nFirst line functions own and manage AI risks as part of their operational responsibilities. Development teams, business units deploying AI, and operational staff managing AI systems are first line.\nSecond line functions provide oversight, frameworks, and expertise. AI governance teams, privacy offices, compliance functions, and risk management functions are second line.\nThird line functions provide independent assurance. Internal audit and external auditors are third line.\nClear delineation of responsibilities across lines helps ensure comprehensive coverage without gaps or redundant effort.\n\n\n\n\n\n\nFigure 4.4: AI Governance in Enterprise Risk Management\n\n\n\nFigure 4.4: AI Governance in Enterprise Risk Management — Integration with three lines model and enterprise risk categories.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#chapter-4-summary",
    "href": "chapters/04-deployment.html#chapter-4-summary",
    "title": "4  Governing AI Deployment",
    "section": "4.12 Chapter 4 Summary",
    "text": "4.12 Chapter 4 Summary\nThis chapter examined governance during AI deployment and operation, addressing the transition from controlled development to real-world use and the ongoing governance required throughout an AI system’s operational life.\nDeployment decisions require evaluating system readiness, organizational readiness, and deployment context. System readiness encompasses technical, performance, and compliance dimensions. Organizational readiness requires operational, oversight, monitoring, and incident response capabilities. Deployment context assessment verifies alignment between the specific deployment scenario and validated system capabilities.\nThird-party AI requires due diligence assessing providers and systems, contractual provisions protecting organizational interests and governance needs, and ongoing vendor management maintaining appropriate oversight.\nDeployment options including cloud, on-premise, edge, and hybrid architectures have different governance implications regarding control, visibility, latency, and regulatory considerations.\nOperational governance requires continuous monitoring of technical health, model performance, fairness, and drift. Human oversight must be implemented as designed, with attention to automation bias. Documentation including logs, audit trails, and performance records supports governance and compliance. Change management controls modifications to deployed systems.\nDownstream risk management considers affected parties beyond immediate users, communicates appropriately about risks, and implements measures to prevent misuse.\nExternal communication addresses regulatory reporting requirements, individual disclosure and explanation obligations, and public communication about AI practices.\nDeactivation capabilities provide technical mechanisms and organizational authority to stop AI systems when necessary, with documented procedures and periodic testing.\nUser rights including opt-out, explanation, and contestation require organizational processes to implement effectively, not just policies acknowledging rights exist.\nWorkforce considerations address AI augmentation of work, potential workforce displacement, and the particular sensitivities of AI in employment decisions.\nIntegration with enterprise risk management situates AI governance within broader organizational structures, mapping AI risks to risk categories and AI governance to control frameworks, reporting, and assurance functions.",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#chapter-4-review-questions",
    "href": "chapters/04-deployment.html#chapter-4-review-questions",
    "title": "4  Governing AI Deployment",
    "section": "4.13 Chapter 4 Review Questions",
    "text": "4.13 Chapter 4 Review Questions\n\nAn organization is preparing to deploy an AI customer service chatbot. System testing has been completed with positive results, but the organization has not yet trained customer service staff on when and how to escalate from the chatbot to human agents. How should the deployment decision address this situation?\nAn organization is evaluating an AI vendor’s system for use in screening job applicants. The vendor provides accuracy metrics but declines to share detailed information about training data, citing competitive concerns. How should the organization’s governance process address this limitation?\nAn AI credit decisioning system has been deployed for six months. Monitoring shows that overall accuracy remains within acceptable bounds, but accuracy for applicants over age 60 has declined significantly compared to initial deployment. What governance response is most appropriate?\nA retail company uses an AI system to personalize product recommendations. A customer requests an explanation of why they were recommended a particular product. What information should the company provide to satisfy explanation requirements?\nAn organization is planning to deploy an AI system that will automate significant portions of work currently performed by 200 employees. What workforce-related governance considerations should inform the deployment decision?",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/04-deployment.html#references",
    "href": "chapters/04-deployment.html#references",
    "title": "4  Governing AI Deployment",
    "section": "4.14 References",
    "text": "4.14 References\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.\nEuropean Parliament and Council. Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union, 2024.\nEuropean Parliament and Council. Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation). Official Journal of the European Union, 2016.\nInstitute of Internal Auditors. The IIA’s Three Lines Model. IIA Position Paper, 2020.\nNational Institute of Standards and Technology. AI Risk Management Framework 1.0. NIST AI 100-1, 2023.\nFederal Reserve Board, Office of the Comptroller of the Currency. Supervisory Guidance on Model Risk Management. SR 11-7, 2011.\nIAPP. AI Governance in Practice Report 2025. International Association of Privacy Professionals, 2025.\nOgunseye, S. “Stop Training Your Competitor’s AI.” Communications of the ACM Blog, 2025. https://cacm.acm.org/blogcacm/stop-training-your-competitors-ai/",
    "crumbs": [
      "Governing the AI Lifecycle",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Governing AI Deployment</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html",
    "href": "chapters/05-lifecycle.html",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "",
    "text": "5.1 Introduction\nThe previous chapters examined AI governance through knowledge domains: foundations, legal frameworks, development practices, and deployment considerations. That structure works for learning, but governance in practice emerges from people in different roles working together. A lawyer sees different things than an engineer. A product manager thinks about users differently than a compliance officer thinks about regulators. Effective governance integrates these perspectives throughout the AI lifecycle rather than treating governance as a checkpoint at the end.\nThis chapter presents governance by design—an approach that embeds governance throughout development and deployment. It begins by examining the different perspectives that must collaborate for governance to succeed, then maps governance activities to lifecycle stages, showing what each perspective contributes at each stage. The chapter concludes with practical guidance for implementation.\nGovernance by design parallels concepts familiar from privacy and security: privacy by design builds privacy considerations into systems from the start rather than attempting to add them afterward; security by design builds security into architecture rather than bolting it on. Similarly, governance by design builds governance considerations into AI development from conception through retirement.\nFigure 5.1: AI Governance by Design Lifecycle Model — Comprehensive lifecycle with stages, key activities, and primary perspectives.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#introduction",
    "href": "chapters/05-lifecycle.html#introduction",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "",
    "text": "Figure 5.1: AI Governance by Design Lifecycle Model",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#the-perspectives-that-shape-governance",
    "href": "chapters/05-lifecycle.html#the-perspectives-that-shape-governance",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.2 The Perspectives That Shape Governance",
    "text": "5.2 The Perspectives That Shape Governance\nAI governance is inherently cross-functional. No single role possesses all the knowledge required to govern AI responsibly. The failures of AI initiatives often trace not to technology but to leadership imbalance—the widespread myth that one person or one function can be strategist, technologist, communicator, and guardian all at once.\nConsider the pattern across high-profile AI failures. IBM’s Watson Health consumed more than five billion dollars before being sold at a fraction of that cost. Zillow lost half a billion in a single quarter before shutting its algorithmic home-buying business. In each case, the promise of a narrow leadership profile left critical gaps. AI initiatives are too complex for single-perspective governance. Just as no hospital would ask one person to be surgeon, administrator, ethicist, and fundraiser, no organization should expect a single function to carry every governance role.\nAcross both successes and failures, five types of contributions appear repeatedly. Understanding these archetypes helps organizations ensure balanced governance.\n\nBuilders: The Technical Foundation\nBuilders are the engineers and data scientists who create and maintain AI systems. They understand what the technology can actually do, where it breaks, and what it would take to fix it. Without builders, governance discussions remain abstract—disconnected from the concrete realities of data pipelines, model architectures, and system constraints.\nBuilders contribute accuracy about technical capabilities. When others promise what AI cannot deliver, builders provide the reality check. When governance requirements seem impractical, builders often find creative technical solutions. Their risk is tunnel vision—optimizing for technical metrics while missing broader implications that fall outside their domain of expertise.\n\n\nStrategists: The Long View\nStrategists are executives and senior leaders who connect today’s choices to tomorrow’s advantage. They see how AI investments fit organizational strategy, how competitive dynamics are shifting, and where the industry is heading. Without strategists, governance becomes reactive—responding to problems rather than positioning for opportunities.\nStrategists contribute direction and resources. They decide which AI initiatives receive investment and how governance programs are staffed. Their risk is impatience—pushing for deployment before systems are ready because competitive pressure makes delay feel costly. They may also underestimate technical complexity, treating AI as just another technology project.\n\n\nTranslators: The Bridge Builders\nTranslators are product managers, compliance leads, and domain experts who help technical and business sides understand one another. They convert legal requirements into engineering specifications. They explain technical limitations in terms business leaders can act on. They ensure that what gets built actually serves the users who will rely on it.\nTranslators contribute coherence. Without them, legal teams impose requirements that engineers cannot implement, engineers build systems that do not fit clinical workflows, and business teams make promises that technology cannot keep. Their risk is becoming bottlenecks—so essential to communication that everything slows down waiting for translation.\n\n\nEvangelists: The Mobilizers\nEvangelists are the charismatic champions who raise resources, rally support, and maintain momentum when enthusiasm flags. They secure budget approvals, recruit talent, and keep stakeholders engaged through the long middle stretches of AI development when visible progress slows.\nEvangelists contribute energy and resources. AI initiatives that lack champions often wither from neglect—not rejected, just slowly starved of attention and funding. Their risk is over-promising—generating expectations that technology cannot meet, creating pressure to deploy prematurely, or dismissing concerns as obstacles to progress rather than signals requiring attention.\n\n\nCustodians: The Guardians of Trust\nCustodians are risk officers, ethicists, auditors, and compliance professionals who protect trust and slow things down when everyone else wants to race ahead. They ask uncomfortable questions about what could go wrong, who might be harmed, and whether the organization is really ready.\nCustodians contribute protection. They catch problems before they become crises. They ensure the organization can defend its choices to regulators, courts, and the public. Their risk is excessive caution—imposing so many requirements that AI initiatives cannot proceed, or failing to distinguish between risks that matter and risks that can be managed.\n\n\nThe Myth of the Balanced Leader\nMost individuals blend two or three of these orientations, but under pressure they revert to their dominant one. The technically brilliant leader may discount user concerns. The strategically minded executive may override compliance objections. The cautious custodian may block initiatives that should proceed with appropriate safeguards.\nThe lesson is not to avoid naming AI leadership. The lesson is to stop looking for unicorns who embody every strength. Effective AI governance requires orchestration—ensuring that builders, strategists, translators, evangelists, and custodians all contribute, and that none drowns out the others. This orchestration is the work of governance structures and processes, not individual heroics.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#when-balance-is-missing",
    "href": "chapters/05-lifecycle.html#when-balance-is-missing",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.3 When Balance Is Missing",
    "text": "5.3 When Balance Is Missing\nThe cost of imbalanced governance extends far beyond the organizations where it originates. When firms get AI governance wrong, the consequences ripple through sectors and society.\nIn healthcare, evangelists have promised diagnostic breakthroughs that capture executive imagination and investor dollars. But without translators ensuring systems fit clinical workflows and custodians insisting on validation before deployment, tools fail to integrate with how medicine actually works. Patients receive confident diagnoses from systems that have not been adequately tested on populations like them. Clinicians lose trust in AI assistance and revert to ignoring algorithmic recommendations entirely—even when those recommendations would help.\nIn finance, builders and strategists have driven automated trading at machine speed, capturing profits from reaction times no human could match. But without custodians to insist on circuit breakers and systemic risk assessment, volatility spills beyond trading floors into retirement accounts and community wealth. The flash crash of 2010 erased nearly a trillion dollars in market value in minutes. Algorithmic trading did not cause the underlying instability, but it amplified consequences at speeds that outpaced human intervention.\nIn education, evangelists have promised personalized learning that meets each student where they are. But without translators connecting technology to pedagogy and builders ensuring systems actually work at scale, implementations have often widened the gaps they promised to close. Students in well-resourced districts receive AI-enhanced instruction while students elsewhere receive AI babysitting—software that generates busywork without genuine adaptation.\nIn government, strategists and performers have sold predictive systems—for policing, for benefits eligibility, for child welfare—without custodians to insist on oversight and affected community input. The result has often been systematic discrimination laundered through algorithmic objectivity, eroding confidence in institutions that citizens need to trust.\nThe imbalance inside firms does not stay inside. It leaks into every sector of society.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#the-lifecycle-model",
    "href": "chapters/05-lifecycle.html#the-lifecycle-model",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.4 The Lifecycle Model",
    "text": "5.4 The Lifecycle Model\nThe AI lifecycle can be modeled in various ways depending on organizational methodology. This chapter uses an eight-stage model detailed enough to map governance activities meaningfully while remaining general enough to apply across different organizational approaches.\n\nConception\nThe conception stage is where AI opportunities are identified, initial ideas are formed, and decisions are made about whether to pursue development. This stage precedes formal project initiation.\nKey activities include opportunity identification, initial feasibility assessment, alignment check with organizational strategy and values, and decision whether to proceed to formal requirements.\nGovernance focus at this stage is on ensuring that proposed AI uses are appropriate and aligned with organizational values before resources are committed. This is the first and most important filter. The question is not just “can we build this?” but “should we build this?”—a question that requires input from custodians and translators, not just builders and strategists.\n\n\nRequirements\nThe requirements stage defines what the AI system should do, who will use it, who will be affected, what constraints apply, and how success will be measured.\nKey activities include detailed use case definition, stakeholder identification, requirements elicitation and documentation, success criteria definition, and initial legal and risk analysis.\nGovernance focus at this stage is on ensuring requirements are complete and governance-relevant requirements are included. If fairness requirements, transparency requirements, and human oversight requirements are not in the requirements document, they are unlikely to be implemented. This stage requires heavy translator involvement to convert legal and ethical considerations into specifications that builders can implement.\n\n\nDesign\nThe design stage determines how the AI system will be built to meet requirements. This includes data strategy, model architecture, integration approach, and human oversight mechanisms.\nKey activities include data source identification and assessment, model architecture selection, system architecture design, interface design, and design review.\nGovernance focus at this stage is on ensuring design choices support governance objectives. Choices made during design have lasting implications for explainability, testability, controllability, and compliance. A deep learning architecture may achieve higher accuracy but sacrifice interpretability. A federated approach may preserve privacy but complicate validation. These tradeoffs require dialogue between builders who understand technical implications and custodians who understand governance requirements.\n\n\nBuild\nThe build stage implements the design, including data preparation, model training, system development, and integration.\nKey activities include data collection and preparation, model training, software development, integration with other systems, and iterative refinement.\nGovernance focus at this stage is on ensuring build activities follow governance requirements, documentation is maintained, and governance-relevant issues identified during build are escalated appropriately. Builders dominate this stage, but custodians should have visibility into progress and problems.\n\n\nValidate\nThe validate stage tests whether the built system meets requirements, including functional requirements, performance requirements, fairness requirements, and compliance requirements.\nKey activities include functional testing, performance testing, fairness testing, security testing, compliance verification, and documentation completion.\nGovernance focus at this stage is on ensuring validation is thorough enough to provide confidence that governance requirements have been met. Validation is where governance commitments are verified. This stage requires collaboration between builders who conduct testing, custodians who define what “good enough” means, and translators who connect technical metrics to real-world implications.\n\n\nDeploy\nThe deploy stage transitions the validated system into production use, including technical deployment, user training, and communication to affected parties.\nKey activities include deployment planning, technical deployment, user training, stakeholder communication, monitoring activation, and initial operation.\nGovernance focus at this stage is on ensuring deployment follows established procedures, required communications occur, and monitoring is operational before the system serves real users. Evangelists may push for faster deployment; custodians must ensure readiness is genuine, not performed.\n\n\nOperate\nThe operate stage is the ongoing period when the AI system is in production use, serving users and affecting individuals.\nKey activities include operational management, user support, human oversight as designed, issue response, and change management.\nGovernance focus at this stage is on ensuring the system operates as intended, human oversight is effective, and issues are identified and addressed promptly. This stage often reveals problems that testing missed—edge cases that appear only at scale, user behaviors that differ from assumptions, drift that develops over time.\n\n\nMonitor\nThe monitor stage encompasses ongoing surveillance of system performance, fairness, drift, and incidents. While monitoring is continuous during operation, it is conceptually distinct as an activity.\nKey activities include performance monitoring, fairness monitoring, drift detection, incident detection, and reporting.\nGovernance focus at this stage is on ensuring monitoring is comprehensive enough to detect governance-relevant issues and that monitoring findings trigger appropriate responses. Custodians must ensure that monitoring is not merely measurement but actionable intelligence.\n\n\nRetire\nThe retire stage addresses end of life for AI systems, including deactivation, data disposition, and learning capture.\nKey activities include retirement planning, deactivation, data retention or deletion, documentation archiving, and lessons learned capture.\nGovernance focus at this stage is on ensuring retirement occurs safely, data is handled appropriately, and organizational learning is preserved. Systems that made consequential decisions may need documentation preserved for litigation or regulatory review long after the system itself is deactivated.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#governance-activities-by-stage",
    "href": "chapters/05-lifecycle.html#governance-activities-by-stage",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.5 Governance Activities by Stage",
    "text": "5.5 Governance Activities by Stage\nThis section details specific governance activities at each lifecycle stage.\n\nConception Stage Governance\nUse case screening evaluates whether the proposed AI use is consistent with organizational values, policies, and risk appetite. Some uses should be rejected at this early stage before resources are invested.\nPreliminary risk classification assigns an initial risk tier that determines what governance processes will apply. Higher-risk applications receive more intensive governance.\nStrategic alignment check verifies that the proposed AI use supports organizational strategy and is appropriately prioritized against other opportunities.\nStakeholder identification begins identifying who will use the system, who will be affected, and whose input should inform development.\n\n\nRequirements Stage Governance\nImpact assessment initiation begins formal assessment of potential impacts. For higher-risk applications, this may be a full impact assessment process; for lower-risk applications, a streamlined assessment.\nLegal requirements identification analyzes applicable laws and regulations and translates them into requirements that development must satisfy.\nFairness criteria definition establishes how fairness will be measured and what fairness standards the system must meet.\nTransparency and explainability requirements determine what information must be provided to users and affected individuals and how.\nHuman oversight requirements specify what human oversight the system requires and how it will be implemented.\n\n\nDesign Stage Governance\nArchitecture review evaluates whether the proposed architecture supports governance requirements including explainability, testability, and controllability.\nData governance integration ensures that data sourcing, preparation, and use comply with data governance policies and legal requirements.\nPrivacy by design review evaluates privacy implications of design choices and ensures privacy requirements are addressed in architecture.\nHuman oversight design specifies how human oversight mechanisms will work, including what information humans will receive, what decisions they can make, and how they can intervene.\n\n\nBuild Stage Governance\nData lineage documentation maintains records of data sources, transformations, and quality measures.\nTraining process documentation records how models are trained, including data used, parameters selected, and decisions made.\nChange management tracks modifications to requirements, design, or implementation, ensuring changes receive appropriate review.\nIssue escalation ensures that governance-relevant issues identified during build reach appropriate decision-makers.\n\n\nValidate Stage Governance\nPerformance validation verifies that the system meets performance requirements on appropriate test data.\nFairness validation verifies that the system meets fairness requirements, with testing disaggregated across relevant demographic groups.\nCompliance validation verifies that legal and regulatory requirements have been satisfied.\nRelease readiness assessment provides a structured evaluation of whether the system is ready for deployment.\nApproval gate requires appropriate sign-off before proceeding to deployment.\n\n\nDeploy Stage Governance\nDeployment checklist verification confirms that all pre-deployment requirements have been satisfied.\nCommunication execution delivers required notifications to users, affected individuals, and regulators.\nMonitoring activation confirms that monitoring systems are operational and configured correctly.\nRollback readiness confirms that the organization can roll back deployment if serious issues emerge.\n\n\nOperate Stage Governance\nOperational compliance monitoring ensures ongoing compliance with applicable requirements.\nHuman oversight effectiveness monitoring evaluates whether human oversight is functioning as designed.\nIssue management addresses problems that arise during operation through established procedures.\nPeriodic review provides scheduled evaluation of whether the system continues to meet requirements.\n\n\nMonitor Stage Governance\nPerformance trend analysis identifies changes in system performance over time.\nFairness trend analysis identifies changes in fairness metrics that might indicate emerging disparities.\nDrift detection identifies changes in data distributions or relationships that might affect model validity.\nIncident investigation examines adverse events to determine causes and appropriate responses.\n\n\nRetire Stage Governance\nRetirement impact assessment evaluates implications of retiring the system.\nData disposition ensures that data is retained or deleted appropriately based on legal requirements and organizational policy.\nDocumentation preservation archives relevant documentation for future reference, compliance demonstration, or litigation support.\nLessons learned capture identifies insights that should inform future AI governance.\n\n\n\n\n\n\nFigure 5.2: Governance Activities Matrix\n\n\n\nFigure 5.2: Governance Activities Matrix — Activity categories mapped to lifecycle stages showing intensity at each intersection.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#perspective-contributions-across-the-lifecycle",
    "href": "chapters/05-lifecycle.html#perspective-contributions-across-the-lifecycle",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.6 Perspective Contributions Across the Lifecycle",
    "text": "5.6 Perspective Contributions Across the Lifecycle\nDifferent perspectives contribute most significantly at different stages. Understanding these patterns helps organizations ensure appropriate involvement throughout development.\n\nHow Builders Contribute\nBuilders provide feasibility assessment at conception, answering whether the proposed system is technically achievable. At requirements, they advise on what is technically possible and help define technical requirements. They lead architecture design, execute development, conduct testing, and operate production systems. Their deepest involvement is during build and validate stages, but they contribute throughout.\nWithout builder input, governance requirements may be technically impractical. With only builder input, governance may focus on what is easy to measure rather than what matters.\n\n\nHow Strategists Contribute\nStrategists provide direction at conception, deciding whether opportunities align with organizational strategy. They approve resource commitment at requirements, approve significant design decisions, and provide release approval at validation. During operation, they receive reports and make decisions on significant issues.\nWithout strategist involvement, governance lacks authority and resources. With only strategist involvement, governance may be overridden by competitive pressure.\n\n\nHow Translators Contribute\nTranslators facilitate throughout the lifecycle, but contribute most heavily at requirements and deployment. At requirements, they convert legal and ethical considerations into specifications builders can implement. At deployment, they ensure training materials and communications make sense to users. They maintain documentation and coordinate across functions throughout.\nWithout translators, legal teams and engineering teams talk past each other. With only translators, decisions may be delayed waiting for interpretation.\n\n\nHow Evangelists Contribute\nEvangelists secure resources and maintain momentum, contributing most at conception and deployment. At conception, they champion promising opportunities. At deployment, they communicate the system’s value and build user adoption. During long development phases, they maintain stakeholder engagement.\nWithout evangelists, promising initiatives wither from inattention. With only evangelists, systems deploy before they are ready.\n\n\nHow Custodians Contribute\nCustodians contribute at every stage but are most critical at requirements, validation, and monitoring. At requirements, they ensure governance requirements are specified. At validation, they define what “good enough” means and participate in release decisions. At monitoring, they interpret findings and trigger responses.\nWithout custodians, problems go unnoticed until they become crises. With only custodians, nothing deploys because nothing is ever safe enough.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#implementing-governance-by-design",
    "href": "chapters/05-lifecycle.html#implementing-governance-by-design",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.7 Implementing Governance by Design",
    "text": "5.7 Implementing Governance by Design\nMoving from governance as a checkpoint to governance by design requires organizational change.\n\nFor Organizations Starting AI Governance\nBegin with use case screening at the conception stage. Even simple screening that asks “Is this AI use consistent with our values?” and “What could go wrong?” prevents obviously problematic applications from proceeding.\nAdd impact assessment at the requirements stage. A basic impact assessment template that captures intended use, potential risks, and planned mitigations provides a foundation.\nEnsure validation includes fairness testing. Even basic disaggregated performance analysis can identify significant disparities before deployment.\nEstablish monitoring for deployed systems. At minimum, track whether systems are performing as expected.\nDocument what you do. Even informal documentation builds organizational memory.\n\n\nFor Organizations with Basic AI Governance\nExtend governance earlier into the lifecycle. If governance currently focuses on validation and deployment, add activities at design and build stages.\nFormalize impact assessment processes. Move from ad hoc assessments to structured processes with consistent templates.\nStrengthen monitoring capabilities. Move beyond basic health monitoring to include fairness metrics and drift detection.\nBuild cross-functional integration. Create mechanisms for input from all perspectives at appropriate stages.\n\n\nFor Organizations with Mature AI Governance\nAutomate where appropriate. Routine governance activities can often be automated, freeing human attention for judgment-intensive activities.\nIntegrate governance tooling. Connect governance activities across the lifecycle through platforms that maintain traceability.\nBenchmark and improve. Compare governance practices against external standards and peer organizations.\nShare learning. Capture insights from governance activities and spread effective practices.\n\n\nCommon Challenges\nSeveral challenges commonly arise in implementing governance by design.\nResistance from development teams who perceive governance as slowing development. Address this by making governance efficient, demonstrating value, and involving development in governance design.\nGovernance becoming bureaucratic checkbox exercise. Address this by focusing on outcomes rather than process compliance.\nInconsistent implementation across different AI initiatives. Address this through standardized processes and templates while allowing appropriate flexibility.\nDifficulty maintaining governance over time as organizational attention shifts. Address this through embedded governance roles, automated monitoring, and regular reviews.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#chapter-summary",
    "href": "chapters/05-lifecycle.html#chapter-summary",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.8 Chapter Summary",
    "text": "5.8 Chapter Summary\nThis chapter presented governance by design as an approach that embeds governance throughout the AI lifecycle rather than treating it as a checkpoint at the end.\nEffective AI governance requires multiple perspectives: builders who understand technical realities, strategists who provide direction and resources, translators who bridge domains, evangelists who maintain momentum, and custodians who protect trust. When these perspectives are imbalanced—when one dominates or another is absent—the consequences extend beyond organizations into sectors and society.\nThe lifecycle model includes eight stages from conception through retirement. Each stage has distinct governance activities and draws on different perspectives. Conception stage governance screens use cases before resources are committed. Requirements stage governance ensures governance requirements are specified. Design stage governance ensures architecture supports governance objectives. Build stage governance maintains documentation and escalates issues. Validation stage governance verifies requirements are met. Deployment stage governance ensures safe transition to production. Operation stage governance ensures ongoing compliance. Monitor stage governance detects issues requiring response. Retirement stage governance ensures safe deactivation and learning capture.\nImplementing governance by design requires organizational change appropriate to current maturity. Organizations starting AI governance should focus on essential elements. Organizations with basic governance should extend coverage and build cross-functional integration. Organizations with mature governance should automate, integrate, and continuously improve.",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#review-questions",
    "href": "chapters/05-lifecycle.html#review-questions",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.9 Review Questions",
    "text": "5.9 Review Questions\n\nAn organization currently has AI governance focused on a review meeting before deployment. Development teams frequently express frustration that governance raises issues late in development that require significant rework. What governance by design principle does this situation illustrate, and how should the organization respond?\nA startup has a brilliant technical founder who built an impressive AI system largely alone. The company is now scaling and needs governance. What risks does single-perspective leadership create, and what perspectives should the company ensure are represented?\nDuring the design stage of an AI project, the technical team proposes a deep neural network architecture that achieves the highest accuracy on benchmark tests. However, governance review raises concerns about explainability requirements. What perspectives are in tension, and how might the tension be resolved?\nAn organization has implemented governance activities at all lifecycle stages but finds that different AI projects follow different approaches. What implementation challenge does this represent, and how might it be addressed?\nA deployed AI system has been operating for 18 months without issues. The monitoring team proposes reducing monitoring frequency to save costs. What governance considerations should inform this decision?",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/05-lifecycle.html#references",
    "href": "chapters/05-lifecycle.html#references",
    "title": "5  Governance by Design Across the Lifecycle",
    "section": "5.10 References",
    "text": "5.10 References\nCavoukian, Ann. “Privacy by Design: The 7 Foundational Principles.” Information and Privacy Commissioner of Ontario, 2009.\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.\nNational Institute of Standards and Technology. AI Risk Management Framework 1.0. NIST AI 100-1, 2023.\nMicrosoft. Responsible AI Standard, v2. Microsoft Corporation, 2022.\nOgunseye, S. “Letter to the Head of AI.” AI & Society (2025). https://doi.org/10.1007/s00146-025-02669-0",
    "crumbs": [
      "Perspectives and Integration",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Governance by Design Across the Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html",
    "href": "chapters/06-execution.html",
    "title": "6  Governance in Motion",
    "section": "",
    "text": "6.1 Introduction\nThe AI governance frameworks presented in previous chapters assume a reasonably sequential development process with distinct phases allowing governance review at each stage. But the practice of building with AI is changing rapidly. Development cycles that once took months now take weeks or days. Tools enable individuals with limited technical training to build AI-powered applications. The boundaries between development and deployment blur as systems learn and adapt continuously. Governance approaches designed for deliberate, phased development struggle to keep pace.\nThis chapter examines “execution compression,” the phenomenon of development and deployment cycles compressing to timescales that challenge traditional governance approaches. It explores how governance must adapt to remain effective when development moves faster than traditional review cycles allow. The chapter presents patterns for embedded governance that operates within compressed timelines rather than alongside them, and it considers the implications of AI tools that expand who can build AI systems and how quickly.\nThis material addresses emerging challenges that the AIGP Body of Knowledge touches upon in its discussion of ongoing issues and that governance professionals will increasingly face as AI capabilities advance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#understanding-execution-compression",
    "href": "chapters/06-execution.html#understanding-execution-compression",
    "title": "6  Governance in Motion",
    "section": "6.2 Understanding Execution Compression",
    "text": "6.2 Understanding Execution Compression\nExecution compression refers to the reduction in time required to move from idea to deployed AI capability. Where traditional software development might follow multi-month cycles with distinct requirements, design, development, testing, and deployment phases, modern AI development with advanced tools can compress this to days or hours.\n\nSources of Compression\nSeveral factors drive execution compression.\nPre-trained models eliminate the need to train models from scratch for many applications. A developer can access powerful language models, image models, or other capabilities through APIs without gathering training data, designing model architecture, or conducting model training. The development task shifts from building AI to applying AI.\nLow-code and no-code platforms enable people without deep technical expertise to create AI-powered applications. Visual interfaces, natural language instructions, and pre-built components reduce the skill and time required to build functional applications.\nAI-assisted development uses AI to accelerate development itself. Code generation tools write code from natural language descriptions. Testing tools generate test cases automatically. Documentation tools produce documentation from code. Development tasks that previously required hours of skilled labor can be completed in minutes.\nCloud infrastructure eliminates provisioning delays. Computing resources are available instantly rather than requiring procurement and setup. Deployment can occur with a few clicks rather than requiring infrastructure preparation.\nContinuous deployment practices push changes to production rapidly and frequently rather than accumulating changes for periodic releases. What was deployed weekly might now deploy multiple times daily.\n\n\nImplications for Governance\nExecution compression challenges governance in several ways.\nReview cycles may be too slow. If development moves from idea to deployment in days, governance review cycles measured in weeks cannot provide pre-deployment scrutiny. Governance that arrives after deployment has already occurred provides limited value.\nSequential governance breaks down. Traditional governance assumes distinct phases where specific governance activities occur. When phases collapse into continuous flow, governance activities must be reorganized.\nScale of governance increases. When each developer can build and deploy AI applications rapidly, the volume of AI requiring governance increases dramatically. Governance processes designed for a handful of significant AI projects cannot scale to hundreds or thousands of small applications.\nAccountability diffuses. When individuals can build and deploy AI without involving traditional development teams, the organizational roles that governance traditionally holds accountable may not be involved. Who is responsible for governance when anyone can build AI?\nChange velocity increases. When systems change frequently, governance that assumed stable deployed systems must adapt to systems in constant flux.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#where-governance-gets-squeezed",
    "href": "chapters/06-execution.html#where-governance-gets-squeezed",
    "title": "6  Governance in Motion",
    "section": "6.3 Where Governance Gets Squeezed",
    "text": "6.3 Where Governance Gets Squeezed\nWhen execution compresses, governance activities that previously fit comfortably into development timelines get squeezed. Understanding where compression creates the most pressure helps prioritize governance adaptation.\n\nImpact Assessment\nTraditional impact assessments require time to gather information, consult stakeholders, analyze risks, and document conclusions. A thorough impact assessment might take weeks. When development takes days, there is no time for traditional assessment.\nThe pressure point is not the assessment itself but the underlying goal: ensuring that significant risks are identified and addressed before deployment. The challenge is achieving that goal through means that fit compressed timelines.\n\n\nLegal Review\nTraditional legal review involves lawyers analyzing proposed AI applications against applicable requirements. For novel applications or significant risks, this review might require substantial legal analysis. When development moves faster than legal review, systems may deploy without legal scrutiny.\nThe pressure point is ensuring legal compliance without requiring extensive legal involvement in every AI application. This suggests need for clearer upfront guidance, automated compliance checking where possible, and risk-based triage to focus legal attention where it matters most.\n\n\nTesting and Validation\nComprehensive testing requires time: time to design tests, execute tests, analyze results, and address issues. When development compresses, testing is often what gets squeezed. Systems deploy with less testing than governance would prefer.\nThe pressure point is ensuring adequate validation without requiring extensive testing time. This suggests need for automated testing, risk-proportionate testing requirements, and continuous testing approaches that validate throughout development rather than only at the end.\n\n\nDocumentation\nCreating and maintaining governance documentation takes time. When development moves quickly, documentation often lags or is omitted entirely. Systems deploy without the documentation that governance policies require.\nThe pressure point is ensuring appropriate documentation exists without requiring extensive documentation effort. This suggests need for automated documentation generation, documentation integrated into development tools, and right-sized documentation requirements based on risk.\n\n\nHuman Oversight\nMeaningful human oversight requires humans to have time to understand AI outputs, exercise judgment, and intervene when appropriate. When AI operates at machine speed on high volumes, human review of individual decisions becomes impossible.\nThe pressure point is ensuring meaningful human control without requiring humans to review every AI decision. This suggests need for risk-based determination of what requires human review, statistical monitoring as alternative to individual review, and intervention capabilities that enable humans to act when needed.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#from-gatekeeping-to-embedded-governance",
    "href": "chapters/06-execution.html#from-gatekeeping-to-embedded-governance",
    "title": "6  Governance in Motion",
    "section": "6.4 From Gatekeeping to Embedded Governance",
    "text": "6.4 From Gatekeeping to Embedded Governance\nTraditional governance operates through gatekeeping: review points where governance teams evaluate whether AI applications should proceed. Gatekeeping works when development moves slowly enough that review points do not create unacceptable delays. When execution compresses, gatekeeping becomes a bottleneck.\nThe alternative is embedded governance: governance mechanisms built into development processes and tools so that governance occurs as development occurs rather than alongside it.\n\nCharacteristics of Embedded Governance\nEmbedded governance is continuous rather than episodic. Rather than governance review at designated checkpoints, governance activities occur throughout development as integral parts of the process.\nEmbedded governance is automated where possible. Automated checks, automated documentation, automated monitoring reduce governance burden while ensuring governance activities actually occur.\nEmbedded governance is proportionate to risk. Not every AI application requires the same governance intensity. Embedded approaches apply light-touch governance to low-risk applications while escalating higher-risk applications for more intensive scrutiny.\nEmbedded governance is designed into tools and processes rather than being added alongside them. When governance is built into the development environment, compliance becomes the path of least resistance rather than an additional burden.\n\n\nExamples of Embedded Governance\nRisk-based routing automatically classifies proposed AI applications based on risk indicators and routes them to appropriate governance processes. Low-risk applications might proceed with minimal review while high-risk applications receive intensive scrutiny.\nAutomated policy checking validates proposed AI applications against defined policies. A tool might check whether proposed data use complies with data governance policies, whether required disclosures are planned, or whether prohibited use cases are being attempted.\nIntegrated documentation tools generate governance documentation from development artifacts rather than requiring separate documentation effort. Model cards, data sheets, and compliance documentation can be auto-populated from information already captured during development.\nContinuous testing infrastructure runs governance-relevant tests automatically as part of development and deployment pipelines. Fairness tests, accuracy tests, and security tests execute automatically rather than requiring separate testing phases.\nReal-time monitoring begins when deployment begins rather than being added afterward. Monitoring infrastructure is part of the deployment platform, not a separate system that must be integrated.\nAutomated compliance reporting aggregates information from development and deployment into reports that satisfy compliance requirements without manual report generation.\n\n\nImplementing Embedded Governance\nMoving from gatekeeping to embedded governance requires investment in several areas.\nPolicy codification translates governance policies into rules that can be checked automatically. This requires making policies precise enough to automate while preserving their intent.\nTool integration builds governance capabilities into development tools rather than operating separate governance systems. This may require working with tool vendors or building custom integrations.\nProcess redesign restructures development processes to incorporate governance activities rather than treating governance as a parallel track.\nMetrics and monitoring establish visibility into whether embedded governance is operating effectively. Without visibility, embedded governance may fail silently.\nException handling establishes how situations that embedded governance cannot handle automatically are escalated for human judgment. Embedded governance handles routine cases; humans handle exceptions.\n\n\n\n\n\n\nFigure 6.1: Gatekeeping vs. Embedded Governance\n\n\n\nFigure 6.1: Gatekeeping vs. Embedded Governance — Comparison showing how embedded governance maintains coverage while enabling faster development.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#role-fluidity-and-accountability-gaps",
    "href": "chapters/06-execution.html#role-fluidity-and-accountability-gaps",
    "title": "6  Governance in Motion",
    "section": "6.5 Role Fluidity and Accountability Gaps",
    "text": "6.5 Role Fluidity and Accountability Gaps\nExecution compression and democratization of AI building create situations where traditional governance roles may not apply. Understanding these changes helps governance adapt.\n\nChanging Developer Profiles\nTraditionally, AI development required specialized skills: data science, machine learning engineering, and related expertise. Governance could assume that AI was built by technical teams who could be held accountable and who had some understanding of AI characteristics.\nToday, AI capabilities are accessible to people without specialized training. A marketing professional can use AI tools to build a customer-facing chatbot. A analyst can use AI to automate data processing. A manager can use AI to generate reports. These “citizen developers” may not understand AI characteristics, risks, or governance requirements.\nGovernance must account for developers who may not know what they do not know. This suggests need for guardrails built into tools, simplified guidance for non-specialist developers, and mechanisms to identify AI built outside traditional channels.\n\n\nBlurring of Build and Deploy\nTraditional governance distinguished between development and deployment, with different governance activities appropriate to each. When systems update continuously, when deployment is automated, when there is no clear line between building and running, this distinction breaks down.\nGovernance must address continuous change rather than discrete releases. This suggests need for continuous monitoring rather than pre-deployment validation alone, automated regression testing that catches problematic changes, and rollback capabilities that can quickly undo harmful deployments.\n\n\nAccountability in Tool-Mediated Development\nWhen AI is built using pre-trained models, cloud services, and no-code platforms, responsibility distributes across multiple parties. The person who assembled the application, the vendor who provided the model, the platform that enabled deployment, the organization that deployed the result: all contributed, but who is accountable?\nGovernance must clarify accountability even when multiple parties are involved. This suggests need for clear contractual allocation of responsibility, organizational policies that assign accountability regardless of how AI was built, and vendor management that ensures external parties meet appropriate standards.\n\n\nGovernance for AI Agents\nEmerging AI systems can take autonomous action, not just provide outputs for humans to act upon. AI agents can browse the web, execute code, interact with services, and accomplish multi-step tasks with minimal human involvement. These systems compress not just development but decision-making, with AI making choices that might previously have required human judgment.\nGovernance must address AI systems that act, not just AI systems that recommend. This suggests need for boundaries on what autonomous action AI can take, monitoring of AI actions not just AI outputs, and mechanisms to intervene when AI actions go wrong.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#tools-driving-compression-and-the-expanding-governance-surface",
    "href": "chapters/06-execution.html#tools-driving-compression-and-the-expanding-governance-surface",
    "title": "6  Governance in Motion",
    "section": "6.6 Tools Driving Compression and the Expanding Governance Surface",
    "text": "6.6 Tools Driving Compression and the Expanding Governance Surface\nSpecific categories of tools are driving execution compression and changing the governance landscape.\n\nLarge Language Models and Generative AI\nLarge language models have dramatically compressed the time required to generate text, code, analysis, and creative content. Tasks that required hours of skilled human effort can be completed in seconds.\nGovernance implications include potential for generation of harmful or inaccurate content at scale, intellectual property concerns when models produce content derived from training data, and difficulty attributing responsibility when AI generates outputs that cause harm.\n\n\nAI-Assisted Development Tools\nAI-assisted development tools, including code generation, automated testing, and intelligent development environments, compress development time and expand who can build software.\nGovernance implications include potential for generated code to contain security vulnerabilities or bugs, difficulty ensuring code quality when developers do not fully understand generated code, and need for governance to extend to AI-generated artifacts.\n\n\nNo-Code and Low-Code AI Platforms\nNo-code and low-code platforms enable non-technical users to build AI-powered applications through visual interfaces and pre-built components.\nGovernance implications include expansion of AI building beyond technical teams who traditionally bore governance responsibility, potential for applications built without understanding of AI characteristics or risks, and need for governance guardrails built into platforms.\n\n\nAPI-Accessible AI Services\nCloud providers offer AI capabilities through APIs that developers can integrate into applications with minimal effort. Sentiment analysis, image recognition, language translation, and many other capabilities are available as services.\nGovernance implications include dependence on external services that may change without notice, difficulty ensuring external services meet governance requirements, and need for vendor management processes that address API services.\n\n\nAI Agent Frameworks\nFrameworks for building AI agents that can take autonomous action are emerging rapidly. These frameworks enable developers to create systems that can browse the web, execute code, manage files, and interact with external services.\nGovernance implications include potential for AI to take harmful actions without human review, difficulty predicting what autonomous agents will do, and need for boundaries and monitoring around autonomous AI action.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#governance-patterns-for-compressed-execution",
    "href": "chapters/06-execution.html#governance-patterns-for-compressed-execution",
    "title": "6  Governance in Motion",
    "section": "6.7 Governance Patterns for Compressed Execution",
    "text": "6.7 Governance Patterns for Compressed Execution\nSeveral patterns help governance remain effective despite execution compression.\n\nRisk-Based Triage\nNot all AI applications require the same governance intensity. Risk-based triage quickly classifies applications and routes them to appropriate governance tracks.\nLow-risk applications might proceed with minimal review, relying on built-in guardrails and automated checks. Medium-risk applications might require documented assessment and standard review. High-risk applications might require intensive scrutiny, legal review, and executive approval.\nThe key is quick, reliable classification that directs governance effort where it matters most.\n\n\nGuardrails and Defaults\nWhen governance cannot review every application, guardrails prevent the worst outcomes and defaults encourage appropriate behavior.\nGuardrails are hard constraints that prevent certain actions: prohibited use cases blocked by policy, data access limited by technical controls, deployment restricted to approved environments. Guardrails operate automatically without requiring review.\nDefaults are soft constraints that guide behavior toward appropriate choices: templates that include required elements, workflows that prompt for governance information, tools that suggest compliant approaches. Defaults can be overridden, but compliance is the path of least resistance.\n\n\nContinuous Monitoring and Response\nWhen pre-deployment review cannot be comprehensive, post-deployment monitoring provides ongoing assurance. Issues that review did not catch can be detected through monitoring and addressed through response.\nMonitoring must operate at appropriate timescales. If systems change hourly, monitoring that reports weekly cannot keep pace. Real-time or near-real-time monitoring enables rapid response to emerging issues.\nResponse capabilities must match monitoring speed. Detecting a problem quickly provides limited value if response takes weeks. Automated response, rapid escalation, and pre-planned remediation enable timely action.\n\n\nProgressive Rollout\nRather than deploying fully at once, progressive rollout deploys to increasing audiences over time. Initial deployment might serve a small percentage of users, expanding as confidence builds.\nProgressive rollout limits the blast radius of problems. If an issue emerges, it affects fewer users than full deployment would have affected. The organization has opportunity to detect and address issues before they reach everyone.\nProgressive rollout requires monitoring that can detect issues in partial deployment and mechanisms to expand or contract deployment based on observed behavior.\n\n\nAutomated Compliance Evidence\nGovernance often requires evidence that compliance activities occurred. Traditionally, this evidence was created through manual documentation. When development moves quickly, manual documentation becomes a bottleneck.\nAutomated systems can capture compliance evidence as a byproduct of development and deployment. Test results, configuration records, approval workflows, monitoring data: all can be captured automatically and aggregated into compliance records.\nAutomated evidence collection ensures evidence exists even when development moves too quickly for manual documentation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#implementing-governance-for-compressed-execution",
    "href": "chapters/06-execution.html#implementing-governance-for-compressed-execution",
    "title": "6  Governance in Motion",
    "section": "6.8 Implementing Governance for Compressed Execution",
    "text": "6.8 Implementing Governance for Compressed Execution\nOrganizations must adapt governance practices to address execution compression while maintaining appropriate risk management.\n\nAssess Current State\nBegin by understanding how execution compression affects your organization. Which AI development happens through traditional channels with deliberate timelines? Which happens rapidly through self-service tools? Where are the gaps between current governance and actual development patterns?\nThis assessment may reveal AI applications that are not captured by current governance processes, development patterns that bypass established review points, or governance bottlenecks that teams are working around.\n\n\nEstablish Risk-Based Framework\nDevelop a risk classification framework that enables quick triage of AI applications. Define criteria that can be assessed rapidly, potentially through automated questionnaires or rule-based classification.\nMap governance requirements to risk levels. Low-risk applications need minimal governance burden; high-risk applications need thorough review regardless of timeline pressure.\n\n\nBuild Embedded Capabilities\nIdentify governance activities that can be embedded into development processes and tools. Prioritize activities that are currently bottlenecks or that are frequently skipped due to time pressure.\nWork with development tool owners to integrate governance capabilities. This might mean working with platform teams who manage internal tools, with vendors who provide commercial tools, or with both.\n\n\nStrengthen Monitoring and Response\nInvest in monitoring capabilities that can detect governance-relevant issues in deployed AI systems. Connect monitoring to response capabilities that can address issues quickly.\nEstablish escalation paths for issues that require human judgment. Not everything can be automated, and rapid escalation ensures human attention when needed.\n\n\nAddress the Expanding Developer Population\nDevelop governance approaches for non-specialist AI builders. This might include simplified guidance, required training, guardrails in self-service tools, or registration requirements for AI applications.\nCreate mechanisms to identify AI applications built outside traditional channels. If governance does not know about applications, governance cannot address them.\n\n\nMaintain Accountability\nClarify accountability for AI outcomes regardless of how AI was built. Organizational accountability should not depend on development methodology.\nAddress vendor and platform accountability through contracts and vendor management. When AI depends on external services, governance must extend to those dependencies.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#chapter-summary",
    "href": "chapters/06-execution.html#chapter-summary",
    "title": "6  Governance in Motion",
    "section": "6.9 Chapter Summary",
    "text": "6.9 Chapter Summary\nThis chapter examined execution compression and its implications for AI governance, presenting approaches for governance that remains effective when development and deployment accelerate.\nExecution compression results from pre-trained models, low-code platforms, AI-assisted development, cloud infrastructure, and continuous deployment practices. These factors reduce the time from idea to deployed AI capability from months to days or hours.\nCompression challenges traditional governance by making review cycles too slow, breaking down sequential governance phases, increasing the scale of AI requiring governance, diffusing accountability, and increasing the velocity of change.\nGovernance gets squeezed in impact assessment, legal review, testing, documentation, and human oversight. Each area requires adaptation to fit compressed timelines while maintaining governance goals.\nEmbedded governance offers an alternative to gatekeeping. Rather than reviewing at checkpoints, embedded governance integrates governance activities into development processes and tools through automated checks, integrated documentation, continuous testing, and real-time monitoring.\nRole fluidity creates accountability gaps as AI building expands beyond traditional development teams to include citizen developers using accessible tools. Governance must address developers who may not understand AI risks, continuous change rather than discrete releases, distributed accountability when multiple parties contribute, and AI agents that take autonomous action.\nSpecific tool categories driving compression include large language models, AI-assisted development tools, no-code platforms, API-accessible AI services, and AI agent frameworks. Each creates governance implications requiring attention.\nGovernance patterns for compressed execution include risk-based triage, guardrails and defaults, continuous monitoring and response, progressive rollout, and automated compliance evidence.\nImplementing governance for compressed execution requires assessing current state, establishing risk-based frameworks, building embedded capabilities, strengthening monitoring and response, addressing the expanding developer population, and maintaining accountability.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#review-questions",
    "href": "chapters/06-execution.html#review-questions",
    "title": "6  Governance in Motion",
    "section": "6.10 Review Questions",
    "text": "6.10 Review Questions\n\nAn organization has established AI governance processes including a review committee that meets monthly to evaluate proposed AI applications. Development teams complain that the monthly cycle does not fit their sprint-based development approach, and some teams have begun deploying AI applications without committee review. What governance principle does this situation illustrate, and how might the organization respond?\nA no-code platform enables marketing staff to create AI-powered chatbots for customer interaction. The platform is popular and multiple chatbots have been deployed. The governance team was not aware of these deployments until a customer complained about an inappropriate chatbot response. What governance challenge does this represent?\nAn organization is implementing embedded governance and wants to automate compliance checking for AI applications. Which types of governance requirements are most amenable to automated checking, and which require human judgment?\nA deployed AI system updates its behavior based on continuous learning from user interactions. Traditional governance treated deployment as a point event followed by stable operation. How should governance adapt to continuous learning systems?\nAn AI agent framework enables developers to create systems that can browse the web, execute code, and interact with external services autonomously. What governance considerations are most important for AI agents compared to AI systems that only provide recommendations?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/06-execution.html#references",
    "href": "chapters/06-execution.html#references",
    "title": "6  Governance in Motion",
    "section": "6.11 References",
    "text": "6.11 References\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.\nAnthropic. “Claude’s Character.” Anthropic Research, 2024.\nOpenAI. “GPT-4 System Card.” OpenAI Technical Report, 2023.\nGoogle DeepMind. “Gemini: A Family of Highly Capable Multimodal Models.” Google DeepMind Technical Report, 2023.\nMicrosoft. “Responsible AI Standard, v2.” Microsoft Corporation, 2022.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Governance in Motion</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html",
    "href": "chapters/07-future.html",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "",
    "text": "7.1 Introduction\nAI governance is a young field addressing a rapidly evolving technology within shifting legal and social landscapes. Many fundamental questions remain unsettled: how liability for AI harms should be allocated, how intellectual property law should adapt to AI, what the AI auditing profession should look like, how workforces will transform as AI capabilities expand, and how individuals who prefer not to interact with AI can be accommodated.\nThis chapter examines ongoing issues that AI governance professionals should monitor and prepare for, even where clear answers have not yet emerged. It also looks toward future directions, considering how the field may develop as AI capabilities advance, regulatory frameworks mature, and organizational practices evolve. The goal is not to predict the future but to prepare readers to engage with emerging challenges as they arise.\nThe issues discussed here appear in the AIGP Body of Knowledge as topics requiring awareness without necessarily having settled best practices. AI governance professionals should understand these issues, track developments, and help their organizations navigate uncertainty as clarity emerges.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#liability-frameworks-for-ai-harms",
    "href": "chapters/07-future.html#liability-frameworks-for-ai-harms",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.2 Liability Frameworks for AI Harms",
    "text": "7.2 Liability Frameworks for AI Harms\nWhen AI systems cause harm, questions arise about who bears legal responsibility. Traditional liability frameworks were not designed for AI’s distinctive characteristics, and how those frameworks will adapt remains unsettled.\n\nChallenges in AI Liability\nAI complicates traditional liability analysis in several ways.\nCausation is difficult to establish. When a deep learning model contributes to a harmful outcome, tracing causation through millions of parameters and learned patterns may be practically impossible. Traditional liability often requires showing how the defendant’s conduct caused the plaintiff’s harm; AI opacity can make this showing difficult.\nMultiple parties contribute. An AI system might involve a model developed by one company, trained on data from multiple sources, deployed by another company, operating on infrastructure from a cloud provider. When harm occurs, identifying which party’s contribution was legally significant may be contentious.\nForeseeability is uncertain. Liability often depends on whether harm was foreseeable. AI systems can behave in unexpected ways, and developers may genuinely not have anticipated harmful behaviors that emerged from learning processes they did not fully control.\nExisting categories may not fit. Product liability traditionally distinguishes manufacturing defects (individual units deviate from design) from design defects (the design itself is dangerous). AI “defects” may not fit these categories neatly.\n\n\nEmerging Approaches\nJurisdictions are developing approaches to AI liability, though consensus has not emerged.\nThe EU is most advanced with its proposed AI Liability Directive, which would establish rebuttable presumptions that make it easier for claimants to establish AI-related causation. If a defendant failed to comply with certain duties of care, and harm occurred that the compliance failure made more likely, the defendant would be presumed to have caused the harm unless they could prove otherwise. The directive would also require defendants to disclose evidence relevant to AI systems.\nThe United States has no comprehensive AI liability framework, leaving courts to apply existing doctrines with uncertain results. Product liability, negligence, and other traditional theories apply, but how they apply to AI is being worked out case by case.\nSome propose strict liability for certain AI uses, holding deployers liable for harms without requiring proof of fault. This approach would simplify liability determination and create strong incentives for care, but critics argue it could chill beneficial AI innovation.\n\n\nImplications for Governance\nEven without settled liability rules, governance professionals can take steps to manage liability exposure.\nDocumentation of diligence provides evidence that the organization acted reasonably, which matters under most liability standards. Impact assessments, testing records, monitoring data, and incident response documentation all support defense against liability claims.\nInsurance can transfer some financial risk, though AI-specific coverage is still developing. Organizations should evaluate their insurance coverage for AI-related claims and consider whether specialized coverage is warranted.\nContractual allocation addresses liability as between contracting parties, even if it does not affect third-party claims. Vendor contracts, deployment agreements, and terms of service should address AI-related liability allocation.\nRisk-proportionate use means avoiding high-risk AI applications where the organization is not prepared to accept potential liability. If liability exposure for an AI application is unacceptable, the application should not proceed.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#intellectual-property-and-ai",
    "href": "chapters/07-future.html#intellectual-property-and-ai",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.3 Intellectual Property and AI",
    "text": "7.3 Intellectual Property and AI\nAI intersects with intellectual property law in novel ways that existing frameworks do not cleanly address.\n\nTraining Data and Copyright\nTraining AI models on copyrighted works raises unresolved questions. Model training involves making copies of training data, which implicates reproduction rights. The trained model arguably derives from those works, which implicates derivative work rights.\nWhether training on copyrighted works constitutes fair use (in the United States) or falls under other exceptions (in other jurisdictions) is unsettled. Major lawsuits are pending, and outcomes will significantly shape the legal landscape for AI training.\nOrganizations training AI models should understand their training data sources and the intellectual property implications. Using data without clear rights creates legal risk that may materialize as case law develops.\n\n\nAI-Generated Outputs and Copyright\nWhether AI-generated outputs are copyrightable is contested. The U.S. Copyright Office has stated that copyright requires human authorship, and works created autonomously by AI without creative human involvement are not copyrightable. This leaves AI-generated content in a precarious position where it may lack copyright protection.\nThe human authorship requirement does not mean AI-assisted works are never copyrightable. Works where humans provide creative input and use AI as a tool may qualify for copyright based on the human contribution. The boundary between AI-as-tool and AI-as-creator is not clearly defined.\nOrganizations should understand the copyright status of AI-generated content they create or rely upon. Content that is not copyrightable cannot be protected from copying by others.\n\n\nPatents and AI\nPatent systems globally have determined that AI cannot be listed as an inventor; only humans can be inventors for patent purposes. This does not prevent patenting inventions developed with AI assistance, provided a human qualifies as inventor.\nAI may increasingly contribute to patentable inventions, raising questions about inventorship that current frameworks do not fully address. If AI contributes the inventive step while humans merely implement or select from AI-generated options, does human inventorship still apply?\n\n\nTrade Secrets and AI\nTrade secret law may protect AI-related assets including trained models, training data, and development techniques, provided they meet trade secret requirements: they derive economic value from secrecy and are subject to reasonable secrecy measures.\nMaintaining trade secret protection requires ongoing diligence. Organizations must implement and maintain secrecy measures appropriate to the asset’s value and the threats it faces.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#the-emerging-ai-auditing-profession",
    "href": "chapters/07-future.html#the-emerging-ai-auditing-profession",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.4 The Emerging AI Auditing Profession",
    "text": "7.4 The Emerging AI Auditing Profession\nAs AI regulation expands and organizational commitment to responsible AI grows, demand increases for independent verification of AI systems and practices. An AI auditing profession is emerging to meet this demand, though it remains in early stages.\n\nCurrent State\nAI auditing is currently fragmented. Some traditional audit firms are building AI audit capabilities. Specialized AI audit firms are emerging. Academic researchers conduct AI audits as research projects. Regulatory bodies conduct examinations of AI in regulated industries. Civil society organizations conduct accountability investigations.\nNo single professional framework governs AI auditing comparable to financial auditing frameworks. Standards for what AI audits should examine, what methodologies are appropriate, and what assurance audits can provide are still developing.\n\n\nChallenges in AI Auditing\nSeveral challenges complicate AI audit practice.\nTechnical complexity requires auditors to understand AI systems deeply enough to evaluate them meaningfully. This requires technical expertise that traditional auditors may lack.\nAccess limitations constrain what auditors can examine. Organizations may not provide full access to training data, model parameters, or system operations. Black-box testing may be all that is possible.\nDynamic systems change over time. An audit conducted at one point may not reflect system behavior after subsequent updates or learning.\nStandard setting is incomplete. Without agreed standards for what constitutes adequate AI, auditors must make judgment calls that may be contested.\nAssurance limitations mean auditors cannot guarantee AI system behavior. Audit findings reflect what was observed during the audit period using available methods; they cannot promise future performance.\n\n\nEmerging Frameworks\nSeveral efforts aim to develop AI auditing frameworks.\nThe NIST AI Risk Management Framework provides a structure for AI risk management that auditors can use as a reference, though it is not specifically an audit standard.\nISO/IEC 42001 provides requirements for AI management systems that can be audited for conformance.\nRegulatory requirements like the EU AI Act create audit-relevant obligations that auditors can verify.\nIndustry initiatives are developing sector-specific audit frameworks, such as frameworks for AI in financial services or healthcare.\n\n\nImplications for Governance\nGovernance professionals should prepare for increased audit scrutiny of AI systems.\nDocumentation practices should produce audit-ready records. If auditors will want to see evidence of testing, monitoring, incident response, and governance processes, that evidence should exist and be accessible.\nInternal audit should build AI audit capabilities or ensure access to external expertise. As AI governance matures, internal audit attention to AI will increase.\nExternal audit relationships should address AI. Whether through existing audit relationships or specialized AI auditors, organizations should have access to independent AI assurance.\nAudit readiness should be part of AI governance maturity. Organizations should periodically assess whether their AI systems and practices could withstand audit scrutiny.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#workforce-transformation",
    "href": "chapters/07-future.html#workforce-transformation",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.5 Workforce Transformation",
    "text": "7.5 Workforce Transformation\nAI is transforming how work gets done, affecting both how workers use AI and which work AI might displace. These workforce implications are significant ongoing issues for organizations and society.\n\nAI Augmentation\nAI increasingly augments worker capabilities rather than replacing workers entirely. Knowledge workers use AI assistants for research, writing, and analysis. Customer service workers use AI to handle routine inquiries while focusing on complex issues. Creative workers use AI for ideation and production assistance.\nEffective AI augmentation requires attention to how AI and humans work together. Poor integration can frustrate workers, reduce quality, or create new risks. Effective integration amplifies human capabilities while maintaining appropriate human judgment.\nTraining workers to use AI effectively becomes an organizational priority. Workers need to understand what AI can and cannot do, how to evaluate AI outputs critically, when to rely on AI and when to exercise independent judgment, and how to identify when AI is making errors.\n\n\nWorkforce Displacement\nAI may reduce demand for certain types of work, potentially displacing workers. While economists debate long-term employment effects, transition disruptions are real.\nSome tasks are more susceptible to AI displacement: routine cognitive work, pattern matching, data processing, and content generation among them. Work requiring physical presence, complex interpersonal interaction, novel problem-solving, and creative judgment may be more resistant.\nOrganizations face choices about how to manage AI’s workforce impacts. Some approaches prioritize automation efficiency; others prioritize worker retention and transition. Organizational values, stakeholder expectations, and strategic considerations all inform these choices.\n\n\nWorker Well-being\nAI in the workplace affects worker well-being in ways governance should consider.\nSurveillance concerns arise when AI monitors worker performance, communications, or behavior. Intensive monitoring can feel dehumanizing and may affect mental health and job satisfaction.\nAutonomy concerns arise when AI directs work rather than supporting it. Workers who feel controlled by algorithms may experience reduced job satisfaction and engagement.\nSkill concerns arise when AI handles tasks that previously developed worker capabilities. Workers may lose skills they do not practice, potentially reducing their value and adaptability.\nJob quality concerns arise when AI changes the nature of work. Work that was varied may become routine; work that was meaningful may become mechanical.\nGovernance should consider these impacts alongside efficiency gains. Sustainable AI adoption requires attention to the humans who work alongside AI.\n\n\nImplications for Governance\nWorkforce considerations should be part of AI governance.\nWorkforce impact assessment should be part of evaluating proposed AI applications. How will this AI affect workers? What training or support do they need? What displacement might occur?\nWorker voice should inform AI decisions. Workers who will use or be affected by AI systems have perspectives that governance should consider.\nTransition support should be part of AI deployment planning. When AI will displace work, plans should address affected workers.\nWell-being monitoring should track AI’s effects on workers. If AI is harming well-being, adjustments may be needed.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#opt-out-rights-and-alternatives",
    "href": "chapters/07-future.html#opt-out-rights-and-alternatives",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.6 Opt-Out Rights and Alternatives",
    "text": "7.6 Opt-Out Rights and Alternatives\nAs AI becomes ubiquitous, questions arise about whether and how individuals can avoid AI interactions. Some people prefer not to interact with AI for various reasons: distrust of technology, desire for human connection, concerns about privacy, or simply personal preference.\n\nThe Opt-Out Challenge\nProviding meaningful opt-out is challenging when AI is deeply integrated into products and services.\nPractical challenges arise when AI is embedded in systems rather than offered as a separate option. If a company uses AI for customer service, offering a non-AI alternative may require maintaining parallel systems.\nQuality differences may exist between AI and non-AI alternatives. If the AI option is faster, more accurate, or otherwise better, the non-AI alternative may be inferior in ways that disadvantage those who opt out.\nDiscrimination concerns arise if opt-out is harder for some populations. If opting out requires technical sophistication, digital access, or awareness that AI is involved, some people may be unable to exercise opt-out even if they would prefer to.\nBusiness model challenges arise when AI is essential to the service. If AI enables pricing, availability, or features that make the service viable, requiring non-AI alternatives may not be economically feasible.\n\n\nLegal Requirements\nSome legal frameworks require alternatives to AI processing.\nGDPR Article 22 provides the right not to be subject to decisions based solely on automated processing with legal or significant effects. This right includes the ability to obtain human intervention. However, significant exceptions apply, and what constitutes meaningful human intervention is not fully defined.\nEmerging laws in some jurisdictions require human alternatives in specific contexts. For example, some laws require human review options for AI in employment decisions.\nConsumer protection principles may require that essential services not be conditioned entirely on AI interaction, though this principle is not universally established.\n\n\nImplications for Governance\nGovernance should address opt-out and alternatives where appropriate.\nAssessment should identify where individuals might reasonably want to avoid AI interaction. Consequential decisions, sensitive contexts, and situations where human connection matters may warrant alternatives.\nDesign should enable alternatives where they are appropriate. Building in human pathways from the start is easier than retrofitting them later.\nCommunication should inform individuals about AI involvement so they can make informed choices. Transparency enables autonomy even if full opt-out is not feasible.\nEquity should ensure that alternatives do not disadvantage those who use them. If opting out of AI means worse service, the alternative is not meaningful.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#preventive-governance-and-the-limits-of-point-in-time-consent",
    "href": "chapters/07-future.html#preventive-governance-and-the-limits-of-point-in-time-consent",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.7 Preventive Governance and the Limits of Point-in-Time Consent",
    "text": "7.7 Preventive Governance and the Limits of Point-in-Time Consent\nMuch of contemporary AI governance operates reactively. Organizations deploy systems, regulators respond with rules, auditors examine compliance after the fact, and enforcement arrives when harms have already occurred. This pattern—build first, govern later—has deep roots in technology regulation. But AI systems present characteristics that make reactive governance structurally inadequate.\n\nThe Reactive Governance Problem\nConsider the typical consent interaction: a user clicks “I agree” to terms permitting data use for “service improvement and AI training.” At that moment, neither party can fully specify what this means. The organization doesn’t know what models it will build in three years, what capabilities those models will have, or how the data might combine with other sources. The user certainly doesn’t know. The consent is real, but what exactly has been consented to?\nThis is not a disclosure problem that better privacy policies can solve. It is a temporal problem: the meaning of the agreement is constituted by future events that haven’t happened yet.\n\n\nThree Conditions That Undermine Point-in-Time Consent\nTraditional consent works reasonably well when you know what you’re agreeing to, when you can change your mind later, and when the consequences are bounded. AI and data systems often fail all three conditions.\nPractical irreversibility. Once data enters training pipelines, the effects propagate in ways that are difficult or impossible to undo. A model trained on your data doesn’t have a “delete this person’s contribution” button. Derived inferences, downstream models, and knowledge representations persist even after source data deletion. The GDPR’s right to erasure runs into technical limits when erasure cannot meaningfully reach the artifacts that matter.\nCompounding downstream value. Data and model capabilities compound over time. Your browsing history from 2019 might be marginally useful alone, but combined with millions of other users’ data, processed through increasingly sophisticated models, and integrated into systems you’ve never heard of, it becomes part of something far more valuable—and potentially more consequential—than anything you could have anticipated. The value extracted grows geometrically while any compensation or control you retained remains fixed at the moment of agreement.\nTemporally unknowable consequences. Future uses depend on technologies that don’t exist yet, business models not yet invented, and regulatory environments not yet established. When you agreed to let a photo app use your images in 2018, you weren’t agreeing to facial recognition training—that use emerged from later capabilities and decisions. The downstream meaning of consent is constructed by subsequent events, not merely revealed.\nWhen all three conditions hold, consent becomes a weak legitimating mechanism. It finalizes authority before anyone can know what that authority will mean.\n\n\nFrom Reactive to Preventive Governance\nPreventive governance doesn’t abandon consent—it restructures how and when authority settles. Instead of treating agreement as a single legitimating event, preventive governance distributes authorization across time, binding it to actual uses rather than hypothetical possibilities.\nThe core insight is that if the problem is temporal, the solution must be temporal. Governance must shape the conditions under which authority becomes final, not merely respond after finalization has occurred.\nThis is not paternalism. The mechanisms don’t override user choices or prohibit transactions. They create structured opportunities for reconsideration as circumstances change, ensuring that authority settles only when its meaning can be meaningfully understood.\n\n\nMechanisms for Preventive Governance\nSeveral design patterns can implement preventive governance in practice.\nPhased and staged permissions. Rather than requesting all possible permissions at onboarding, systems can stage authorization to match actual use categories. A minimal tier covers what’s strictly necessary for service delivery. Additional tiers—for personalization, for model training, for external sharing—activate only when those uses actually arise, with specific disclosure tied to concrete purposes rather than speculative future possibilities. This approach aligns authority settlement with actual use. Users aren’t asked to authorize model training until the organization actually wants to use their data for training, at which point both parties have clearer understanding of what’s involved.\nTime-bounded authority with renewal. Permissions, especially for high-uncertainty uses, can expire unless renewed. Rolling renewal cycles—annually for training rights, more frequently for sharing permissions—prevent “perpetual consent” and force periodic re-legitimation as contexts evolve. This doesn’t mean bombarding users with renewal requests. Low-risk operational permissions remain stable. The renewal requirement applies to high-risk, high-uncertainty authorizations where the gap between consent time and consequence time is largest.\nUsage-contingent escalation. Systems can detect when usage patterns change materially and require re-authorization at the moment of change. Triggers might include: data entering training pipelines for the first time, new inference categories being created (especially sensitive attributes), sharing with new recipient classes, or use in high-stakes decision support. This couples consent to concrete use rather than hypothetical possibility. The user who agreed to personalization isn’t automatically enrolled in model training—that transition requires a new authorization tied to the specific new use.\nRenegotiation windows. When downstream uses change materially, users can be offered renegotiation rather than binary accept/exit choices. Options might include accepting revised terms, accepting with constraints, accepting with compensation alternatives, or declining expanded uses while retaining core service access. This makes authority settlement conditional rather than absolute. Material changes reopen the legitimacy question rather than hiding behind original consent.\nQuarantine and cooling-off periods. For high-irreversibility transfers, a deliberate delay before authority becomes final creates space for reflection. Data might enter a quarantine buffer before inclusion in training corpora. Higher permission tiers might activate only after a waiting period following initial grant. This adds temporal structure without prohibiting the transaction. It acknowledges that the moment of agreement is often not the best moment for final settlement.\n\n\nImplementing Preventive Governance\nThese mechanisms require both technical and organizational implementation.\nTechnical requirements include: consent state machines that track authorization across tiers and time, logging that links downstream uses to consent state at time of use, trigger detection for usage-contingent escalation, renewal scheduling and notification systems, and quarantine buffers for high-risk processing.\nOrganizational requirements include: governance roles with authority over tier definitions and trigger criteria, processes for reviewing material changes and renegotiation terms, integration with model lifecycle governance so that training gates connect to consent states, and audit capabilities that can verify consent-to-use linkage.\nThe RACI matrix for preventive governance typically involves:\n\nProduct teams defining staged permissions and user flows\nPrivacy functions defining risk tiers and decay schedules\nLegal ensuring contract terms accommodate staged authority\nAI governance or model risk connecting model lifecycle gates to consent triggers\nData engineering building separate processing paths and traceability infrastructure\n\n\n\nTensions and Tradeoffs\nPreventive governance is not without costs or tensions.\nFriction concerns. Critics argue that staged permissions and renewal requirements add friction that impedes adoption and innovation. The response is proportionality: low-risk permissions remain low-friction, while high-risk authorizations bear appropriately higher process costs. The alternative—bundling all permissions into a single low-friction click—merely shifts costs onto users who bear consequences they couldn’t anticipate.\nTechnical complexity. Implementing consent state machines, trigger detection, and use-to-consent traceability requires engineering investment. But this infrastructure increasingly aligns with regulatory expectations. The EU AI Act’s logging and documentation requirements, deployer transparency obligations, and incident reporting duties all assume traceability that preventive governance also requires.\nCompetitive dynamics. Organizations that implement preventive governance may face competitive disadvantage against those that extract maximum permissions through simpler flows. This is a collective action problem that regulation can address—but organizations with strong governance postures may also benefit from trust premiums and reduced compliance risk.\nImperfect reversibility. Even with quarantine buffers and staged rollout, some irreversibility remains. Models can’t always be untrained. Preventive governance reduces harm from premature settlement but doesn’t guarantee full reversibility. Honesty about these limits is part of the transparency that preventive governance demands.\n\n\nImplications for AI Governance Professionals\nFor practitioners, preventive governance suggests several action areas.\nAssess current consent architecture. Where does your organization rely on point-in-time consent for uses whose meaning will emerge later? What permissions bundle together uses with very different risk profiles?\nIdentify high-risk authorization gaps. Which data uses involve irreversibility, compounding value, and unknowable downstream consequences? These are candidates for staged permissions, time bounds, or escalation triggers.\nConnect consent to model lifecycle. Does your model governance process know what consent state applies to training data? Can you verify that models in production were trained under appropriate authorization?\nDesign for renegotiation. When your organization’s capabilities or uses change materially, do affected individuals have meaningful options beyond accept-all or exit-entirely?\nBuild traceability. Can you demonstrate, for any given downstream use, what consent state authorized it and when? This capability increasingly matters for regulatory compliance and will matter more.\nPreventive governance represents a maturation of the consent paradigm—not its abandonment. It acknowledges that legitimacy requires more than a moment of agreement when consequences unfold over time. For AI governance professionals, this framing offers both a diagnostic lens for current gaps and a design orientation for more durable governance architectures.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#looking-forward",
    "href": "chapters/07-future.html#looking-forward",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.8 Looking Forward",
    "text": "7.8 Looking Forward\nAI governance will continue to evolve as technology advances, regulation matures, and organizational practices develop. Several directions seem likely.\n\nRegulatory Convergence and Divergence\nSome regulatory convergence is likely as jurisdictions learn from each other and international coordination efforts continue. Core concepts like risk-based regulation, transparency requirements, and human oversight are appearing across jurisdictions.\nDivergence will also persist as different jurisdictions prioritize different values and approaches. The EU’s comprehensive regulatory approach, China’s sector-specific regulations, and the United States’ fragmented approach reflect different governance philosophies that may not converge.\nOrganizations operating globally will need governance approaches that can accommodate multiple regulatory frameworks while maintaining coherent practices.\n\n\nMaturing Standards and Assurance\nAI governance standards will continue to develop and mature. ISO standards, industry frameworks, and regulatory requirements will provide increasingly detailed guidance on what good AI governance looks like.\nAI assurance and auditing will professionalize. Standards for AI audits will develop, professional credentials will emerge, and independent assurance will become more common and more meaningful.\nMaturity models will help organizations assess their AI governance capabilities and identify improvement priorities.\n\n\nTechnical Advances in Governance\nTechnical approaches to governance will advance. Explainability techniques will improve, making AI decisions more interpretable. Fairness tools will become more sophisticated and widely deployed. Privacy-enhancing technologies will enable AI on sensitive data with stronger protections.\nAI will be used to govern AI. Automated compliance checking, continuous testing, and monitoring systems will use AI to provide governance at the speed and scale that AI systems require.\nHowever, technical solutions will not eliminate the need for human judgment on governance questions. Technical tools can support governance; they cannot replace the human deliberation that governance ultimately requires.\n\n\nOrganizational Evolution\nAI governance will become more embedded in organizational operations. Separate AI governance functions may merge into enterprise risk management, privacy, or compliance functions as AI governance becomes normalized.\nAI literacy will become a basic organizational competency. As AI becomes ubiquitous, everyone will need some understanding of AI capabilities, limitations, and appropriate use.\nGovernance by design will become standard practice. Building governance into AI development from the start, rather than adding it afterward, will become the expected approach.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#chapter-summary",
    "href": "chapters/07-future.html#chapter-summary",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.9 Chapter Summary",
    "text": "7.9 Chapter Summary\nThis chapter examined ongoing issues where AI governance questions remain unsettled and future directions where the field may evolve.\nLiability frameworks for AI harms remain unclear as traditional doctrines adapt to AI’s distinctive characteristics. Causation, multi-party contribution, foreseeability, and categorical fit all present challenges. The EU is developing AI-specific liability rules; other jurisdictions are working within existing frameworks. Governance should emphasize documentation, insurance, contractual allocation, and risk-proportionate use.\nIntellectual property and AI raises questions about training data copyright, AI-generated output copyrightability, patent inventorship, and trade secret protection. Organizations should understand the intellectual property implications of their AI activities and prepare for evolving legal treatment.\nThe AI auditing profession is emerging but immature. Technical complexity, access limitations, dynamic systems, incomplete standards, and assurance limitations all challenge audit practice. Governance should prepare for increased audit scrutiny through documentation, internal audit capability, external audit relationships, and audit readiness.\nWorkforce transformation includes AI augmentation of workers, potential displacement, and effects on worker well-being. Governance should assess workforce impacts, incorporate worker voice, plan transition support, and monitor well-being.\nOpt-out rights and alternatives present challenges when AI is deeply integrated. Legal requirements in some contexts mandate human alternatives. Governance should assess where alternatives are appropriate, design to enable them, communicate about AI involvement, and ensure equity.\nPreventive governance addresses the structural limitations of point-in-time consent for AI systems characterized by irreversibility, compounding value, and unknowable downstream consequences. Mechanisms including phased permissions, time-bounded authority, usage-contingent escalation, renegotiation windows, and quarantine periods can distribute authorization across time rather than finalizing it prematurely. This represents an emerging area where governance professionals can lead organizational practice.\nLooking forward, regulatory convergence and divergence will both continue, standards and assurance will mature, technical governance capabilities will advance, and AI governance will become more embedded in organizational operations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#review-questions",
    "href": "chapters/07-future.html#review-questions",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.10 Review Questions",
    "text": "7.10 Review Questions\n\nAn organization is evaluating an AI application that could cause significant harm to individuals if it malfunctions. The legal team notes that liability rules for AI harms are unsettled. How should this uncertainty affect the governance decision?\nA company is using a generative AI system to produce marketing content. The company wants to claim copyright in this content to prevent competitors from copying it. What intellectual property considerations should inform this strategy?\nAn external auditor is conducting an AI audit of a company’s customer service chatbot. The company refuses to provide access to the chatbot’s training data, citing competitive sensitivity. How might the auditor respond to this access limitation?\nAn organization is deploying an AI system that will automate work currently performed by customer service representatives. Some workers will be retrained for other roles; others may be laid off. What workforce governance considerations apply?\nA healthcare organization uses AI to analyze patient symptoms and recommend treatment plans. Some patients express preference for human-only care without AI involvement. How should the organization address this preference?\nA social media company’s terms of service include broad consent for data use in AI training, obtained when users sign up. Three years later, the company develops new AI capabilities that can infer sensitive attributes from user content—capabilities that didn’t exist when consent was obtained. What preventive governance mechanisms might have addressed this situation, and what should the company do now?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/07-future.html#references",
    "href": "chapters/07-future.html#references",
    "title": "7  Ongoing Issues and Future Directions",
    "section": "7.11 References",
    "text": "7.11 References\nEuropean Commission. Proposal for a Directive on adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive). COM(2022) 496, 2022.\nU.S. Copyright Office. Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence. Federal Register Vol. 88, No. 51, 2023.\nRaji, Inioluwa Deborah, et al. “Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing.” FAT* ’20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020.\nMökander, Jakob, et al. “Auditing large language models: a three-layered approach.” AI and Ethics (2023).\nAcemoglu, Daron, and Pascual Restrepo. “Tasks, Automation, and the Rise in US Wage Inequality.” Econometrica 90, no. 5 (2022).\nInternational Organization for Standardization. ISO/IEC 42001:2023 Information technology — Artificial intelligence — Management system, 2023.\nIAPP. AIGP Body of Knowledge, Version 2.0.1. International Association of Privacy Professionals, 2025.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ongoing Issues and Future Directions</span>"
    ]
  },
  {
    "objectID": "chapters/08-appendix-a.html",
    "href": "chapters/08-appendix-a.html",
    "title": "8  Frameworks and Templates",
    "section": "",
    "text": "8.1 A.1 AI Impact Assessment Template\nThis appendix provides practical templates for common AI governance artifacts. These templates should be adapted to organizational context; they represent starting points rather than final forms.\nSystem Information\nSystem name: System owner: Date of assessment: Assessor(s):\nPurpose and Use\nDescribe the purpose of the AI system. What problem does it solve? What decisions does it support or make?\nDescribe the intended users. Who will interact with the system? What training or expertise do they have?\nDescribe the affected individuals. Whose data is processed? Who receives outputs? Who is affected by decisions?\nSystem Description\nDescribe how the system works at a level appropriate for governance review. What type of AI/ML approach is used? What are the inputs and outputs?\nDescribe the training data. What data was used? Where did it come from? How representative is it?\nDescribe the deployment environment. Where does the system run? How does it integrate with other systems?\nRisk Assessment\nIdentify potential risks across categories:\nAccuracy risks: What if the system makes errors? How severe would consequences be? How would errors be detected?\nFairness risks: Could the system disadvantage protected groups? Has fairness been tested? What disparities exist?\nTransparency risks: Can affected individuals understand AI involvement and decisions? Can explanations be provided?\nPrivacy risks: What personal data is processed? What are the privacy implications? How is data protected?\nSecurity risks: What security vulnerabilities exist? What would be the impact of security breaches?\nMisuse risks: How could the system be misused? What safeguards prevent misuse?\nFor each identified risk, assess likelihood (low/medium/high) and severity (low/medium/high).\nMitigation Measures\nFor each significant risk, describe planned mitigation measures:\nRisk: [Description] Mitigation: [Description] Residual risk after mitigation: [Low/Medium/High]\nHuman Oversight\nDescribe the human oversight approach. What human review occurs? When can humans override the system? How is oversight meaningful rather than perfunctory?\nMonitoring Plan\nDescribe how the system will be monitored after deployment. What metrics will be tracked? How frequently? Who reviews monitoring output?\nConclusion and Approval\nOverall risk assessment: [Low/Medium/High] Recommendation: [Proceed/Proceed with conditions/Do not proceed] Conditions (if applicable):\nApproval: Approver name: Approver title: Date:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Frameworks and Templates</span>"
    ]
  },
  {
    "objectID": "chapters/08-appendix-a.html#a.2-model-card-template",
    "href": "chapters/08-appendix-a.html#a.2-model-card-template",
    "title": "8  Frameworks and Templates",
    "section": "8.2 A.2 Model Card Template",
    "text": "8.2 A.2 Model Card Template\nModel Details\nModel name: Version: Developer: Date: Model type: Training approach: Parameters/architecture: License:\nIntended Use\nPrimary intended uses:\nPrimary intended users:\nOut-of-scope uses (uses for which the model is not appropriate):\nTraining Data\nDescription of training data:\nData sources:\nData collection process:\nKnown limitations of training data:\nEvaluation Data\nDescription of evaluation data:\nHow evaluation data differs from training data:\nPerformance Metrics\nOverall performance: [Metric]: [Value] [Metric]: [Value]\nDisaggregated performance by relevant groups: [Group]: [Metric]: [Value] [Group]: [Metric]: [Value]\nFairness Considerations\nFairness criteria applied:\nFairness testing performed:\nKnown disparities:\nLimitations and Biases\nKnown limitations:\nPotential biases:\nSituations where the model may perform poorly:\nEthical Considerations\nPotential risks from model use:\nPopulations that may be affected:\nUse cases that raise ethical concerns:\nRecommendations\nFor users of this model:\nFor deployers of this model:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Frameworks and Templates</span>"
    ]
  },
  {
    "objectID": "chapters/08-appendix-a.html#a.3-vendor-ai-assessment-checklist",
    "href": "chapters/08-appendix-a.html#a.3-vendor-ai-assessment-checklist",
    "title": "8  Frameworks and Templates",
    "section": "8.3 A.3 Vendor AI Assessment Checklist",
    "text": "8.3 A.3 Vendor AI Assessment Checklist\nVendor Information\nVendor name: Contact: System/service being evaluated: Date of assessment:\nVendor Governance\nDoes the vendor have documented AI governance policies? [Yes/No/Unknown] Has the vendor committed to responsible AI principles? [Yes/No/Unknown] Does the vendor have relevant certifications (e.g., ISO 42001)? [Yes/No/Unknown] What is the vendor’s track record with AI incidents? [Description]\nSystem Information\nHas the vendor provided adequate description of system capabilities? [Yes/No] Has the vendor provided information about training data? [Yes/No] Has the vendor provided performance metrics? [Yes/No] Has the vendor provided disaggregated performance by demographic groups? [Yes/No] Has the vendor disclosed known limitations? [Yes/No]\nCompliance\nHas the vendor addressed EU AI Act requirements (if applicable)? [Yes/No/N/A] Has the vendor addressed other applicable regulatory requirements? [Yes/No] Will the vendor support the organization’s compliance obligations? [Yes/No]\nContractual Provisions\nDoes the contract provide adequate information rights? [Yes/No] Does the contract provide audit rights? [Yes/No] Does the contract address liability allocation? [Yes/No] Does the contract address incident notification? [Yes/No] Does the contract address data handling and privacy? [Yes/No] Does the contract address updates and change management? [Yes/No] Does the contract address termination and data return? [Yes/No]\nRisk Assessment\nOverall vendor risk: [Low/Medium/High] Technical/performance risk: [Low/Medium/High] Compliance risk: [Low/Medium/High] Operational risk: [Low/Medium/High] Reputational risk: [Low/Medium/High]\nRecommendation\n[Proceed/Proceed with conditions/Do not proceed] Conditions (if applicable):",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Frameworks and Templates</span>"
    ]
  },
  {
    "objectID": "chapters/08-appendix-a.html#a.4-ai-incident-report-template",
    "href": "chapters/08-appendix-a.html#a.4-ai-incident-report-template",
    "title": "8  Frameworks and Templates",
    "section": "8.4 A.4 AI Incident Report Template",
    "text": "8.4 A.4 AI Incident Report Template\nIncident Information\nIncident ID: Date/time detected: Date/time of incident (if different): Reporter: System involved:\nIncident Description\nWhat happened:\nHow the incident was detected:\nWho was affected:\nEstimated impact:\nSeverity Assessment\nSeverity: [Critical/High/Medium/Low]\nCriteria for severity: - Critical: Significant harm to individuals, major regulatory implications, widespread impact - High: Moderate harm to individuals, regulatory implications, significant impact - Medium: Minor harm, limited impact - Low: No harm, minimal impact\nImmediate Response\nActions taken:\nSystem status (active/suspended/modified):\nStakeholders notified:\nInvestigation\nRoot cause (if determined):\nContributing factors:\nEvidence collected:\nRemediation\nShort-term remediation:\nLong-term remediation:\nTimeline for remediation:\nRegulatory Reporting\nReporting required? [Yes/No] If yes, to which authorities: Reporting deadline: Reporting status:\nLessons Learned\nWhat governance processes worked well:\nWhat governance processes need improvement:\nRecommendations for preventing recurrence:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Frameworks and Templates</span>"
    ]
  },
  {
    "objectID": "chapters/08-appendix-a.html#a.5-raci-matrix-for-ai-governance",
    "href": "chapters/08-appendix-a.html#a.5-raci-matrix-for-ai-governance",
    "title": "8  Frameworks and Templates",
    "section": "8.5 A.5 RACI Matrix for AI Governance",
    "text": "8.5 A.5 RACI Matrix for AI Governance\nThis matrix shows typical responsibility allocation for AI governance activities. R = Responsible (does the work), A = Accountable (makes decisions), C = Consulted, I = Informed. Adapt to your organization’s structure.\nUse Case Assessment - Executive: A - AI Governance: R - Legal: C - Technical: C - Business Unit: R\nImpact Assessment - Executive: I - AI Governance: A/R - Legal: C - Technical: C - Risk: C - Business Unit: C\nLegal Requirements Analysis - Executive: I - AI Governance: C - Legal: A/R - Technical: I - Business Unit: I\nSystem Design - Executive: I - AI Governance: C - Legal: C - Technical: A/R - Business Unit: C\nData Governance - Executive: I - AI Governance: C - Data Team: A/R - Technical: C - Legal: C\nTesting and Validation - Executive: I - AI Governance: A - Technical: R - Legal: C - Risk: C\nRelease Approval - Executive: A - AI Governance: R - Legal: C - Technical: C - Risk: C\nOperational Monitoring - Executive: I - AI Governance: A - Technical: R - Operations: R\nIncident Response - Executive: I/A (for serious incidents) - AI Governance: A - Technical: R - Legal: C - Risk: C - Communications: C",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Frameworks and Templates</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html",
    "href": "chapters/09-appendix-b.html",
    "title": "9  Quick Reference Tables",
    "section": "",
    "text": "9.1 B.1 EU AI Act Risk Classification Quick Reference\nProhibited AI (Article 5)\nHigh-Risk AI (Annex III)\nBiometrics: identification, categorization, emotion recognition\nCritical infrastructure: safety components in infrastructure management\nEducation: access, admission, assessment, monitoring, proctoring\nEmployment: recruitment, screening, evaluation, promotion, termination, task allocation, monitoring\nEssential services: credit, insurance, emergency services, public benefits, creditworthiness assessment\nLaw enforcement: risk assessment, polygraphs, evidence analysis, profiling, crime analytics\nMigration: risk assessment, verification, application examination\nJustice: research assistance, legal interpretation\nLimited Risk (Article 50)\nMinimal Risk",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html#b.1-eu-ai-act-risk-classification-quick-reference",
    "href": "chapters/09-appendix-b.html#b.1-eu-ai-act-risk-classification-quick-reference",
    "title": "9  Quick Reference Tables",
    "section": "",
    "text": "Manipulative/deceptive techniques causing significant harm\nExploitation of vulnerabilities (age, disability, economic situation)\nSocial scoring by public authorities\nEmotion inference in workplace/education (with exceptions)\nUntargeted facial image scraping for databases\nReal-time remote biometric identification (with limited exceptions)\nRisk assessment predicting criminal behavior from profiling alone\n\n\n\n\n\n\n\n\n\n\n\n\nSystems interacting with natural persons (disclosure required)\nEmotion recognition / biometric categorization (disclosure required)\nDeep fakes / synthetic content (labeling required)\n\n\n\nAll other AI systems\nNo specific requirements; codes of conduct encouraged",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html#b.2-key-deadlines-and-dates",
    "href": "chapters/09-appendix-b.html#b.2-key-deadlines-and-dates",
    "title": "9  Quick Reference Tables",
    "section": "9.2 B.2 Key Deadlines and Dates",
    "text": "9.2 B.2 Key Deadlines and Dates\nEU AI Act Implementation\nFebruary 2, 2025: Prohibited AI practices effective August 2, 2025: GPAI model provisions effective; governance structures established August 2, 2026: Most provisions effective including high-risk requirements August 2, 2027: High-risk systems in Annex I products effective\nUS State Laws\nColorado AI Act: February 1, 2026 effective date NYC Local Law 144: July 5, 2023 effective date (already in effect)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html#b.3-nist-ai-rmf-core-functions",
    "href": "chapters/09-appendix-b.html#b.3-nist-ai-rmf-core-functions",
    "title": "9  Quick Reference Tables",
    "section": "9.3 B.3 NIST AI RMF Core Functions",
    "text": "9.3 B.3 NIST AI RMF Core Functions\nGOVERN (GV) Establish accountability, culture, and organizational commitment to AI risk management\nGV-1: Policies, processes, procedures GV-2: Accountability structures GV-3: Workforce diversity and expertise GV-4: Organizational culture GV-5: Stakeholder engagement GV-6: Integration with enterprise risk management\nMAP (MP) Understand context and identify risks\nMP-1: Context establishment MP-2: AI system characterization MP-3: Impact and harm identification MP-4: Risk and benefit analysis MP-5: AI actor identification\nMEASURE (ME) Assess, analyze, and track identified risks\nME-1: Risk measurement approaches ME-2: Metric identification and tracking ME-3: Risk monitoring ME-4: Feedback mechanisms\nMANAGE (MG) Prioritize and treat risks\nMG-1: Risk prioritization MG-2: Risk treatment strategies MG-3: Risk response and recovery MG-4: Residual risk management",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html#b.4-common-fairness-metrics",
    "href": "chapters/09-appendix-b.html#b.4-common-fairness-metrics",
    "title": "9  Quick Reference Tables",
    "section": "9.4 B.4 Common Fairness Metrics",
    "text": "9.4 B.4 Common Fairness Metrics\nGroup Fairness Metrics\nDemographic Parity (Statistical Parity): Positive outcomes are proportional across groups Formula: P(Ŷ=1|A=a) = P(Ŷ=1|A=b)\nEqualized Odds: True positive and false positive rates are equal across groups Formula: P(Ŷ=1|A=a,Y=y) = P(Ŷ=1|A=b,Y=y) for y ∈ {0,1}\nEqual Opportunity: True positive rates are equal across groups Formula: P(Ŷ=1|A=a,Y=1) = P(Ŷ=1|A=b,Y=1)\nPredictive Parity: Positive predictive values are equal across groups Formula: P(Y=1|Ŷ=1,A=a) = P(Y=1|Ŷ=1,A=b)\nCalibration: Predicted probabilities match actual outcomes across groups Formula: P(Y=1|S=s,A=a) = P(Y=1|S=s,A=b) for all s\nIndividual Fairness\nSimilar individuals should be treated similarly Requires defining similarity metric\nNote: Some fairness metrics are mathematically incompatible. Organizations must choose which metrics are appropriate for their context.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/09-appendix-b.html#b.5-documentation-requirements-comparison",
    "href": "chapters/09-appendix-b.html#b.5-documentation-requirements-comparison",
    "title": "9  Quick Reference Tables",
    "section": "9.5 B.5 Documentation Requirements Comparison",
    "text": "9.5 B.5 Documentation Requirements Comparison\nGDPR Article 30 Records\n\nPurposes of processing\nCategories of data subjects\nCategories of personal data\nRecipients\nInternational transfers\nRetention periods\nSecurity measures\n\nEU AI Act Technical Documentation (Annex IV)\n\nGeneral description\nDetailed description of AI system elements\nDetailed description of monitoring, functioning, control\nRisk management system\nData and data governance\nLogging capabilities\nInformation about human oversight measures\nPre-determined changes\nMetrics for accuracy, robustness, cybersecurity\nDiscriminatory impacts assessment\n\nModel Cards (Mitchell et al.)\n\nModel details\nIntended use\nFactors (groups, instruments, environments)\nMetrics\nEvaluation data\nTraining data\nQuantitative analyses\nEthical considerations\nCaveats and recommendations",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quick Reference Tables</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html",
    "href": "chapters/10-appendix-c.html",
    "title": "10  Practice Scenarios",
    "section": "",
    "text": "10.1 Scenario 1: The Rushed Deployment\nThese scenarios present realistic AI governance situations for discussion and analysis. They may be used for exam preparation, team training, or governance process exercises.\nSituation\nA retail company has developed an AI recommendation system intended to personalize product suggestions. The development team completed technical testing and the system meets accuracy targets. Marketing wants to deploy for the holiday shopping season, which begins in three weeks.\nThe governance team has not completed its impact assessment. Initial review identified potential concerns about whether recommendations might systematically differ across demographic groups, but disaggregated fairness testing has not been performed. The legal team is still analyzing whether certain personalization approaches might raise consumer protection concerns.\nMarketing argues that delay means missing the most important shopping season of the year. The technical team says the system can always be updated if issues emerge. The governance team is concerned about deploying without completed review.\nQuestions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-1-the-rushed-deployment",
    "href": "chapters/10-appendix-c.html#scenario-1-the-rushed-deployment",
    "title": "10  Practice Scenarios",
    "section": "",
    "text": "What are the key governance considerations in this situation?\nWhat additional information would help inform the decision?\nWhat options exist beyond simply approving or rejecting deployment?\nHow should the organization balance business urgency against governance thoroughness?\nWhat governance process improvements might prevent this situation in future projects?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-2-the-fairness-tradeoff",
    "href": "chapters/10-appendix-c.html#scenario-2-the-fairness-tradeoff",
    "title": "10  Practice Scenarios",
    "section": "10.2 Scenario 2: The Fairness Tradeoff",
    "text": "10.2 Scenario 2: The Fairness Tradeoff\nSituation\nA financial services company is testing an AI model for credit decisions. Overall accuracy is excellent, exceeding the existing manual process.\nFairness testing reveals that the model has significantly higher false negative rates for applicants over age 60, meaning creditworthy older applicants are more likely to be incorrectly declined. The technical team has explored mitigation approaches, but each degrades overall accuracy.\nOption A: Deploy the current model with higher accuracy but known age-related disparities. Option B: Deploy a modified model with reduced disparities but lower overall accuracy. Option C: Do not deploy and continue using the existing manual process.\nQuestions\n\nWhat legal requirements might apply to this situation?\nHow should the organization think about the tradeoff between accuracy and fairness?\nWhat stakeholders should be involved in this decision?\nWhat documentation should accompany whatever decision is made?\nIf Option A is chosen, what monitoring and mitigation measures would be appropriate?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-3-the-vendor-limitation",
    "href": "chapters/10-appendix-c.html#scenario-3-the-vendor-limitation",
    "title": "10  Practice Scenarios",
    "section": "10.3 Scenario 3: The Vendor Limitation",
    "text": "10.3 Scenario 3: The Vendor Limitation\nSituation\nAn HR department wants to use a vendor’s AI tool for initial screening of job applications. The vendor provides a demonstration that impresses the HR team with its ability to quickly rank candidates.\nDuring governance review, the legal team requests information about the model’s training data, accuracy across demographic groups, and compliance with employment discrimination laws. The vendor responds that training data composition is proprietary, that they cannot provide disaggregated accuracy metrics, but that they warrant the tool complies with applicable laws.\nThe HR team argues that the vendor is well-established, the tool is widely used, and the organization’s competitors are already using similar tools.\nQuestions\n\nWhat governance concerns does the vendor’s limited disclosure create?\nHow should the organization evaluate the vendor’s warranty of legal compliance?\nWhat contractual provisions might address the organization’s concerns?\nWhat testing could the organization perform itself to address information gaps?\nHow should competitive considerations factor into the governance decision?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-4-the-unexpected-behavior",
    "href": "chapters/10-appendix-c.html#scenario-4-the-unexpected-behavior",
    "title": "10  Practice Scenarios",
    "section": "10.4 Scenario 4: The Unexpected Behavior",
    "text": "10.4 Scenario 4: The Unexpected Behavior\nSituation\nA healthcare organization has deployed an AI system that assists with patient triage, helping staff prioritize patients based on reported symptoms. The system has been operating for six months with positive feedback from staff.\nA nurse notices that the system seems to consistently assign lower priority to elderly patients reporting certain symptoms. She raises this concern with her supervisor, who asks the technical team to investigate.\nInvestigation reveals that the model learned patterns from historical data that reflected a genuine clinical pattern: certain symptoms are more concerning in younger patients. However, the nurse argues that some elderly patients who should have been seen urgently were deprioritized.\nQuestions\n\nHow should the organization respond to this finding?\nWas the model behaving incorrectly, or reflecting legitimate clinical patterns?\nWhat governance mechanisms should have detected this issue earlier?\nHow should the organization communicate with staff and patients about this issue?\nWhat changes to the system or its governance are appropriate?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-5-the-scope-expansion",
    "href": "chapters/10-appendix-c.html#scenario-5-the-scope-expansion",
    "title": "10  Practice Scenarios",
    "section": "10.5 Scenario 5: The Scope Expansion",
    "text": "10.5 Scenario 5: The Scope Expansion\nSituation\nA technology company developed an AI content moderation system for their social media platform. The system was designed and tested for identifying spam and obvious terms-of-service violations. Impact assessment and governance review addressed this scope.\nOver time, product managers have expanded the system’s use to also flag potential misinformation, hate speech, and self-harm content. Each expansion seemed incremental and did not trigger new governance review. The system is now making significant content decisions with potential impacts on free expression that the original governance review did not contemplate.\nA content creator whose posts were removed files a complaint alleging the moderation system is biased. Investigation reveals the governance documentation does not reflect the system’s current scope.\nQuestions\n\nWhat governance failures allowed this scope expansion without review?\nHow should the organization respond to the current situation?\nWhat governance mechanisms could prevent similar scope creep?\nHow should change management apply to AI system evolution?\nWhat are the organization’s obligations regarding the original governance documentation?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-6-the-autonomous-agent",
    "href": "chapters/10-appendix-c.html#scenario-6-the-autonomous-agent",
    "title": "10  Practice Scenarios",
    "section": "10.6 Scenario 6: The Autonomous Agent",
    "text": "10.6 Scenario 6: The Autonomous Agent\nSituation\nA company is piloting an AI agent that can autonomously handle customer service inquiries. Unlike a chatbot that provides information, this agent can take actions: process refunds, modify orders, apply discounts, and escalate to human agents.\nDuring pilot testing, the agent handles most inquiries appropriately. However, reviewers note several concerning patterns: the agent sometimes offers discounts that exceed policy limits, the agent occasionally processes refunds for orders that do not qualify, and the agent’s escalation decisions are inconsistent.\nThe technical team says these issues can be addressed through additional training. The customer service team loves the agent’s efficiency and wants to expand deployment. The finance team is concerned about unauthorized discounts and refunds.\nQuestions\n\nHow do governance considerations for autonomous agents differ from advisory AI?\nWhat controls should exist before expanding deployment?\nHow should human oversight work for an agent making many autonomous decisions?\nWhat accountability mechanisms are appropriate for agent actions?\nWhat monitoring would detect problematic patterns in agent behavior?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/10-appendix-c.html#scenario-7-the-cross-border-deployment",
    "href": "chapters/10-appendix-c.html#scenario-7-the-cross-border-deployment",
    "title": "10  Practice Scenarios",
    "section": "10.7 Scenario 7: The Cross-Border Deployment",
    "text": "10.7 Scenario 7: The Cross-Border Deployment\nSituation\nA US-headquartered company wants to deploy an AI system across its global operations. The system will be used in the United States, European Union, United Kingdom, Canada, and Japan. It involves processing personal data and makes recommendations that affect individuals.\nLegal analysis reveals different requirements across jurisdictions: EU AI Act high-risk requirements may apply for EU deployment; GDPR Article 22 creates obligations for EU data subjects; various Canadian provincial laws apply; Japan has guidelines but limited binding requirements; US requirements vary by state.\nThe product team wants a unified global deployment. Legal suggests significant differences in compliance requirements across jurisdictions.\nQuestions\n\nHow should the organization approach governance for a system with different requirements across jurisdictions?\nShould the organization apply the most stringent requirements globally, or comply differently in different jurisdictions?\nWhat documentation strategy supports multi-jurisdictional compliance?\nHow should the organization handle jurisdictions where requirements are unclear or evolving?\nWhat organizational structure supports governance across jurisdictions?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Practice Scenarios</span>"
    ]
  },
  {
    "objectID": "chapters/11-appendix-d.html",
    "href": "chapters/11-appendix-d.html",
    "title": "11  Glossary and Resources",
    "section": "",
    "text": "11.1 D.1 Glossary of Key Terms\nAccountability: The principle that organizations and individuals responsible for AI systems should be identifiable and answerable for the systems’ impacts.\nAdversarial attack: An attempt to cause an AI system to make errors through inputs specifically crafted to exploit system vulnerabilities.\nAI governance: The policies, procedures, structures, and practices through which organizations manage the development and deployment of AI systems responsibly.\nAlgorithmic impact assessment: A systematic evaluation of an AI system’s potential impacts on individuals, groups, and society.\nBias: In AI context, systematic errors that create unfair outcomes for certain groups or individuals. Can arise from training data, model design, or deployment context.\nConcept drift: Changes in the underlying patterns or relationships that an AI model learned, which may cause model performance to degrade over time.\nConformity assessment: Under the EU AI Act, the process of verifying that a high-risk AI system meets applicable requirements before it can be placed on the market.\nData drift: Changes in the statistical properties of input data compared to the data used to train the model.\nDeployer: Under the EU AI Act, a natural or legal person using an AI system under its authority, except where the AI system is used in the course of personal non-professional activity.\nExplainability: The degree to which the internal mechanics of an AI system can be explained in human terms.\nFairness: The principle that AI systems should treat people equitably and should not discriminate based on protected characteristics.\nFoundation model: A large AI model trained on broad data that can be adapted for various downstream tasks. Also called general-purpose AI model.\nGenerative AI: AI systems that create new content (text, images, audio, video) rather than analyzing or classifying existing content.\nHallucination: When a generative AI system produces confident but false outputs, such as fabricated facts or citations.\nHigh-risk AI system: Under the EU AI Act, an AI system that poses significant risks to health, safety, or fundamental rights and is subject to extensive compliance requirements.\nHuman oversight: The capacity for humans to understand, supervise, and when necessary intervene in or stop AI system operation.\nImpact assessment: A systematic evaluation of potential consequences of an AI system, including privacy, fairness, and other impacts.\nMachine learning: An approach to AI where systems learn patterns from data rather than being explicitly programmed with rules.\nModel card: A documentation format that provides information about a trained model’s intended uses, performance, limitations, and ethical considerations.\nProvider: Under the EU AI Act, a natural or legal person who develops an AI system or has an AI system developed and places it on the market or puts it into service under its own name or trademark.\nResponsible AI: Development and deployment of AI systems that are ethical, transparent, fair, accountable, and aligned with human values.\nRisk-based approach: Regulatory or governance strategy that applies requirements proportionate to the level of risk an AI system presents.\nRobustness: The ability of an AI system to maintain performance when facing unexpected inputs, adversarial attacks, or changing conditions.\nTransparency: The principle that AI development and deployment should be open about methods, limitations, and impacts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Glossary and Resources</span>"
    ]
  },
  {
    "objectID": "chapters/11-appendix-d.html#d.2-key-resources",
    "href": "chapters/11-appendix-d.html#d.2-key-resources",
    "title": "11  Glossary and Resources",
    "section": "11.2 D.2 Key Resources",
    "text": "11.2 D.2 Key Resources\nRegulatory and Standards Bodies\nEuropean Commission AI Act resources: https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai\nNIST AI Risk Management Framework: https://www.nist.gov/itl/ai-risk-management-framework\nOECD AI Policy Observatory: https://oecd.ai/en/\nISO AI standards: https://www.iso.org/committee/6794475.html\nProfessional Resources\nIAPP AI Governance resources: https://iapp.org/resources/topics/artificial-intelligence/\nResearch and Academia\nPartnership on AI: https://partnershiponai.org/\nAI Now Institute: https://ainowinstitute.org/\nStanford Human-Centered AI: https://hai.stanford.edu/\nRegulatory Guidance\nUS FTC AI guidance: https://www.ftc.gov/business-guidance/blog/tags/artificial-intelligence\nUK ICO AI guidance: https://ico.org.uk/for-organisations/ai/\nTechnical Resources\nModel Cards paper: Mitchell et al., “Model Cards for Model Reporting” (2019)\nDatasheets paper: Gebru et al., “Datasheets for Datasets” (2021)\nFairness ML resources: https://fairmlbook.org/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Glossary and Resources</span>"
    ]
  },
  {
    "objectID": "99-backmatter.html",
    "href": "99-backmatter.html",
    "title": "About the Author",
    "section": "",
    "text": "Connect\nShawn Ogunseye, PhD is a researcher and educator working at the intersection of artificial intelligence, information quality, and digital transformation. His work focuses on how organizations can harness AI responsibly while navigating the complex governance challenges that emerge when intelligent systems meet human institutions.\nDr. Ogunseye brings both academic rigor and practical perspective to AI governance. Before entering academia, he accumulated extensive industry experience in software development and IT management. He earned his PhD in Information Systems from Memorial University of Newfoundland, Canada, and served as a postdoctoral research and teaching fellow at the Sauder School of Business, University of British Columbia.\nHis research on intelligence augmentation, data quality, and the sociotechnical dimensions of AI has been published in leading venues including AI & Society, Communications of the ACM, and proceedings of the International Conference on Information Systems (ICIS). He has organized workshops on the social impact of AI for the Association for the Advancement of Artificial Intelligence (AAAI) and serves as Secretary for the AIS Special Interest Group on Systems Analysis and Design (SIGSAND).\nDr. Ogunseye is an award-winning educator, recognized for innovation in teaching. He is a member of the Association for Computing Machinery (ACM) and the Association for Information Systems (AIS), and contributes to advancing the field through service on conference committees and editorial boards.\nLinkedIn: linkedin.com/in/shawnogunseye\nWeb: ogunseye.com\nEmail: shawn[at]ogunseye[.]com",
    "crumbs": [
      "About the Author"
    ]
  },
  {
    "objectID": "99-backmatter.html#connect",
    "href": "99-backmatter.html#connect",
    "title": "About the Author",
    "section": "",
    "text": "TipStay Updated\n\n\n\nThis is a living book. Follow the author for updates on new chapters, corrections, and supplementary materials.",
    "crumbs": [
      "About the Author"
    ]
  },
  {
    "objectID": "99-backmatter.html#also-by-shawn-ogunseye",
    "href": "99-backmatter.html#also-by-shawn-ogunseye",
    "title": "About the Author",
    "section": "Also by Shawn Ogunseye",
    "text": "Also by Shawn Ogunseye\nSelected Publications:\n\n“Letter to the Head of AI” — AI & Society (2025)\n“They Can Include AI, But Should They? Teaching students about sensible solutions in the age of AI hype” — Communications of the ACM Blog (2025)\n“Stop Training Your Competitor’s AI” — Communications of the ACM Blog (2025)\n“Agile Development: The Promise, the Reality, the Opportunity” — CAiSE Workshop on Agile Information Systems Engineering (2024)",
    "crumbs": [
      "About the Author"
    ]
  },
  {
    "objectID": "99-backmatter.html#acknowledgments",
    "href": "99-backmatter.html#acknowledgments",
    "title": "About the Author",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book reflects the contributions of many colleagues, students, and practitioners who have shaped my thinking about AI governance. Special thanks to the AI governance community whose questions and challenges continue to push the field forward.\nTo Dolapo, Tara, and Tessa—for the patience, support, and joy that make all work meaningful.",
    "crumbs": [
      "About the Author"
    ]
  }
]